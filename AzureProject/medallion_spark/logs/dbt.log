[0m22:06:51.763129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021886D7FD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021886BAAF30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021886BABF80>]}


============================== 22:06:51.767130 | 06fba000-46c7-44cb-80c3-8cbf70ab54e8 ==============================
[0m22:06:51.767130 [info ] [MainThread]: Running with dbt=1.8.2
[0m22:06:51.768130 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:06:51.787135 [info ] [MainThread]: dbt version: 1.8.2
[0m22:06:51.788135 [info ] [MainThread]: python version: 3.12.4
[0m22:06:51.788135 [info ] [MainThread]: python path: C:\Program Files\Python312\python.exe
[0m22:06:51.789135 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m22:06:51.961174 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:06:51.961174 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:06:51.962174 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:06:53.347487 [info ] [MainThread]: Using profiles dir at C:\Users\shuru\.dbt
[0m22:06:53.348487 [info ] [MainThread]: Using profiles.yml file at C:\Users\shuru\.dbt\profiles.yml
[0m22:06:53.349488 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\shuru\OneDrive\Desktop\database\medallion_spark\dbt_project.yml
[0m22:06:53.349488 [info ] [MainThread]: adapter type: databricks
[0m22:06:53.350488 [info ] [MainThread]: adapter version: 1.8.1
[0m22:06:53.444513 [info ] [MainThread]: Configuration:
[0m22:06:53.445513 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m22:06:53.446514 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m22:06:53.447514 [info ] [MainThread]: Required dependencies:
[0m22:06:53.448513 [debug] [MainThread]: Executing "git --help"
[0m22:06:53.487523 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:06:53.487523 [debug] [MainThread]: STDERR: "b''"
[0m22:06:53.488523 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m22:06:53.489523 [info ] [MainThread]: Connection:
[0m22:06:53.490523 [info ] [MainThread]:   host: adb-1655488425849601.1.azuredatabricks.net
[0m22:06:53.490523 [info ] [MainThread]:   http_path: sql/protocolv1/o/1655488425849601/0614-014958-bfyxstku
[0m22:06:53.491524 [info ] [MainThread]:   catalog: hive_metastore
[0m22:06:53.491524 [info ] [MainThread]:   schema: saleslt
[0m22:06:53.492524 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m22:06:53.493524 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2304366817792, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(8332, 2696), compute-name=) - Creating connection
[0m22:06:53.494524 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m22:06:53.494524 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2304366817792, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8332, 2696), compute-name=) - Acquired connection on thread (8332, 2696), using default compute resource
[0m22:06:53.494524 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2304366817792, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8332, 2696), compute-name=) - Checking idleness
[0m22:06:53.495524 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2304366817792, session-id=None, name=debug, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(8332, 2696), compute-name=) - Retrieving connection
[0m22:06:53.495524 [debug] [MainThread]: Using databricks connection "debug"
[0m22:06:53.496524 [debug] [MainThread]: On debug: select 1 as id
[0m22:06:53.496524 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:06:53.775410 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2304366817792, session-id=18adcee0-f3e9-4f1f-915b-e7bcd08c994d, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8332, 2696), compute-name=) - Connection created
[0m22:06:53.776410 [debug] [MainThread]: Databricks adapter: Cursor(session-id=18adcee0-f3e9-4f1f-915b-e7bcd08c994d, command-id=Unknown) - Created cursor
[0m22:06:53.996483 [debug] [MainThread]: SQL status: OK in 0.5 seconds
[0m22:06:53.998497 [debug] [MainThread]: Databricks adapter: Cursor(session-id=18adcee0-f3e9-4f1f-915b-e7bcd08c994d, command-id=5c1633b7-396f-47ab-ab19-e56f391ba9cd) - Closing cursor
[0m22:06:53.999485 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2304366817792, session-id=18adcee0-f3e9-4f1f-915b-e7bcd08c994d, name=debug, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8332, 2696), compute-name=) - Released connection
[0m22:06:53.999485 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m22:06:54.001484 [info ] [MainThread]: [32mAll checks passed![0m
[0m22:06:54.003498 [debug] [MainThread]: Command `dbt debug` succeeded at 22:06:54.002486 after 2.40 seconds
[0m22:06:54.003498 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m22:06:54.004485 [debug] [MainThread]: On debug: Close
[0m22:06:54.004485 [debug] [MainThread]: Databricks adapter: Connection(session-id=18adcee0-f3e9-4f1f-915b-e7bcd08c994d) - Closing connection
[0m22:06:54.178486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021886A23740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218A1858C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021883F4DB80>]}
[0m22:06:54.179487 [debug] [MainThread]: Flushing usage events
[0m22:18:14.328577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249CFCA7410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249D24EB1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249D24EB680>]}


============================== 22:18:14.334577 | a4c6c573-b142-422e-b09e-9e3d297cf979 ==============================
[0m22:18:14.334577 [info ] [MainThread]: Running with dbt=1.8.2
[0m22:18:14.335577 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m22:18:14.356581 [info ] [MainThread]: dbt version: 1.8.2
[0m22:18:14.357582 [info ] [MainThread]: python version: 3.12.4
[0m22:18:14.358583 [info ] [MainThread]: python path: C:\Program Files\Python312\python.exe
[0m22:18:14.359583 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m22:18:14.544624 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:18:14.544624 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:18:14.545625 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:18:15.896930 [info ] [MainThread]: Using profiles dir at C:\Users\shuru\.dbt
[0m22:18:15.897931 [info ] [MainThread]: Using profiles.yml file at C:\Users\shuru\.dbt\profiles.yml
[0m22:18:15.898932 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\shuru\OneDrive\Desktop\database\medallion_spark\dbt_project.yml
[0m22:18:15.899932 [info ] [MainThread]: adapter type: databricks
[0m22:18:15.900932 [info ] [MainThread]: adapter version: 1.8.1
[0m22:18:15.989951 [info ] [MainThread]: Configuration:
[0m22:18:15.991952 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m22:18:15.992952 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m22:18:15.992952 [info ] [MainThread]: Required dependencies:
[0m22:18:15.993952 [debug] [MainThread]: Executing "git --help"
[0m22:18:16.018958 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:18:16.019958 [debug] [MainThread]: STDERR: "b''"
[0m22:18:16.020959 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m22:18:16.020959 [info ] [MainThread]: Connection:
[0m22:18:16.021958 [info ] [MainThread]:   host: adb-1655488425849601.1.azuredatabricks.net
[0m22:18:16.022960 [info ] [MainThread]:   http_path: sql/protocolv1/o/1655488425849601/0614-014958-bfyxstku
[0m22:18:16.022960 [info ] [MainThread]:   catalog: hive_metastore
[0m22:18:16.023959 [info ] [MainThread]:   schema: saleslt
[0m22:18:16.024959 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m22:18:16.024959 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2516531070208, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(1856, 15136), compute-name=) - Creating connection
[0m22:18:16.025959 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m22:18:16.025959 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2516531070208, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(1856, 15136), compute-name=) - Acquired connection on thread (1856, 15136), using default compute resource
[0m22:18:16.026960 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2516531070208, session-id=None, name=debug, idle-time=0.0010008811950683594s, acquire-count=1, language=None, thread-identifier=(1856, 15136), compute-name=) - Checking idleness
[0m22:18:16.027960 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2516531070208, session-id=None, name=debug, idle-time=0.0010008811950683594s, acquire-count=1, language=None, thread-identifier=(1856, 15136), compute-name=) - Retrieving connection
[0m22:18:16.027960 [debug] [MainThread]: Using databricks connection "debug"
[0m22:18:16.027960 [debug] [MainThread]: On debug: select 1 as id
[0m22:18:16.028960 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:18:16.512360 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2516531070208, session-id=71d8fb1f-8312-4a52-8176-f136dda5aa91, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(1856, 15136), compute-name=) - Connection created
[0m22:18:16.513360 [debug] [MainThread]: Databricks adapter: Cursor(session-id=71d8fb1f-8312-4a52-8176-f136dda5aa91, command-id=Unknown) - Created cursor
[0m22:18:16.808382 [debug] [MainThread]: SQL status: OK in 0.7799999713897705 seconds
[0m22:18:16.810375 [debug] [MainThread]: Databricks adapter: Cursor(session-id=71d8fb1f-8312-4a52-8176-f136dda5aa91, command-id=b0fec0e0-ee32-419d-94c0-1a9782d05e08) - Closing cursor
[0m22:18:16.811370 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2516531070208, session-id=71d8fb1f-8312-4a52-8176-f136dda5aa91, name=debug, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(1856, 15136), compute-name=) - Released connection
[0m22:18:16.811370 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m22:18:16.813372 [info ] [MainThread]: [32mAll checks passed![0m
[0m22:18:16.815384 [debug] [MainThread]: Command `dbt debug` succeeded at 22:18:16.815384 after 2.61 seconds
[0m22:18:16.816384 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m22:18:16.817371 [debug] [MainThread]: On debug: Close
[0m22:18:16.818377 [debug] [MainThread]: Databricks adapter: Connection(session-id=71d8fb1f-8312-4a52-8176-f136dda5aa91) - Closing connection
[0m22:18:16.883380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249D235E330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249ECF0C500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249D1C8D070>]}
[0m22:18:16.883380 [debug] [MainThread]: Flushing usage events
[0m22:46:11.149629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BE40CF8F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BE468F020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BE48C2360>]}


============================== 22:46:11.154629 | 2fbebb93-d389-4b7f-9be7-25b0dbf96521 ==============================
[0m22:46:11.154629 [info ] [MainThread]: Running with dbt=1.8.2
[0m22:46:11.155630 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:46:11.324668 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:46:11.325669 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:46:11.326670 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:46:12.941637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2fbebb93-d389-4b7f-9be7-25b0dbf96521', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BE2439460>]}
[0m22:46:13.002649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2fbebb93-d389-4b7f-9be7-25b0dbf96521', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BE4A06510>]}
[0m22:46:13.004650 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m22:46:13.038657 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m22:46:13.040659 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:46:13.041659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2fbebb93-d389-4b7f-9be7-25b0dbf96521', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BFF4D2D80>]}
[0m22:46:16.135744 [error] [MainThread]: Encountered an error:
Compilation Error
  Snapshot 'snapshot.medallion_spark.address_snapshot' (snapshots\address.sql) depends on a source named 'saleslt.address' which was not found
[0m22:46:16.137745 [debug] [MainThread]: Command `dbt snapshot` failed at 22:46:16.137745 after 5.15 seconds
[0m22:46:16.138745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BE4A07B60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BFF5AA2A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023BFF43A210>]}
[0m22:46:16.139746 [debug] [MainThread]: Flushing usage events
[0m23:28:05.308253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E80407BC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E80460650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EFFBEF8C0>]}


============================== 23:28:05.313254 | 1a7dc37d-d0a3-41d0-ade6-a443dc4aafdf ==============================
[0m23:28:05.313254 [info ] [MainThread]: Running with dbt=1.8.2
[0m23:28:05.314254 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m23:28:05.337259 [info ] [MainThread]: dbt version: 1.8.2
[0m23:28:05.337259 [info ] [MainThread]: python version: 3.12.4
[0m23:28:05.338259 [info ] [MainThread]: python path: C:\Program Files\Python312\python.exe
[0m23:28:05.339260 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m23:28:05.531304 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:28:05.532303 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:28:05.532303 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:28:06.940631 [info ] [MainThread]: Using profiles dir at C:\Users\shuru\.dbt
[0m23:28:06.941630 [info ] [MainThread]: Using profiles.yml file at C:\Users\shuru\.dbt\profiles.yml
[0m23:28:06.942630 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\shuru\OneDrive\Desktop\database\medallion_spark\dbt_project.yml
[0m23:28:06.943631 [info ] [MainThread]: adapter type: databricks
[0m23:28:06.943631 [info ] [MainThread]: adapter version: 1.8.1
[0m23:28:07.036652 [info ] [MainThread]: Configuration:
[0m23:28:07.036652 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m23:28:07.037652 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m23:28:07.038653 [info ] [MainThread]: Required dependencies:
[0m23:28:07.038653 [debug] [MainThread]: Executing "git --help"
[0m23:28:07.058656 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m23:28:07.059657 [debug] [MainThread]: STDERR: "b''"
[0m23:28:07.060658 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m23:28:07.061659 [info ] [MainThread]: Connection:
[0m23:28:07.062658 [info ] [MainThread]:   host: adb-1655488425849601.1.azuredatabricks.net
[0m23:28:07.063659 [info ] [MainThread]:   http_path: sql/protocolv1/o/1655488425849601/0614-014958-bfyxstku
[0m23:28:07.063659 [info ] [MainThread]:   catalog: hive_metastore
[0m23:28:07.064659 [info ] [MainThread]:   schema: saleslt
[0m23:28:07.065659 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m23:28:07.066658 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1780269868688, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13572, 15656), compute-name=) - Creating connection
[0m23:28:07.066658 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m23:28:07.067659 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1780269868688, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13572, 15656), compute-name=) - Acquired connection on thread (13572, 15656), using default compute resource
[0m23:28:07.067659 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1780269868688, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13572, 15656), compute-name=) - Checking idleness
[0m23:28:07.068659 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1780269868688, session-id=None, name=debug, idle-time=0.001001119613647461s, acquire-count=1, language=None, thread-identifier=(13572, 15656), compute-name=) - Retrieving connection
[0m23:28:07.068659 [debug] [MainThread]: Using databricks connection "debug"
[0m23:28:07.069660 [debug] [MainThread]: On debug: select 1 as id
[0m23:28:07.069660 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:28:07.497537 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1780269868688, session-id=20a55522-d0c7-4441-88ab-b55af03939cf, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13572, 15656), compute-name=) - Connection created
[0m23:28:07.498537 [debug] [MainThread]: Databricks adapter: Cursor(session-id=20a55522-d0c7-4441-88ab-b55af03939cf, command-id=Unknown) - Created cursor
[0m23:28:07.783442 [debug] [MainThread]: SQL status: OK in 0.7099999785423279 seconds
[0m23:28:07.784442 [debug] [MainThread]: Databricks adapter: Cursor(session-id=20a55522-d0c7-4441-88ab-b55af03939cf, command-id=02b1da1b-b074-41ae-bf2b-89a48b0c356d) - Closing cursor
[0m23:28:07.785442 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1780269868688, session-id=20a55522-d0c7-4441-88ab-b55af03939cf, name=debug, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(13572, 15656), compute-name=) - Released connection
[0m23:28:07.785442 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m23:28:07.786444 [info ] [MainThread]: [32mAll checks passed![0m
[0m23:28:07.788443 [debug] [MainThread]: Command `dbt debug` succeeded at 23:28:07.788443 after 2.63 seconds
[0m23:28:07.788443 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m23:28:07.789443 [debug] [MainThread]: On debug: Close
[0m23:28:07.789443 [debug] [MainThread]: Databricks adapter: Connection(session-id=20a55522-d0c7-4441-88ab-b55af03939cf) - Closing connection
[0m23:28:07.984853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E805A6690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E805A40B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E80557E00>]}
[0m23:28:07.984853 [debug] [MainThread]: Flushing usage events
[0m23:28:22.040184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A151B76B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A16CACB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A16CAEC60>]}


============================== 23:28:22.045186 | b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2 ==============================
[0m23:28:22.045186 [info ] [MainThread]: Running with dbt=1.8.2
[0m23:28:22.046187 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m23:28:22.224599 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:28:22.225600 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:28:22.226600 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:28:23.780450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A17CE7B30>]}
[0m23:28:23.837463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A32608890>]}
[0m23:28:23.838463 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m23:28:23.850466 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m23:28:23.852467 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m23:28:23.853467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A32745010>]}
[0m23:28:26.175992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A32B1DF40>]}
[0m23:28:26.318565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A32B42FF0>]}
[0m23:28:26.319564 [info ] [MainThread]: Found 2 models, 7 snapshots, 4 data tests, 9 sources, 586 macros
[0m23:28:26.320566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A32B4C380>]}
[0m23:28:26.323566 [info ] [MainThread]: 
[0m23:28:26.324567 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15424, 16280), compute-name=) - Creating connection
[0m23:28:26.324567 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:28:26.325566 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Acquired connection on thread (15424, 16280), using default compute resource
[0m23:28:26.332566 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15424, 9304), compute-name=) - Creating connection
[0m23:28:26.332566 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m23:28:26.333566 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15424, 9304), compute-name=) - Acquired connection on thread (15424, 9304), using default compute resource
[0m23:28:26.333566 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15424, 9304), compute-name=) - Checking idleness
[0m23:28:26.334567 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=None, name=list_hive_metastore, idle-time=0.0010008811950683594s, acquire-count=1, language=None, thread-identifier=(15424, 9304), compute-name=) - Retrieving connection
[0m23:28:26.334567 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m23:28:26.335567 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m23:28:26.335567 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:28:26.520405 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15424, 9304), compute-name=) - Connection created
[0m23:28:26.521407 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, command-id=Unknown) - Created cursor
[0m23:28:26.842571 [debug] [ThreadPool]: SQL status: OK in 0.5099999904632568 seconds
[0m23:28:26.853574 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, command-id=9078c9d4-4453-480a-a4d5-af131753dbd6) - Closing cursor
[0m23:28:26.854574 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15424, 9304), compute-name=) - Released connection
[0m23:28:26.855575 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=list_hive_metastore, idle-time=0.001001119613647461s, acquire-count=0, language=None, thread-identifier=(15424, 9304), compute-name=) - Checking idleness
[0m23:28:26.855575 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_snapshots)
[0m23:28:26.856575 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=create_hive_metastore_snapshots, idle-time=0.002001523971557617s, acquire-count=0, language=None, thread-identifier=(15424, 9304), compute-name=) - Reusing connection previously named list_hive_metastore
[0m23:28:26.856575 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=create_hive_metastore_snapshots, idle-time=0.002001523971557617s, acquire-count=1, language=None, thread-identifier=(15424, 9304), compute-name=) - Acquired connection on thread (15424, 9304), using default compute resource
[0m23:28:26.857574 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=create_hive_metastore_snapshots, idle-time=0.003000974655151367s, acquire-count=1, language=None, thread-identifier=(15424, 9304), compute-name=) - Checking idleness
[0m23:28:26.858575 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=create_hive_metastore_snapshots, idle-time=0.004001617431640625s, acquire-count=2, language=None, thread-identifier=(15424, 9304), compute-name=) - Acquired connection on thread (15424, 9304), using default compute resource
[0m23:28:26.858575 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "snapshots"
"
[0m23:28:26.869577 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=create_hive_metastore_snapshots, idle-time=0.015003442764282227s, acquire-count=2, language=None, thread-identifier=(15424, 9304), compute-name=) - Checking idleness
[0m23:28:26.870578 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=create_hive_metastore_snapshots, idle-time=0.015003442764282227s, acquire-count=2, language=None, thread-identifier=(15424, 9304), compute-name=) - Retrieving connection
[0m23:28:26.870578 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=create_hive_metastore_snapshots, idle-time=0.01600503921508789s, acquire-count=2, language=None, thread-identifier=(15424, 9304), compute-name=) - Checking idleness
[0m23:28:26.871577 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=create_hive_metastore_snapshots, idle-time=0.017003774642944336s, acquire-count=2, language=None, thread-identifier=(15424, 9304), compute-name=) - Retrieving connection
[0m23:28:26.871577 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:28:26.872578 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_snapshots"
[0m23:28:26.872578 [debug] [ThreadPool]: On create_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "create_hive_metastore_snapshots"} */
create schema if not exists `hive_metastore`.`snapshots`
  
[0m23:28:26.873579 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, command-id=Unknown) - Created cursor
[0m23:28:27.462529 [debug] [ThreadPool]: SQL status: OK in 0.5899999737739563 seconds
[0m23:28:27.464529 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, command-id=203b65ea-06bc-4712-85ea-9027c8b80d57) - Closing cursor
[0m23:28:27.464529 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m23:28:27.465530 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=create_hive_metastore_snapshots, idle-time=0.6109561920166016s, acquire-count=1, language=None, thread-identifier=(15424, 9304), compute-name=) - Released connection
[0m23:28:27.465530 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192726944, session-id=e50c8364-813b-4138-917a-218bbf6eb4f2, name=create_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15424, 9304), compute-name=) - Released connection
[0m23:28:27.467530 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15424, 14732), compute-name=) - Creating connection
[0m23:28:27.468531 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m23:28:27.468531 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Acquired connection on thread (15424, 14732), using default compute resource
[0m23:28:27.469532 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.001001596450805664s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Checking idleness
[0m23:28:27.469532 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.001001596450805664s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Retrieving connection
[0m23:28:27.470533 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:28:27.470533 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:28:27.471531 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:28:27.632929 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Connection created
[0m23:28:27.633929 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6ee145d5-788e-4566-b852-7081efa928a3, command-id=Unknown) - Created cursor
[0m23:28:27.866536 [debug] [ThreadPool]: SQL status: OK in 0.4000000059604645 seconds
[0m23:28:27.870538 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6ee145d5-788e-4566-b852-7081efa928a3, command-id=31c0b333-2cdc-4dd1-8413-aac8879817c7) - Closing cursor
[0m23:28:27.877539 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_saleslt, idle-time=0.24360990524291992s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Checking idleness
[0m23:28:27.877539 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_saleslt, idle-time=0.24460983276367188s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Retrieving connection
[0m23:28:27.878541 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_saleslt, idle-time=0.24561119079589844s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Checking idleness
[0m23:28:27.878541 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_saleslt, idle-time=0.24561119079589844s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Retrieving connection
[0m23:28:27.879539 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:28:27.879539 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:28:27.880540 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:28:27.880540 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6ee145d5-788e-4566-b852-7081efa928a3, command-id=Unknown) - Created cursor
[0m23:28:28.105826 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m23:28:28.108827 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6ee145d5-788e-4566-b852-7081efa928a3, command-id=ecea2081-021c-4688-9163-cb479a3a67ba) - Closing cursor
[0m23:28:28.114829 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_saleslt, idle-time=0.4818992614746094s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Checking idleness
[0m23:28:28.115829 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_saleslt, idle-time=0.48289918899536133s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Retrieving connection
[0m23:28:28.115829 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:28:28.116829 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:28:28.116829 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6ee145d5-788e-4566-b852-7081efa928a3, command-id=Unknown) - Created cursor
[0m23:28:28.313805 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m23:28:28.316805 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6ee145d5-788e-4566-b852-7081efa928a3, command-id=1d633705-5551-461c-b560-ffc594402532) - Closing cursor
[0m23:28:28.317806 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15424, 14732), compute-name=) - Released connection
[0m23:28:28.318806 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_saleslt, idle-time=0.0010001659393310547s, acquire-count=0, language=None, thread-identifier=(15424, 14732), compute-name=) - Checking idleness
[0m23:28:28.320806 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m23:28:28.321807 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_snapshots, idle-time=0.0040013790130615234s, acquire-count=0, language=None, thread-identifier=(15424, 14732), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m23:28:28.321807 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_snapshots, idle-time=0.0040013790130615234s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Acquired connection on thread (15424, 14732), using default compute resource
[0m23:28:28.322807 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_snapshots, idle-time=0.005001544952392578s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Checking idleness
[0m23:28:28.323807 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_snapshots, idle-time=0.005001544952392578s, acquire-count=1, language=None, thread-identifier=(15424, 14732), compute-name=) - Retrieving connection
[0m23:28:28.323807 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:28:28.323807 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:28:28.324808 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6ee145d5-788e-4566-b852-7081efa928a3, command-id=Unknown) - Created cursor
[0m23:28:28.533427 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m23:28:28.536427 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6ee145d5-788e-4566-b852-7081efa928a3, command-id=d3948c24-76b5-46b6-8abf-905b4da773c7) - Closing cursor
[0m23:28:28.537428 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1418192905872, session-id=6ee145d5-788e-4566-b852-7081efa928a3, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15424, 14732), compute-name=) - Released connection
[0m23:28:28.538428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A14467020>]}
[0m23:28:28.539428 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=2.2138617038726807s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Checking idleness
[0m23:28:28.539428 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=2.2138617038726807s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Retrieving connection
[0m23:28:28.540428 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=2.21486234664917s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Checking idleness
[0m23:28:28.540428 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=2.21486234664917s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Retrieving connection
[0m23:28:28.541429 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:28:28.541429 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:28:28.542429 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15424, 16280), compute-name=) - Released connection
[0m23:28:28.543429 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:28:28.543429 [info ] [MainThread]: 
[0m23:28:28.549432 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m23:28:28.550431 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m23:28:28.551433 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15424, 16592), compute-name=) - Creating connection
[0m23:28:28.552432 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m23:28:28.553432 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Acquired connection on thread (15424, 16592), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m23:28:28.553432 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m23:28:28.566435 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m23:28:28.685835 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark.address_snapshot"
[0m23:28:28.688836 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.1354048252105713s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:28.688836 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.13640403747558594s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Retrieving connection
[0m23:28:28.689836 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.1374044418334961s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:28.689836 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.1374044418334961s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Retrieving connection
[0m23:28:28.690836 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:28:28.691837 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m23:28:28.691837 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`address_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/address/address_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



  
      
[0m23:28:28.692837 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:28:28.870393 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Connection created
[0m23:28:28.871393 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=Unknown) - Created cursor
[0m23:28:41.796405 [debug] [Thread-1 (]: SQL status: OK in 13.100000381469727 seconds
[0m23:28:41.798406 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=0560b1d4-a638-4b1a-904d-659c8599a93a) - Closing cursor
[0m23:28:41.903061 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m23:28:41.905061 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:28:41.905061 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:28:41.907061 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A32363CB0>]}
[0m23:28:41.908063 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 13.36s]
[0m23:28:41.909063 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m23:28:41.910062 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m23:28:41.910062 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m23:28:41.911063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.address_snapshot, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:41.912063 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m23:28:41.912063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.007001638412475586s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m23:28:41.913063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.008002281188964844s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Acquired connection on thread (15424, 16592), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m23:28:41.914063 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m23:28:41.918064 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m23:28:41.924193 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark.customer_snapshot"
[0m23:28:41.926193 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.021132230758666992s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:41.927194 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.02213263511657715s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Retrieving connection
[0m23:28:41.927194 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m23:28:41.928194 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`customer_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customer/customer_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



  
      
[0m23:28:41.929195 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=Unknown) - Created cursor
[0m23:28:46.171600 [debug] [Thread-1 (]: SQL status: OK in 4.239999771118164 seconds
[0m23:28:46.173599 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=9f9017cd-fde0-4569-b525-9ea749d201a9) - Closing cursor
[0m23:28:46.174600 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m23:28:46.175600 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:28:46.176600 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:28:46.176600 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A332D0230>]}
[0m23:28:46.177600 [info ] [Thread-1 (]: 2 of 7 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 4.27s]
[0m23:28:46.179602 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m23:28:46.179602 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m23:28:46.180601 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m23:28:46.181601 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0050008296966552734s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:46.181601 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m23:28:46.182601 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.006000995635986328s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m23:28:46.182601 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.006000995635986328s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Acquired connection on thread (15424, 16592), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m23:28:46.183602 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m23:28:46.187603 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m23:28:46.193604 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark.customeraddress_snapshot"
[0m23:28:46.194604 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.01800394058227539s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:46.195604 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.019004106521606445s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Retrieving connection
[0m23:28:46.195604 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m23:28:46.196604 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customeraddress/customeraddress_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



  
      
[0m23:28:46.196604 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=Unknown) - Created cursor
[0m23:28:49.615511 [debug] [Thread-1 (]: SQL status: OK in 3.4200000762939453 seconds
[0m23:28:49.616511 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=a9a66aa8-ccbe-4a78-8aec-e4fef51aba33) - Closing cursor
[0m23:28:49.618513 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m23:28:49.620513 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:28:49.621513 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:28:49.621513 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A33229D90>]}
[0m23:28:49.622514 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 3.44s]
[0m23:28:49.624514 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m23:28:49.625515 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m23:28:49.625515 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m23:28:49.627515 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.006002902984619141s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:49.627515 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m23:28:49.628515 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.product_snapshot, idle-time=0.008002519607543945s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m23:28:49.628515 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.product_snapshot, idle-time=0.008002519607543945s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Acquired connection on thread (15424, 16592), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m23:28:49.629515 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m23:28:49.633879 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m23:28:49.639520 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark.product_snapshot"
[0m23:28:49.641521 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.product_snapshot, idle-time=0.02100849151611328s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:49.642521 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.product_snapshot, idle-time=0.022008895874023438s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Retrieving connection
[0m23:28:49.642521 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m23:28:49.643521 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`product_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/product/product_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



  
      
[0m23:28:49.644522 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=Unknown) - Created cursor
[0m23:28:53.865603 [debug] [Thread-1 (]: SQL status: OK in 4.21999979019165 seconds
[0m23:28:53.868603 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=f4b79e49-a79d-4510-b11e-14c331d76493) - Closing cursor
[0m23:28:53.869604 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m23:28:53.870604 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:28:53.871604 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:28:53.871604 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A332C6240>]}
[0m23:28:53.872605 [info ] [Thread-1 (]: 4 of 7 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 4.25s]
[0m23:28:53.874605 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m23:28:53.875605 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m23:28:53.875605 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m23:28:53.876606 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.product_snapshot, idle-time=0.005001544952392578s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:53.877606 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m23:28:53.877606 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m23:28:53.878606 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.007001399993896484s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Acquired connection on thread (15424, 16592), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m23:28:53.878606 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m23:28:53.882607 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m23:28:53.888608 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark.productmodel_snapshot"
[0m23:28:53.890609 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.019004106521606445s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:53.891609 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.020004749298095703s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Retrieving connection
[0m23:28:53.892609 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m23:28:53.892609 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`productmodel_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/productmodel/productmodel_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



  
      
[0m23:28:53.893609 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=Unknown) - Created cursor
[0m23:28:57.224560 [debug] [Thread-1 (]: SQL status: OK in 3.3299999237060547 seconds
[0m23:28:57.225560 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=8fafa512-da76-44c6-93c6-bd9b339cb10a) - Closing cursor
[0m23:28:57.228561 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m23:28:57.228561 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:28:57.229561 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:28:57.230562 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A332EF050>]}
[0m23:28:57.231562 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 3.35s]
[0m23:28:57.232563 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m23:28:57.233562 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m23:28:57.234563 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m23:28:57.235563 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:57.236563 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m23:28:57.237563 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.00800180435180664s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m23:28:57.238563 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.009001731872558594s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Acquired connection on thread (15424, 16592), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m23:28:57.240564 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m23:28:57.245565 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m23:28:57.252567 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m23:28:57.254567 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.025005340576171875s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:28:57.255568 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.026005983352661133s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Retrieving connection
[0m23:28:57.255568 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m23:28:57.256567 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderdetail/salesorderdetail_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



  
      
[0m23:28:57.257568 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=Unknown) - Created cursor
[0m23:29:00.659865 [debug] [Thread-1 (]: SQL status: OK in 3.4000000953674316 seconds
[0m23:29:00.661864 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=9aae9911-6252-4c3c-917d-32d8c48088b0) - Closing cursor
[0m23:29:00.662864 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m23:29:00.663864 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:29:00.664864 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:29:00.664864 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A33313B00>]}
[0m23:29:00.666867 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 3.43s]
[0m23:29:00.667867 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m23:29:00.668867 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m23:29:00.668867 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m23:29:00.669877 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.00501251220703125s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:29:00.670870 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m23:29:00.670870 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0060062408447265625s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m23:29:00.671870 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.007006168365478516s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Acquired connection on thread (15424, 16592), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m23:29:00.671870 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m23:29:00.676872 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m23:29:00.682874 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark.salesorderheader_snapshot"
[0m23:29:00.684874 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.020009517669677734s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Checking idleness
[0m23:29:00.684874 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.020009517669677734s, acquire-count=1, language=sql, thread-identifier=(15424, 16592), compute-name=) - Retrieving connection
[0m23:29:00.685873 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m23:29:00.686873 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



  
      
[0m23:29:00.687875 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=Unknown) - Created cursor
[0m23:29:04.636346 [debug] [Thread-1 (]: SQL status: OK in 3.950000047683716 seconds
[0m23:29:04.638346 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15a67a45-edb6-47f9-a69a-120041687c8a, command-id=d135cfd5-a24b-4a64-b17d-92f84b05ee19) - Closing cursor
[0m23:29:04.639347 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m23:29:04.640347 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:29:04.641347 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1418193360208, session-id=15a67a45-edb6-47f9-a69a-120041687c8a, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15424, 16592), compute-name=) - Released connection
[0m23:29:04.641347 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b26b5f9a-7b66-4f2b-96a0-f3ce3be20ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A332FB5F0>]}
[0m23:29:04.642348 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 3.97s]
[0m23:29:04.644348 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m23:29:04.645348 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=36.10291886329651s, acquire-count=0, language=None, thread-identifier=(15424, 16280), compute-name=) - Checking idleness
[0m23:29:04.646349 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=36.103920221328735s, acquire-count=0, language=None, thread-identifier=(15424, 16280), compute-name=) - Reusing connection previously named master
[0m23:29:04.647348 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=36.10491919517517s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Acquired connection on thread (15424, 16280), using default compute resource
[0m23:29:04.648349 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=36.10592031478882s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Checking idleness
[0m23:29:04.648349 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=None, name=master, idle-time=36.10592031478882s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Retrieving connection
[0m23:29:04.649349 [debug] [MainThread]: On master: ROLLBACK
[0m23:29:04.650349 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:29:04.821500 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=defbfc81-0111-45b0-8f19-677012ba4203, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Connection created
[0m23:29:04.821500 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:29:04.822500 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=defbfc81-0111-45b0-8f19-677012ba4203, name=master, idle-time=0.0020008087158203125s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Checking idleness
[0m23:29:04.822500 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=defbfc81-0111-45b0-8f19-677012ba4203, name=master, idle-time=0.0020008087158203125s, acquire-count=1, language=None, thread-identifier=(15424, 16280), compute-name=) - Retrieving connection
[0m23:29:04.823500 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:29:04.823500 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:29:04.824500 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1418192097104, session-id=defbfc81-0111-45b0-8f19-677012ba4203, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15424, 16280), compute-name=) - Released connection
[0m23:29:04.824500 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:29:04.825500 [debug] [MainThread]: On master: ROLLBACK
[0m23:29:04.825500 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:29:04.826500 [debug] [MainThread]: On master: Close
[0m23:29:04.826500 [debug] [MainThread]: Databricks adapter: Connection(session-id=defbfc81-0111-45b0-8f19-677012ba4203) - Closing connection
[0m23:29:04.884683 [debug] [MainThread]: Connection 'create_hive_metastore_snapshots' was properly closed.
[0m23:29:04.885686 [debug] [MainThread]: On create_hive_metastore_snapshots: ROLLBACK
[0m23:29:04.886686 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:29:04.887687 [debug] [MainThread]: On create_hive_metastore_snapshots: Close
[0m23:29:04.887687 [debug] [MainThread]: Databricks adapter: Connection(session-id=e50c8364-813b-4138-917a-218bbf6eb4f2) - Closing connection
[0m23:29:04.956987 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m23:29:04.957974 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m23:29:04.957974 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:29:04.958976 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m23:29:04.958976 [debug] [MainThread]: Databricks adapter: Connection(session-id=6ee145d5-788e-4566-b852-7081efa928a3) - Closing connection
[0m23:29:05.016982 [debug] [MainThread]: Connection 'snapshot.medallion_spark.salesorderheader_snapshot' was properly closed.
[0m23:29:05.017982 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: ROLLBACK
[0m23:29:05.017982 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:29:05.018982 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: Close
[0m23:29:05.018982 [debug] [MainThread]: Databricks adapter: Connection(session-id=15a67a45-edb6-47f9-a69a-120041687c8a) - Closing connection
[0m23:29:05.089641 [info ] [MainThread]: 
[0m23:29:05.090642 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 38.77 seconds (38.77s).
[0m23:29:05.092642 [debug] [MainThread]: Command end result
[0m23:29:05.135658 [info ] [MainThread]: 
[0m23:29:05.136658 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:29:05.137658 [info ] [MainThread]: 
[0m23:29:05.137658 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m23:29:05.139659 [debug] [MainThread]: Command `dbt snapshot` succeeded at 23:29:05.139659 after 43.22 seconds
[0m23:29:05.140659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A17C93860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A17805520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014A32B17890>]}
[0m23:29:05.140659 [debug] [MainThread]: Flushing usage events
[0m23:39:48.573440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011C02A4E4E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011C02A4FB00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011C02A4F7A0>]}


============================== 23:39:48.577441 | edf41363-00e4-4f75-8f45-964f946133be ==============================
[0m23:39:48.577441 [info ] [MainThread]: Running with dbt=1.8.2
[0m23:39:48.578441 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m23:39:48.597445 [info ] [MainThread]: dbt version: 1.8.2
[0m23:39:48.597445 [info ] [MainThread]: python version: 3.12.4
[0m23:39:48.598448 [info ] [MainThread]: python path: C:\Program Files\Python312\python.exe
[0m23:39:48.599446 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m23:39:48.782502 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:39:48.783493 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:39:48.783493 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:39:50.175807 [info ] [MainThread]: Using profiles dir at C:\Users\shuru\.dbt
[0m23:39:50.176807 [info ] [MainThread]: Using profiles.yml file at C:\Users\shuru\.dbt\profiles.yml
[0m23:39:50.177807 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\shuru\OneDrive\Desktop\database\medallion_spark\dbt_project.yml
[0m23:39:50.178807 [info ] [MainThread]: adapter type: databricks
[0m23:39:50.179808 [info ] [MainThread]: adapter version: 1.8.1
[0m23:39:50.319840 [info ] [MainThread]: Configuration:
[0m23:39:50.321840 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m23:39:50.321840 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m23:39:50.322840 [info ] [MainThread]: Required dependencies:
[0m23:39:50.323840 [debug] [MainThread]: Executing "git --help"
[0m23:39:50.349846 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m23:39:50.350846 [debug] [MainThread]: STDERR: "b''"
[0m23:39:50.351847 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m23:39:50.352847 [info ] [MainThread]: Connection:
[0m23:39:50.353847 [info ] [MainThread]:   host: adb-1655488425849601.1.azuredatabricks.net
[0m23:39:50.354847 [info ] [MainThread]:   http_path: sql/protocolv1/o/1655488425849601/0614-014958-bfyxstku
[0m23:39:50.355847 [info ] [MainThread]:   catalog: hive_metastore
[0m23:39:50.355847 [info ] [MainThread]:   schema: saleslt
[0m23:39:50.356848 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m23:39:50.357848 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1219821031008, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(2616, 1840), compute-name=) - Creating connection
[0m23:39:50.358849 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m23:39:50.359849 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1219821031008, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(2616, 1840), compute-name=) - Acquired connection on thread (2616, 1840), using default compute resource
[0m23:39:50.359849 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1219821031008, session-id=None, name=debug, idle-time=0.0009996891021728516s, acquire-count=1, language=None, thread-identifier=(2616, 1840), compute-name=) - Checking idleness
[0m23:39:50.360849 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1219821031008, session-id=None, name=debug, idle-time=0.0019996166229248047s, acquire-count=1, language=None, thread-identifier=(2616, 1840), compute-name=) - Retrieving connection
[0m23:39:50.360849 [debug] [MainThread]: Using databricks connection "debug"
[0m23:39:50.361848 [debug] [MainThread]: On debug: select 1 as id
[0m23:39:50.361848 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:39:50.783218 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1219821031008, session-id=ac783c4a-7508-4425-84b9-cc8293a3da2b, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(2616, 1840), compute-name=) - Connection created
[0m23:39:50.784217 [debug] [MainThread]: Databricks adapter: Cursor(session-id=ac783c4a-7508-4425-84b9-cc8293a3da2b, command-id=Unknown) - Created cursor
[0m23:39:51.100841 [debug] [MainThread]: SQL status: OK in 0.7400000095367432 seconds
[0m23:39:51.101841 [debug] [MainThread]: Databricks adapter: Cursor(session-id=ac783c4a-7508-4425-84b9-cc8293a3da2b, command-id=faa6a7dc-677b-4110-b35e-dbed4d618cb5) - Closing cursor
[0m23:39:51.102841 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1219821031008, session-id=ac783c4a-7508-4425-84b9-cc8293a3da2b, name=debug, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(2616, 1840), compute-name=) - Released connection
[0m23:39:51.102841 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m23:39:51.103842 [info ] [MainThread]: [32mAll checks passed![0m
[0m23:39:51.105842 [debug] [MainThread]: Command `dbt debug` succeeded at 23:39:51.105842 after 2.68 seconds
[0m23:39:51.105842 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m23:39:51.106842 [debug] [MainThread]: On debug: Close
[0m23:39:51.106842 [debug] [MainThread]: Databricks adapter: Connection(session-id=ac783c4a-7508-4425-84b9-cc8293a3da2b) - Closing connection
[0m23:39:51.259093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011C02FFCE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011C1DEEB9B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011C027C97F0>]}
[0m23:39:51.260096 [debug] [MainThread]: Flushing usage events
[0m23:58:44.736009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028167088470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028167668050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028163F7CB90>]}


============================== 23:58:44.741009 | b649b34b-411a-412b-acde-060a0bf540ea ==============================
[0m23:58:44.741009 [info ] [MainThread]: Running with dbt=1.8.2
[0m23:58:44.742010 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt test', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m23:58:44.925970 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:58:44.926970 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:58:44.926970 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:58:46.629130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b649b34b-411a-412b-acde-060a0bf540ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028102C5B140>]}
[0m23:58:46.690144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b649b34b-411a-412b-acde-060a0bf540ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028102C021E0>]}
[0m23:58:46.691144 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m23:58:46.706147 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m23:58:46.990871 [debug] [MainThread]: Partial parsing enabled: 4 files deleted, 7 files added, 0 files changed.
[0m23:58:46.991871 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\marts\sales\sales.sql
[0m23:58:46.992871 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\staging\bronze.yml
[0m23:58:46.992871 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\marts\customer\customer.sql
[0m23:58:46.993871 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\marts\sales\sales.yml
[0m23:58:46.993871 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\marts\customer\customer.yml
[0m23:58:46.994871 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\marts\product\product.yml
[0m23:58:46.995872 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\marts\product\product.sql
[0m23:58:46.996872 [debug] [MainThread]: Partial parsing: deleted file: medallion_spark://models\example\my_first_dbt_model.sql
[0m23:58:46.996872 [debug] [MainThread]: Partial parsing: deleted file: medallion_spark://models\example\my_second_dbt_model.sql
[0m23:58:47.687791 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `tests` config has been renamed to `data_tests`. Please see
https://docs.getdbt.com/docs/build/data-tests#new-data_tests-syntax for more
information.
[0m23:58:47.689792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'b649b34b-411a-412b-acde-060a0bf540ea', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028103470F20>]}
[0m23:58:47.701796 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models\marts\sales\sales.yml'
[0m23:58:47.883870 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\customer.yml'
[0m23:58:47.903876 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models\marts\product\product.yml'
[0m23:58:47.923881 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.unique_dim_sales_saleOrderID.810c5f247c' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.924881 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_saleOrderID.48ce11e7f3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.926881 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.unique_dim_sales_saleOrderDetailID.343b942405' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.927881 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.927881 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_orderQty.66af966596' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.928882 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_productID.cbf6d34890' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.929881 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_unitPrice.3545b5473a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.930882 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_lineTotal.d55bca27f8' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.931883 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_name.4c7b961f77' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.932882 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_productNumber.3a23a94ddd' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.933883 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_standardCost.d3f58be9a3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.934883 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_listPrice.4ee58b9e3f' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.934883 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_sellStartDate.b44c8ea118' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.935883 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_orderDate.6f6f720ec3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.936883 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_customerID.60b0993af5' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.938888 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_subTotal.bfeb62a487' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.939884 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_taxAmt.94cff67d6a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.941885 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_freight.ca13e04131' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.941885 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_totalDue.920571e023' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m23:58:47.942884 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:58:47.943885 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:58:47.944887 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:58:47.945886 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_customers_AddressId.86b771f63e' (models\marts\customer\customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:58:47.947885 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.unique_dim_products_product_sk.8f20ac7c5b' (models\marts\product\product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m23:58:47.948886 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_products_product_sk.2a2df3e1b9' (models\marts\product\product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m23:58:47.948886 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_products_product_name.991aec73f3' (models\marts\product\product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m23:58:47.949886 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_products_sellstartdate.f97a265a0f' (models\marts\product\product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m23:58:48.037906 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.medallion_spark.example
[0m23:58:48.050909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b649b34b-411a-412b-acde-060a0bf540ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028103417200>]}
[0m23:58:48.214755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b649b34b-411a-412b-acde-060a0bf540ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028103427830>]}
[0m23:58:48.214755 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m23:58:48.216755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b649b34b-411a-412b-acde-060a0bf540ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281035FE3F0>]}
[0m23:58:48.218756 [info ] [MainThread]: 
[0m23:58:48.219756 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m23:58:48.220756 [debug] [MainThread]: Command end result
[0m23:58:48.264766 [debug] [MainThread]: Command `dbt test` succeeded at 23:58:48.264766 after 3.69 seconds
[0m23:58:48.265766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028166BD20C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002810338E6F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281033B69C0>]}
[0m23:58:48.266767 [debug] [MainThread]: Flushing usage events
[0m00:00:01.371922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CB91D1F440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CB90E9CAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CB944FCBC0>]}


============================== 00:00:01.376923 | c1a28d4a-9cbe-4dee-85fa-382a5d5fba9b ==============================
[0m00:00:01.376923 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:00:01.377923 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt test', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:00:01.562274 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:00:01.563274 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:00:01.563274 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:00:03.241324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c1a28d4a-9cbe-4dee-85fa-382a5d5fba9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CB9389C5C0>]}
[0m00:00:03.301341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c1a28d4a-9cbe-4dee-85fa-382a5d5fba9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CB93FD5790>]}
[0m00:00:03.302340 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:00:03.316344 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:00:03.450897 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m00:00:03.451897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'c1a28d4a-9cbe-4dee-85fa-382a5d5fba9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBAF3F1850>]}
[0m00:00:05.401368 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `tests` config has been renamed to `data_tests`. Please see
https://docs.getdbt.com/docs/build/data-tests#new-data_tests-syntax for more
information.
[0m00:00:05.403368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'c1a28d4a-9cbe-4dee-85fa-382a5d5fba9b', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBAF67E780>]}
[0m00:00:05.414370 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\customer.yml'
[0m00:00:05.528396 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models\marts\product\product.yml'
[0m00:00:05.553401 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models\marts\sales\sales.yml'
[0m00:00:05.690433 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m00:00:05.691433 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m00:00:05.692434 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m00:00:05.693433 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_customers_AddressId.86b771f63e' (models\marts\customer\customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m00:00:05.694435 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.unique_dim_products_product_sk.8f20ac7c5b' (models\marts\product\product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m00:00:05.696436 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_products_product_sk.2a2df3e1b9' (models\marts\product\product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m00:00:05.697436 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_products_product_name.991aec73f3' (models\marts\product\product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m00:00:05.698435 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_products_sellstartdate.f97a265a0f' (models\marts\product\product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m00:00:05.699435 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.unique_dim_sales_saleOrderID.810c5f247c' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.701436 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_saleOrderID.48ce11e7f3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.703437 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.unique_dim_sales_saleOrderDetailID.343b942405' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.704443 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.705439 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_orderQty.66af966596' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.706436 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_productID.cbf6d34890' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.707438 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_unitPrice.3545b5473a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.708664 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_lineTotal.d55bca27f8' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.709438 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_name.4c7b961f77' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.710438 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_productNumber.3a23a94ddd' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.711438 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_standardCost.d3f58be9a3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.712439 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_listPrice.4ee58b9e3f' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.713439 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_sellStartDate.b44c8ea118' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.714440 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_orderDate.6f6f720ec3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.716439 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_customerID.60b0993af5' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.716439 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_subTotal.bfeb62a487' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.717439 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_taxAmt.94cff67d6a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.718442 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_freight.ca13e04131' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.719440 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_sales_totalDue.920571e023' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m00:00:05.839466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c1a28d4a-9cbe-4dee-85fa-382a5d5fba9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBAF72DA30>]}
[0m00:00:06.001503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c1a28d4a-9cbe-4dee-85fa-382a5d5fba9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBAF757E00>]}
[0m00:00:06.002504 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:00:06.002504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c1a28d4a-9cbe-4dee-85fa-382a5d5fba9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBAFA4FD40>]}
[0m00:00:06.005505 [info ] [MainThread]: 
[0m00:00:06.006504 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m00:00:06.007506 [debug] [MainThread]: Command end result
[0m00:00:06.052514 [debug] [MainThread]: Command `dbt test` succeeded at 00:00:06.051526 after 4.82 seconds
[0m00:00:06.052514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CB93AF2360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CB90E9CAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBAF6B1BE0>]}
[0m00:00:06.053515 [debug] [MainThread]: Flushing usage events
[0m00:00:24.370950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAC4526960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAC4526660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAC49FC290>]}


============================== 00:00:24.376952 | f72639db-4b2c-4edb-9810-ef7266807aba ==============================
[0m00:00:24.376952 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:00:24.377951 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m00:00:24.631552 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:00:24.631552 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:00:24.632553 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:00:26.280141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f72639db-4b2c-4edb-9810-ef7266807aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DADF348B60>]}
[0m00:00:26.343155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f72639db-4b2c-4edb-9810-ef7266807aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DADF083EF0>]}
[0m00:00:26.344156 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:00:26.360163 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:00:26.605224 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:00:26.606224 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:00:26.669238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f72639db-4b2c-4edb-9810-ef7266807aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DADF432ED0>]}
[0m00:00:26.914074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f72639db-4b2c-4edb-9810-ef7266807aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DADF653B00>]}
[0m00:00:26.914074 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:00:26.915074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f72639db-4b2c-4edb-9810-ef7266807aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DADF4889B0>]}
[0m00:00:26.918074 [info ] [MainThread]: 
[0m00:00:26.919074 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16300, 7628), compute-name=) - Creating connection
[0m00:00:26.920074 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:00:26.921074 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Acquired connection on thread (16300, 7628), using default compute resource
[0m00:00:26.927075 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562449456, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16300, 15484), compute-name=) - Creating connection
[0m00:00:26.928077 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:00:26.929076 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562449456, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16300, 15484), compute-name=) - Acquired connection on thread (16300, 15484), using default compute resource
[0m00:00:26.929076 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562449456, session-id=None, name=list_hive_metastore, idle-time=0.0009992122650146484s, acquire-count=1, language=None, thread-identifier=(16300, 15484), compute-name=) - Checking idleness
[0m00:00:26.930076 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562449456, session-id=None, name=list_hive_metastore, idle-time=0.0009992122650146484s, acquire-count=1, language=None, thread-identifier=(16300, 15484), compute-name=) - Retrieving connection
[0m00:00:26.931078 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:00:26.932078 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:00:26.932888 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:00:27.329099 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562449456, session-id=f6ff8920-f132-46bf-af91-426605f5d1c2, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16300, 15484), compute-name=) - Connection created
[0m00:00:27.330099 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f6ff8920-f132-46bf-af91-426605f5d1c2, command-id=Unknown) - Created cursor
[0m00:00:27.899105 [debug] [ThreadPool]: SQL status: OK in 0.9700000286102295 seconds
[0m00:00:27.903106 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f6ff8920-f132-46bf-af91-426605f5d1c2, command-id=04541858-f9b3-4c70-b240-1bf4ddc6510e) - Closing cursor
[0m00:00:27.904107 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562449456, session-id=f6ff8920-f132-46bf-af91-426605f5d1c2, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16300, 15484), compute-name=) - Released connection
[0m00:00:27.906107 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16300, 9120), compute-name=) - Creating connection
[0m00:00:27.907107 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:00:27.908108 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Acquired connection on thread (16300, 9120), using default compute resource
[0m00:00:27.908108 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Checking idleness
[0m00:00:27.909107 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0009996891021728516s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Retrieving connection
[0m00:00:27.909107 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:00:27.910108 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:00:27.910108 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:00:28.112468 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Connection created
[0m00:00:28.113468 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=Unknown) - Created cursor
[0m00:00:28.316465 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m00:00:28.321467 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=a68c6b35-f08b-4bc5-822c-1e74484ea31b) - Closing cursor
[0m00:00:28.334768 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_snapshots, idle-time=0.22229933738708496s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Checking idleness
[0m00:00:28.335767 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_snapshots, idle-time=0.2232985496520996s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Retrieving connection
[0m00:00:28.336768 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_snapshots, idle-time=0.22429943084716797s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Checking idleness
[0m00:00:28.336768 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_snapshots, idle-time=0.22429943084716797s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Retrieving connection
[0m00:00:28.337768 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:28.337768 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:00:28.338768 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:00:28.338768 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=Unknown) - Created cursor
[0m00:00:28.510308 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:00:28.513308 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=9bf13722-e68f-4bbc-8585-6891c1a121a0) - Closing cursor
[0m00:00:28.521297 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_snapshots, idle-time=0.4088289737701416s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Checking idleness
[0m00:00:28.522298 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_snapshots, idle-time=0.4088289737701416s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Retrieving connection
[0m00:00:28.522298 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:00:28.523299 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:00:28.523299 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=Unknown) - Created cursor
[0m00:00:28.676863 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:00:28.679863 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=dfdf00bc-a1b4-4c79-a894-16944e5ae353) - Closing cursor
[0m00:00:28.680864 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16300, 9120), compute-name=) - Released connection
[0m00:00:28.681863 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_snapshots, idle-time=0.00099945068359375s, acquire-count=0, language=None, thread-identifier=(16300, 9120), compute-name=) - Checking idleness
[0m00:00:28.683865 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:00:28.684864 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_saleslt, idle-time=0.004000186920166016s, acquire-count=0, language=None, thread-identifier=(16300, 9120), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m00:00:28.684864 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_saleslt, idle-time=0.004000186920166016s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Acquired connection on thread (16300, 9120), using default compute resource
[0m00:00:28.685864 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_saleslt, idle-time=0.004999876022338867s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Checking idleness
[0m00:00:28.685864 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_saleslt, idle-time=0.004999876022338867s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Retrieving connection
[0m00:00:28.686865 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:00:28.686865 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:00:28.687865 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=Unknown) - Created cursor
[0m00:00:28.800894 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:00:28.804895 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=3ae222f9-47b0-4d8e-9e31-32b8981c7b79) - Closing cursor
[0m00:00:28.807897 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_saleslt, idle-time=0.12703275680541992s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Checking idleness
[0m00:00:28.808896 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_saleslt, idle-time=0.12703275680541992s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Retrieving connection
[0m00:00:28.808896 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:00:28.809897 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:00:28.809897 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=Unknown) - Created cursor
[0m00:00:29.007835 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m00:00:29.009836 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=e1735f91-0a94-47d6-8f20-8854abbac874) - Closing cursor
[0m00:00:29.012836 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_saleslt, idle-time=0.3319721221923828s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Checking idleness
[0m00:00:29.013836 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_saleslt, idle-time=0.33297181129455566s, acquire-count=1, language=None, thread-identifier=(16300, 9120), compute-name=) - Retrieving connection
[0m00:00:29.014837 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:00:29.014837 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:00:29.015836 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=Unknown) - Created cursor
[0m00:00:29.173519 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:00:29.176519 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, command-id=1c21a943-51bf-4bda-8687-53d10fba3705) - Closing cursor
[0m00:00:29.177520 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2039562446288, session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16300, 9120), compute-name=) - Released connection
[0m00:00:29.180520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f72639db-4b2c-4edb-9810-ef7266807aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DADF48AC60>]}
[0m00:00:29.180520 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=2.260446786880493s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Checking idleness
[0m00:00:29.181521 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=2.2614474296569824s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Retrieving connection
[0m00:00:29.181521 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=2.2614474296569824s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Checking idleness
[0m00:00:29.182997 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=2.262923002243042s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Retrieving connection
[0m00:00:29.183523 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:29.183523 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:00:29.184521 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16300, 7628), compute-name=) - Released connection
[0m00:00:29.185522 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:00:29.186054 [info ] [MainThread]: 
[0m00:00:29.189046 [debug] [Thread-1 (]: Began running node model.medallion_spark.customer
[0m00:00:29.190046 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.customer .................................. [RUN]
[0m00:00:29.191048 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=None, name=model.medallion_spark.customer, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16300, 12484), compute-name=) - Creating connection
[0m00:00:29.192048 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_spark.customer'
[0m00:00:29.192048 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=None, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Acquired connection on thread (16300, 12484), using default compute resource for model '`hive_metastore`.`saleslt`.`customer`'
[0m00:00:29.193048 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.customer
[0m00:00:29.205895 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.customer"
[0m00:00:29.207887 [debug] [Thread-1 (]: Began executing node model.medallion_spark.customer
[0m00:00:29.222890 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:00:29.229891 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=None, name=model.medallion_spark.customer, idle-time=0.03784346580505371s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Checking idleness
[0m00:00:29.230892 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=None, name=model.medallion_spark.customer, idle-time=0.03884458541870117s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Retrieving connection
[0m00:00:29.231894 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=None, name=model.medallion_spark.customer, idle-time=0.039845943450927734s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Checking idleness
[0m00:00:29.233059 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=None, name=model.medallion_spark.customer, idle-time=0.04101109504699707s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Retrieving connection
[0m00:00:29.233894 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:29.234893 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.customer"
[0m00:00:29.234893 [debug] [Thread-1 (]: On model.medallion_spark.customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.customer"} */

      describe extended `hive_metastore`.`saleslt`.`customer`
  
[0m00:00:29.235894 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:00:29.421582 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Connection created
[0m00:00:29.422582 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=Unknown) - Created cursor
[0m00:00:29.603383 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m00:00:29.606384 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=653ccde8-dc75-40c1-a6e0-505cb453d563) - Closing cursor
[0m00:00:29.621387 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`saleslt`.`customer`
[0m00:00:29.627387 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.customer, idle-time=0.20580577850341797s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Checking idleness
[0m00:00:29.628389 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.customer, idle-time=0.20680713653564453s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Retrieving connection
[0m00:00:29.629388 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.customer"
[0m00:00:29.630389 [debug] [Thread-1 (]: On model.medallion_spark.customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.customer"} */
drop table if exists `hive_metastore`.`saleslt`.`customer`
[0m00:00:29.631389 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=Unknown) - Created cursor
[0m00:00:30.532048 [debug] [Thread-1 (]: SQL status: OK in 0.8999999761581421 seconds
[0m00:00:30.533047 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=bf18aa5b-6971-49d9-b74f-a6d962296c3f) - Closing cursor
[0m00:00:30.580057 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.customer"
[0m00:00:30.582058 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.customer, idle-time=1.1604764461517334s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Checking idleness
[0m00:00:30.583058 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.customer, idle-time=1.1614763736724854s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Retrieving connection
[0m00:00:30.584058 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.customer"
[0m00:00:30.584058 [debug] [Thread-1 (]: On model.medallion_spark.customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m00:00:30.585059 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=Unknown) - Created cursor
[0m00:00:36.858256 [debug] [Thread-1 (]: SQL status: OK in 6.269999980926514 seconds
[0m00:00:36.859256 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=c9f68813-d9a7-4f8b-8248-3dcc20902ed4) - Closing cursor
[0m00:00:36.970922 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16300, 12484), compute-name=) - Released connection
[0m00:00:36.971922 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16300, 12484), compute-name=) - Released connection
[0m00:00:36.973923 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f72639db-4b2c-4edb-9810-ef7266807aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DADF7F6B10>]}
[0m00:00:36.973923 [info ] [Thread-1 (]: 1 of 3 OK created sql table model saleslt.customer ............................. [[32mOK[0m in 7.78s]
[0m00:00:36.975923 [debug] [Thread-1 (]: Finished running node model.medallion_spark.customer
[0m00:00:36.976923 [debug] [Thread-1 (]: Began running node model.medallion_spark.product
[0m00:00:36.976923 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.product ................................... [RUN]
[0m00:00:36.977924 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.customer, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(16300, 12484), compute-name=) - Checking idleness
[0m00:00:36.978924 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.customer, now model.medallion_spark.product)
[0m00:00:36.978924 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.007002115249633789s, acquire-count=0, language=sql, thread-identifier=(16300, 12484), compute-name=) - Reusing connection previously named model.medallion_spark.customer
[0m00:00:36.979924 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.008001565933227539s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Acquired connection on thread (16300, 12484), using default compute resource for model '`hive_metastore`.`saleslt`.`product`'
[0m00:00:36.980924 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.product
[0m00:00:36.984925 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.product"
[0m00:00:36.985925 [debug] [Thread-1 (]: Began executing node model.medallion_spark.product
[0m00:00:36.989928 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:00:36.992928 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.021005868911743164s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Checking idleness
[0m00:00:36.993929 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.021005868911743164s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Retrieving connection
[0m00:00:36.993929 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.product"
[0m00:00:36.994928 [debug] [Thread-1 (]: On model.medallion_spark.product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.product"} */

      describe extended `hive_metastore`.`saleslt`.`product`
  
[0m00:00:36.995928 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=Unknown) - Created cursor
[0m00:00:37.246743 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m00:00:37.249744 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=ce6259ec-3f56-4f2e-9a73-f04312a517eb) - Closing cursor
[0m00:00:37.252746 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`saleslt`.`product`
[0m00:00:37.255745 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.28382325172424316s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Checking idleness
[0m00:00:37.255745 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.28382325172424316s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Retrieving connection
[0m00:00:37.256746 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.product"
[0m00:00:37.257746 [debug] [Thread-1 (]: On model.medallion_spark.product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.product"} */
drop table if exists `hive_metastore`.`saleslt`.`product`
[0m00:00:37.257746 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=Unknown) - Created cursor
[0m00:00:37.637605 [debug] [Thread-1 (]: SQL status: OK in 0.3799999952316284 seconds
[0m00:00:37.638605 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=c31a581d-7c58-46ef-af47-087136553008) - Closing cursor
[0m00:00:37.641605 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.product"
[0m00:00:37.643605 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.6716830730438232s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Checking idleness
[0m00:00:37.644606 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.6726834774017334s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Retrieving connection
[0m00:00:37.644606 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.product"
[0m00:00:37.645606 [debug] [Thread-1 (]: On model.medallion_spark.product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m00:00:37.646606 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=Unknown) - Created cursor
[0m00:00:42.046877 [debug] [Thread-1 (]: SQL status: OK in 4.400000095367432 seconds
[0m00:00:42.047879 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=9df54e3d-ee54-4d55-add2-bc6a8c4e8abd) - Closing cursor
[0m00:00:42.050879 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16300, 12484), compute-name=) - Released connection
[0m00:00:42.051880 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16300, 12484), compute-name=) - Released connection
[0m00:00:42.051880 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f72639db-4b2c-4edb-9810-ef7266807aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DADF880F20>]}
[0m00:00:42.052879 [info ] [Thread-1 (]: 2 of 3 OK created sql table model saleslt.product .............................. [[32mOK[0m in 5.07s]
[0m00:00:42.053879 [debug] [Thread-1 (]: Finished running node model.medallion_spark.product
[0m00:00:42.054879 [debug] [Thread-1 (]: Began running node model.medallion_spark.sales
[0m00:00:42.054879 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m00:00:42.055880 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.product, idle-time=0.004000663757324219s, acquire-count=0, language=sql, thread-identifier=(16300, 12484), compute-name=) - Checking idleness
[0m00:00:42.056881 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.product, now model.medallion_spark.sales)
[0m00:00:42.056881 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.sales, idle-time=0.005001068115234375s, acquire-count=0, language=sql, thread-identifier=(16300, 12484), compute-name=) - Reusing connection previously named model.medallion_spark.product
[0m00:00:42.057880 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.sales, idle-time=0.0060002803802490234s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Acquired connection on thread (16300, 12484), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m00:00:42.057880 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.sales
[0m00:00:42.062881 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.sales"
[0m00:00:42.064883 [debug] [Thread-1 (]: Began executing node model.medallion_spark.sales
[0m00:00:42.066882 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:00:42.069884 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.sales"
[0m00:00:42.072884 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.sales, idle-time=0.0200042724609375s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Checking idleness
[0m00:00:42.072884 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.sales, idle-time=0.021004438400268555s, acquire-count=1, language=sql, thread-identifier=(16300, 12484), compute-name=) - Retrieving connection
[0m00:00:42.073884 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.sales"
[0m00:00:42.074884 [debug] [Thread-1 (]: On model.medallion_spark.sales: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m00:00:42.075885 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=Unknown) - Created cursor
[0m00:00:42.522873 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, command-id=Unknown) - Closing cursor
[0m00:00:42.523874 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=0bea021d-85ba-4a33-9768-5b264dc2424b
[0m00:00:42.525874 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16300, 12484), compute-name=) - Released connection
[0m00:00:42.542338 [debug] [Thread-1 (]: Runtime Error in model sales (models\marts\sales\sales.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
[0m00:00:42.543338 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2039564168384, session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16300, 12484), compute-name=) - Released connection
[0m00:00:42.544339 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f72639db-4b2c-4edb-9810-ef7266807aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DADF7BEC60>]}
[0m00:00:42.545338 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model saleslt.sales ............................ [[31mERROR[0m in 0.49s]
[0m00:00:42.546339 [debug] [Thread-1 (]: Finished running node model.medallion_spark.sales
[0m00:00:42.547339 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=13.362818002700806s, acquire-count=0, language=None, thread-identifier=(16300, 7628), compute-name=) - Checking idleness
[0m00:00:42.548339 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=13.363817930221558s, acquire-count=0, language=None, thread-identifier=(16300, 7628), compute-name=) - Reusing connection previously named master
[0m00:00:42.548339 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=13.363817930221558s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Acquired connection on thread (16300, 7628), using default compute resource
[0m00:00:42.549339 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=13.364818334579468s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Checking idleness
[0m00:00:42.549339 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=None, name=master, idle-time=13.364818334579468s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Retrieving connection
[0m00:00:42.550340 [debug] [MainThread]: On master: ROLLBACK
[0m00:00:42.550340 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:00:42.709736 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=ef6daf03-7de0-49a9-85ef-c002bf37406e, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Connection created
[0m00:00:42.710737 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:42.710737 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=ef6daf03-7de0-49a9-85ef-c002bf37406e, name=master, idle-time=0.0009999275207519531s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Checking idleness
[0m00:00:42.711737 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=ef6daf03-7de0-49a9-85ef-c002bf37406e, name=master, idle-time=0.0020003318786621094s, acquire-count=1, language=None, thread-identifier=(16300, 7628), compute-name=) - Retrieving connection
[0m00:00:42.711737 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:42.712737 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:00:42.712737 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2039562450608, session-id=ef6daf03-7de0-49a9-85ef-c002bf37406e, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16300, 7628), compute-name=) - Released connection
[0m00:00:42.713738 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:00:42.713738 [debug] [MainThread]: On master: ROLLBACK
[0m00:00:42.714737 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:42.714737 [debug] [MainThread]: On master: Close
[0m00:00:42.715738 [debug] [MainThread]: Databricks adapter: Connection(session-id=ef6daf03-7de0-49a9-85ef-c002bf37406e) - Closing connection
[0m00:00:42.899541 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:00:42.900542 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:00:42.900542 [debug] [MainThread]: Databricks adapter: Connection(session-id=f6ff8920-f132-46bf-af91-426605f5d1c2) - Closing connection
[0m00:00:42.973567 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:00:42.974568 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:00:42.975568 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:42.975568 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:00:42.976568 [debug] [MainThread]: Databricks adapter: Connection(session-id=9d45fbc1-6cf6-4736-9282-1ce5d55b4057) - Closing connection
[0m00:00:43.036581 [debug] [MainThread]: Connection 'model.medallion_spark.sales' was properly closed.
[0m00:00:43.037582 [debug] [MainThread]: On model.medallion_spark.sales: ROLLBACK
[0m00:00:43.037582 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:43.038582 [debug] [MainThread]: On model.medallion_spark.sales: Close
[0m00:00:43.038582 [debug] [MainThread]: Databricks adapter: Connection(session-id=ca7707aa-2d57-40ea-949b-b3c58d9ace03) - Closing connection
[0m00:00:43.152607 [info ] [MainThread]: 
[0m00:00:43.153608 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 16.23 seconds (16.23s).
[0m00:00:43.154608 [debug] [MainThread]: Command end result
[0m00:00:43.204620 [info ] [MainThread]: 
[0m00:00:43.205620 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:00:43.207623 [info ] [MainThread]: 
[0m00:00:43.209622 [error] [MainThread]:   Runtime Error in model sales (models\marts\sales\sales.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
[0m00:00:43.212623 [info ] [MainThread]: 
[0m00:00:43.214622 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m00:00:43.217624 [debug] [MainThread]: Command `dbt run` failed at 00:00:43.217624 after 18.96 seconds
[0m00:00:43.218624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAC4526960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAC104CAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAC104C410>]}
[0m00:00:43.219624 [debug] [MainThread]: Flushing usage events
[0m00:02:45.570697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7DCBC7EC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7DCAD0650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7DCBC7B00>]}


============================== 00:02:45.575697 | 7ed67f33-7b96-4ecb-b903-bc1635afb570 ==============================
[0m00:02:45.575697 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:02:45.576697 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:02:45.741736 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:02:45.742736 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:02:45.742736 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:02:47.325150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7ed67f33-7b96-4ecb-b903-bc1635afb570', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7DCC3CD70>]}
[0m00:02:47.384163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7ed67f33-7b96-4ecb-b903-bc1635afb570', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7DC89EEA0>]}
[0m00:02:47.385163 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:02:47.398167 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:02:47.624218 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:02:47.625217 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:02:47.696233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7ed67f33-7b96-4ecb-b903-bc1635afb570', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7F7840530>]}
[0m00:02:48.077562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7ed67f33-7b96-4ecb-b903-bc1635afb570', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7F76847A0>]}
[0m00:02:48.078564 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:02:48.079563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7ed67f33-7b96-4ecb-b903-bc1635afb570', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7F76A09B0>]}
[0m00:02:48.083566 [info ] [MainThread]: 
[0m00:02:48.085565 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m00:02:48.088566 [debug] [MainThread]: Command end result
[0m00:02:48.149579 [debug] [MainThread]: Command `dbt test` succeeded at 00:02:48.149579 after 2.74 seconds
[0m00:02:48.150581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7DBE921E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7DC736960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7F7553DD0>]}
[0m00:02:48.151581 [debug] [MainThread]: Flushing usage events
[0m00:03:36.686003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0E1D67080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0DF2887D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0E1C8CCB0>]}


============================== 00:03:36.691003 | e7b0fe52-5d78-4cc6-8482-166fa177edf1 ==============================
[0m00:03:36.691003 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:03:36.692004 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True'}
[0m00:03:36.860041 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:03:36.860041 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:03:36.861042 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:03:38.380385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e7b0fe52-5d78-4cc6-8482-166fa177edf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0E1028710>]}
[0m00:03:38.438398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e7b0fe52-5d78-4cc6-8482-166fa177edf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0E15AC110>]}
[0m00:03:38.439398 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:03:38.452401 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:03:38.681455 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:03:38.681455 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:03:38.744467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e7b0fe52-5d78-4cc6-8482-166fa177edf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0FCC734A0>]}
[0m00:03:39.008527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e7b0fe52-5d78-4cc6-8482-166fa177edf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0E1FDFB00>]}
[0m00:03:39.009527 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:03:39.010530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7b0fe52-5d78-4cc6-8482-166fa177edf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0FCA10530>]}
[0m00:03:39.012527 [info ] [MainThread]: 
[0m00:03:39.013529 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m00:03:39.016528 [debug] [MainThread]: Command end result
[0m00:03:39.063539 [debug] [MainThread]: Command `dbt test` succeeded at 00:03:39.063539 after 2.48 seconds
[0m00:03:39.063539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0FC93EF00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0E1FB7CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0E1DC6780>]}
[0m00:03:39.065542 [debug] [MainThread]: Flushing usage events
[0m00:03:49.631201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FFFEB1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FFFEA660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FD0DCB00>]}


============================== 00:03:49.636203 | 474a6455-b2b7-49bf-8689-63b5129b48a2 ==============================
[0m00:03:49.636203 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:03:49.637202 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:03:49.810242 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:03:49.811242 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:03:49.811242 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:03:51.337587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '474a6455-b2b7-49bf-8689-63b5129b48a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002769B3E6660>]}
[0m00:03:51.398601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '474a6455-b2b7-49bf-8689-63b5129b48a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002769B3E65A0>]}
[0m00:03:51.399601 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:03:51.412603 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:03:51.634653 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:03:51.635654 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:03:51.692666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '474a6455-b2b7-49bf-8689-63b5129b48a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002769B276420>]}
[0m00:03:51.912717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '474a6455-b2b7-49bf-8689-63b5129b48a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002769B524A10>]}
[0m00:03:51.913717 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:03:51.914716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '474a6455-b2b7-49bf-8689-63b5129b48a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002769B5DAC30>]}
[0m00:03:51.916717 [info ] [MainThread]: 
[0m00:03:51.917718 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16344, 18332), compute-name=) - Creating connection
[0m00:03:51.918718 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:03:51.918718 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Acquired connection on thread (16344, 18332), using default compute resource
[0m00:03:51.925719 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708434923552, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16344, 14860), compute-name=) - Creating connection
[0m00:03:51.926720 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:03:51.926720 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708434923552, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16344, 14860), compute-name=) - Acquired connection on thread (16344, 14860), using default compute resource
[0m00:03:51.927721 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708434923552, session-id=None, name=list_hive_metastore, idle-time=0.0010006427764892578s, acquire-count=1, language=None, thread-identifier=(16344, 14860), compute-name=) - Checking idleness
[0m00:03:51.927721 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708434923552, session-id=None, name=list_hive_metastore, idle-time=0.0010006427764892578s, acquire-count=1, language=None, thread-identifier=(16344, 14860), compute-name=) - Retrieving connection
[0m00:03:51.928720 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:03:51.928720 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:03:51.929720 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:03:52.221597 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708434923552, session-id=b0040e9e-a292-43c9-a413-4b1241a4c3c6, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16344, 14860), compute-name=) - Connection created
[0m00:03:52.222597 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b0040e9e-a292-43c9-a413-4b1241a4c3c6, command-id=Unknown) - Created cursor
[0m00:03:52.707666 [debug] [ThreadPool]: SQL status: OK in 0.7799999713897705 seconds
[0m00:03:52.712669 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b0040e9e-a292-43c9-a413-4b1241a4c3c6, command-id=048caae0-a65a-4477-af77-cb847fe9ea4b) - Closing cursor
[0m00:03:52.713668 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708434923552, session-id=b0040e9e-a292-43c9-a413-4b1241a4c3c6, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16344, 14860), compute-name=) - Released connection
[0m00:03:52.715669 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16344, 12564), compute-name=) - Creating connection
[0m00:03:52.716669 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:03:52.717669 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Acquired connection on thread (16344, 12564), using default compute resource
[0m00:03:52.718669 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0010008811950683594s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Checking idleness
[0m00:03:52.718669 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0010008811950683594s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Retrieving connection
[0m00:03:52.719670 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:03:52.720670 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:03:52.720670 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:03:52.940680 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Connection created
[0m00:03:52.941680 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=Unknown) - Created cursor
[0m00:03:53.117699 [debug] [ThreadPool]: SQL status: OK in 0.4000000059604645 seconds
[0m00:03:53.123701 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=497d256b-46d8-44bf-9ab4-1778e7880cd6) - Closing cursor
[0m00:03:53.139705 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_snapshots, idle-time=0.19902443885803223s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Checking idleness
[0m00:03:53.140705 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_snapshots, idle-time=0.20002460479736328s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Retrieving connection
[0m00:03:53.140705 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_snapshots, idle-time=0.20002460479736328s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Checking idleness
[0m00:03:53.141706 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_snapshots, idle-time=0.20102524757385254s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Retrieving connection
[0m00:03:53.142705 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:03:53.142705 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:03:53.142705 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:03:53.143706 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=Unknown) - Created cursor
[0m00:03:53.357392 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m00:03:53.360393 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=238b0e84-321b-465f-888e-941a09cc1c5f) - Closing cursor
[0m00:03:53.366395 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_snapshots, idle-time=0.42571473121643066s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Checking idleness
[0m00:03:53.366395 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_snapshots, idle-time=0.42571473121643066s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Retrieving connection
[0m00:03:53.367395 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:03:53.367395 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:03:53.368396 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=Unknown) - Created cursor
[0m00:03:53.538751 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:03:53.541751 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=1f8fe24e-1c74-4de4-9537-7b273267d691) - Closing cursor
[0m00:03:53.542752 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16344, 12564), compute-name=) - Released connection
[0m00:03:53.543753 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_snapshots, idle-time=0.0010006427764892578s, acquire-count=0, language=None, thread-identifier=(16344, 12564), compute-name=) - Checking idleness
[0m00:03:53.545752 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:03:53.546753 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_saleslt, idle-time=0.0040013790130615234s, acquire-count=0, language=None, thread-identifier=(16344, 12564), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m00:03:53.546753 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_saleslt, idle-time=0.0040013790130615234s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Acquired connection on thread (16344, 12564), using default compute resource
[0m00:03:53.547754 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_saleslt, idle-time=0.00500178337097168s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Checking idleness
[0m00:03:53.548754 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_saleslt, idle-time=0.006001949310302734s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Retrieving connection
[0m00:03:53.549753 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:03:53.549753 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:03:53.550754 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=Unknown) - Created cursor
[0m00:03:53.666760 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m00:03:53.670761 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=e77b9c79-c3b0-4272-b65a-57f516dfd748) - Closing cursor
[0m00:03:53.673762 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_saleslt, idle-time=0.13000965118408203s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Checking idleness
[0m00:03:53.673762 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_saleslt, idle-time=0.1310100555419922s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Retrieving connection
[0m00:03:53.674762 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:03:53.674762 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:03:53.674762 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=Unknown) - Created cursor
[0m00:03:53.854781 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m00:03:53.858782 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=85813c4c-1b71-4cd2-a8aa-2a267fea81ba) - Closing cursor
[0m00:03:53.863783 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_saleslt, idle-time=0.32003116607666016s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Checking idleness
[0m00:03:53.864783 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_saleslt, idle-time=0.3210313320159912s, acquire-count=1, language=None, thread-identifier=(16344, 12564), compute-name=) - Retrieving connection
[0m00:03:53.864783 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:03:53.865784 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:03:53.866784 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=Unknown) - Created cursor
[0m00:03:54.048807 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m00:03:54.052808 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, command-id=4f845a3d-a45c-4b66-ab3b-f226f51268ed) - Closing cursor
[0m00:03:54.053808 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2708437154752, session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16344, 12564), compute-name=) - Released connection
[0m00:03:54.056809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '474a6455-b2b7-49bf-8689-63b5129b48a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002769B1B0560>]}
[0m00:03:54.056809 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=2.1380910873413086s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Checking idleness
[0m00:03:54.057809 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=2.1390912532806396s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Retrieving connection
[0m00:03:54.057809 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=2.1390912532806396s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Checking idleness
[0m00:03:54.058809 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=2.14009165763855s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Retrieving connection
[0m00:03:54.058809 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:03:54.059810 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:03:54.059810 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16344, 18332), compute-name=) - Released connection
[0m00:03:54.060810 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:03:54.061810 [info ] [MainThread]: 
[0m00:03:54.064811 [debug] [Thread-1 (]: Began running node model.medallion_spark.customer
[0m00:03:54.065811 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.customer .................................. [RUN]
[0m00:03:54.066813 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=None, name=model.medallion_spark.customer, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16344, 18292), compute-name=) - Creating connection
[0m00:03:54.066813 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_spark.customer'
[0m00:03:54.067813 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=None, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Acquired connection on thread (16344, 18292), using default compute resource for model '`hive_metastore`.`saleslt`.`customer`'
[0m00:03:54.067813 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.customer
[0m00:03:54.077814 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.customer"
[0m00:03:54.078814 [debug] [Thread-1 (]: Began executing node model.medallion_spark.customer
[0m00:03:54.092817 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:03:54.098819 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=None, name=model.medallion_spark.customer, idle-time=0.03200578689575195s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Checking idleness
[0m00:03:54.099819 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=None, name=model.medallion_spark.customer, idle-time=0.03300595283508301s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Retrieving connection
[0m00:03:54.099819 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=None, name=model.medallion_spark.customer, idle-time=0.03300595283508301s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Checking idleness
[0m00:03:54.100820 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=None, name=model.medallion_spark.customer, idle-time=0.034006595611572266s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Retrieving connection
[0m00:03:54.101819 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:03:54.101819 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.customer"
[0m00:03:54.102821 [debug] [Thread-1 (]: On model.medallion_spark.customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.customer"} */

      describe extended `hive_metastore`.`saleslt`.`customer`
  
[0m00:03:54.103822 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:03:54.279842 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Connection created
[0m00:03:54.280844 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9032a1e-79c4-45a1-877c-113153973f0f, command-id=Unknown) - Created cursor
[0m00:03:54.663872 [debug] [Thread-1 (]: SQL status: OK in 0.5600000023841858 seconds
[0m00:03:54.666873 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9032a1e-79c4-45a1-877c-113153973f0f, command-id=02d66eea-fe28-42dc-a7c2-35447c312283) - Closing cursor
[0m00:03:54.718884 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.customer"
[0m00:03:54.719884 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.customer, idle-time=0.44004273414611816s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Checking idleness
[0m00:03:54.719884 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.customer, idle-time=0.44004273414611816s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Retrieving connection
[0m00:03:54.720885 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.customer"
[0m00:03:54.721885 [debug] [Thread-1 (]: On model.medallion_spark.customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m00:03:54.721885 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9032a1e-79c4-45a1-877c-113153973f0f, command-id=Unknown) - Created cursor
[0m00:03:58.774761 [debug] [Thread-1 (]: SQL status: OK in 4.050000190734863 seconds
[0m00:03:58.776761 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9032a1e-79c4-45a1-877c-113153973f0f, command-id=d27b6570-95a8-4298-a943-2da731583aca) - Closing cursor
[0m00:03:58.815770 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16344, 18292), compute-name=) - Released connection
[0m00:03:58.816771 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16344, 18292), compute-name=) - Released connection
[0m00:03:58.817771 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '474a6455-b2b7-49bf-8689-63b5129b48a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002768017FD10>]}
[0m00:03:58.818771 [info ] [Thread-1 (]: 1 of 3 OK created sql table model saleslt.customer ............................. [[32mOK[0m in 4.75s]
[0m00:03:58.819771 [debug] [Thread-1 (]: Finished running node model.medallion_spark.customer
[0m00:03:58.820771 [debug] [Thread-1 (]: Began running node model.medallion_spark.product
[0m00:03:58.820771 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.product ................................... [RUN]
[0m00:03:58.821771 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.customer, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(16344, 18292), compute-name=) - Checking idleness
[0m00:03:58.822771 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.customer, now model.medallion_spark.product)
[0m00:03:58.822771 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.product, idle-time=0.007001638412475586s, acquire-count=0, language=sql, thread-identifier=(16344, 18292), compute-name=) - Reusing connection previously named model.medallion_spark.customer
[0m00:03:58.823772 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.product, idle-time=0.00800180435180664s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Acquired connection on thread (16344, 18292), using default compute resource for model '`hive_metastore`.`saleslt`.`product`'
[0m00:03:58.823772 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.product
[0m00:03:58.827772 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.product"
[0m00:03:58.828772 [debug] [Thread-1 (]: Began executing node model.medallion_spark.product
[0m00:03:58.831774 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:03:58.833774 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.product, idle-time=0.01800394058227539s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Checking idleness
[0m00:03:58.834774 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.product, idle-time=0.019004344940185547s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Retrieving connection
[0m00:03:58.834774 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.product"
[0m00:03:58.835774 [debug] [Thread-1 (]: On model.medallion_spark.product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.product"} */

      describe extended `hive_metastore`.`saleslt`.`product`
  
[0m00:03:58.835774 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9032a1e-79c4-45a1-877c-113153973f0f, command-id=Unknown) - Created cursor
[0m00:03:59.075409 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m00:03:59.077409 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9032a1e-79c4-45a1-877c-113153973f0f, command-id=a62c3538-4f9b-441d-aaf0-0e6bfce343a4) - Closing cursor
[0m00:03:59.080409 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.product"
[0m00:03:59.082411 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.product, idle-time=0.2656404972076416s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Checking idleness
[0m00:03:59.082411 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.product, idle-time=0.26664137840270996s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Retrieving connection
[0m00:03:59.083411 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.product"
[0m00:03:59.083411 [debug] [Thread-1 (]: On model.medallion_spark.product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m00:03:59.084410 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9032a1e-79c4-45a1-877c-113153973f0f, command-id=Unknown) - Created cursor
[0m00:04:03.395933 [debug] [Thread-1 (]: SQL status: OK in 4.309999942779541 seconds
[0m00:04:03.397931 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9032a1e-79c4-45a1-877c-113153973f0f, command-id=e257cfce-0437-4578-b8c4-4e090f9cfcf1) - Closing cursor
[0m00:04:03.401932 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16344, 18292), compute-name=) - Released connection
[0m00:04:03.403932 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16344, 18292), compute-name=) - Released connection
[0m00:04:03.404933 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '474a6455-b2b7-49bf-8689-63b5129b48a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002769BBE2DB0>]}
[0m00:04:03.405933 [info ] [Thread-1 (]: 2 of 3 OK created sql table model saleslt.product .............................. [[32mOK[0m in 4.58s]
[0m00:04:03.406933 [debug] [Thread-1 (]: Finished running node model.medallion_spark.product
[0m00:04:03.407934 [debug] [Thread-1 (]: Began running node model.medallion_spark.sales
[0m00:04:03.408933 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m00:04:03.410934 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.product, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(16344, 18292), compute-name=) - Checking idleness
[0m00:04:03.410934 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.product, now model.medallion_spark.sales)
[0m00:04:03.411934 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.sales, idle-time=0.008001565933227539s, acquire-count=0, language=sql, thread-identifier=(16344, 18292), compute-name=) - Reusing connection previously named model.medallion_spark.product
[0m00:04:03.412934 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.sales, idle-time=0.009001731872558594s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Acquired connection on thread (16344, 18292), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m00:04:03.413935 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.sales
[0m00:04:03.420936 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.sales"
[0m00:04:03.422937 [debug] [Thread-1 (]: Began executing node model.medallion_spark.sales
[0m00:04:03.426939 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:04:03.430940 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.sales"
[0m00:04:03.432940 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.sales, idle-time=0.0290071964263916s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Checking idleness
[0m00:04:03.433940 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.sales, idle-time=0.030007362365722656s, acquire-count=1, language=sql, thread-identifier=(16344, 18292), compute-name=) - Retrieving connection
[0m00:04:03.434941 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.sales"
[0m00:04:03.435940 [debug] [Thread-1 (]: On model.medallion_spark.sales: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m00:04:03.436941 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9032a1e-79c4-45a1-877c-113153973f0f, command-id=Unknown) - Created cursor
[0m00:04:03.764971 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9032a1e-79c4-45a1-877c-113153973f0f, command-id=Unknown) - Closing cursor
[0m00:04:03.766971 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=04dc561c-0447-46c9-87e3-a75562e831d0
[0m00:04:03.767972 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16344, 18292), compute-name=) - Released connection
[0m00:04:03.774973 [debug] [Thread-1 (]: Runtime Error in model sales (models\marts\sales\sales.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
[0m00:04:03.775974 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2708442329360, session-id=e9032a1e-79c4-45a1-877c-113153973f0f, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16344, 18292), compute-name=) - Released connection
[0m00:04:03.775974 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '474a6455-b2b7-49bf-8689-63b5129b48a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002769BC5BCE0>]}
[0m00:04:03.776975 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model saleslt.sales ............................ [[31mERROR[0m in 0.37s]
[0m00:04:03.777974 [debug] [Thread-1 (]: Finished running node model.medallion_spark.sales
[0m00:04:03.780975 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=9.720164775848389s, acquire-count=0, language=None, thread-identifier=(16344, 18332), compute-name=) - Checking idleness
[0m00:04:03.780975 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=9.721165418624878s, acquire-count=0, language=None, thread-identifier=(16344, 18332), compute-name=) - Reusing connection previously named master
[0m00:04:03.781974 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=9.722164392471313s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Acquired connection on thread (16344, 18332), using default compute resource
[0m00:04:03.781974 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=9.722164392471313s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Checking idleness
[0m00:04:03.782975 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=None, name=master, idle-time=9.723165035247803s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Retrieving connection
[0m00:04:03.782975 [debug] [MainThread]: On master: ROLLBACK
[0m00:04:03.783974 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:04:03.956991 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=1706c6e3-ce1c-4ea8-b2a4-aa9283349203, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Connection created
[0m00:04:03.957991 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:03.957991 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=1706c6e3-ce1c-4ea8-b2a4-aa9283349203, name=master, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Checking idleness
[0m00:04:03.958991 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=1706c6e3-ce1c-4ea8-b2a4-aa9283349203, name=master, idle-time=0.002000570297241211s, acquire-count=1, language=None, thread-identifier=(16344, 18332), compute-name=) - Retrieving connection
[0m00:04:03.958991 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:04:03.959992 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:04:03.959992 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2708438345120, session-id=1706c6e3-ce1c-4ea8-b2a4-aa9283349203, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16344, 18332), compute-name=) - Released connection
[0m00:04:03.960992 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:04:03.960992 [debug] [MainThread]: On master: ROLLBACK
[0m00:04:03.961994 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:03.961994 [debug] [MainThread]: On master: Close
[0m00:04:03.962992 [debug] [MainThread]: Databricks adapter: Connection(session-id=1706c6e3-ce1c-4ea8-b2a4-aa9283349203) - Closing connection
[0m00:04:04.028004 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:04:04.029004 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:04:04.030005 [debug] [MainThread]: Databricks adapter: Connection(session-id=b0040e9e-a292-43c9-a413-4b1241a4c3c6) - Closing connection
[0m00:04:04.093007 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:04:04.094007 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:04:04.094007 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:04.095008 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:04:04.095008 [debug] [MainThread]: Databricks adapter: Connection(session-id=6b3d0032-b9b8-436c-bfa1-a59b1b23d046) - Closing connection
[0m00:04:04.159021 [debug] [MainThread]: Connection 'model.medallion_spark.sales' was properly closed.
[0m00:04:04.160022 [debug] [MainThread]: On model.medallion_spark.sales: ROLLBACK
[0m00:04:04.160022 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:04.161022 [debug] [MainThread]: On model.medallion_spark.sales: Close
[0m00:04:04.161022 [debug] [MainThread]: Databricks adapter: Connection(session-id=e9032a1e-79c4-45a1-877c-113153973f0f) - Closing connection
[0m00:04:04.233035 [info ] [MainThread]: 
[0m00:04:04.235035 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 12.32 seconds (12.32s).
[0m00:04:04.237036 [debug] [MainThread]: Command end result
[0m00:04:04.310051 [info ] [MainThread]: 
[0m00:04:04.311052 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:04:04.312052 [info ] [MainThread]: 
[0m00:04:04.314053 [error] [MainThread]:   Runtime Error in model sales (models\marts\sales\sales.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
[0m00:04:04.315052 [info ] [MainThread]: 
[0m00:04:04.316053 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m00:04:04.319053 [debug] [MainThread]: Command `dbt run` failed at 00:04:04.319053 after 14.80 seconds
[0m00:04:04.320054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027680876870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FFFEB050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FFFEB1A0>]}
[0m00:04:04.321054 [debug] [MainThread]: Flushing usage events
[0m00:07:21.104828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D952ACAE40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D952C64080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D952C65CD0>]}


============================== 00:07:21.109828 | 6f68a2fc-b646-4232-b3e7-7ec86a732694 ==============================
[0m00:07:21.109828 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:07:21.110828 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:07:21.277867 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:07:21.278867 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:07:21.278867 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:07:22.810215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6f68a2fc-b646-4232-b3e7-7ec86a732694', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D96D485BE0>]}
[0m00:07:22.871227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6f68a2fc-b646-4232-b3e7-7ec86a732694', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D96D771EB0>]}
[0m00:07:22.872227 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:07:22.885231 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:07:23.109280 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:07:23.109280 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:07:23.166294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6f68a2fc-b646-4232-b3e7-7ec86a732694', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D96D9D0AA0>]}
[0m00:07:23.391344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6f68a2fc-b646-4232-b3e7-7ec86a732694', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D96DB14D70>]}
[0m00:07:23.392344 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:07:23.392344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f68a2fc-b646-4232-b3e7-7ec86a732694', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D96D7EC440>]}
[0m00:07:23.395346 [info ] [MainThread]: 
[0m00:07:23.396346 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(17984, 18100), compute-name=) - Creating connection
[0m00:07:23.396346 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:07:23.397346 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Acquired connection on thread (17984, 18100), using default compute resource
[0m00:07:23.403348 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358930528, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(17984, 18384), compute-name=) - Creating connection
[0m00:07:23.403348 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:07:23.404349 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358930528, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17984, 18384), compute-name=) - Acquired connection on thread (17984, 18384), using default compute resource
[0m00:07:23.404349 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358930528, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17984, 18384), compute-name=) - Checking idleness
[0m00:07:23.405348 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358930528, session-id=None, name=list_hive_metastore, idle-time=0.0009996891021728516s, acquire-count=1, language=None, thread-identifier=(17984, 18384), compute-name=) - Retrieving connection
[0m00:07:23.405348 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:07:23.406348 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:07:23.407349 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:07:23.707874 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358930528, session-id=efec0200-1fec-4b44-bcc0-5d39d85ecbdd, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17984, 18384), compute-name=) - Connection created
[0m00:07:23.708874 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=efec0200-1fec-4b44-bcc0-5d39d85ecbdd, command-id=Unknown) - Created cursor
[0m00:07:24.268998 [debug] [ThreadPool]: SQL status: OK in 0.8600000143051147 seconds
[0m00:07:24.272999 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=efec0200-1fec-4b44-bcc0-5d39d85ecbdd, command-id=15b00565-d077-48eb-bfd6-966e653b3cba) - Closing cursor
[0m00:07:24.273999 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358930528, session-id=efec0200-1fec-4b44-bcc0-5d39d85ecbdd, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17984, 18384), compute-name=) - Released connection
[0m00:07:24.276000 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(17984, 15456), compute-name=) - Creating connection
[0m00:07:24.277000 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:07:24.278001 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Acquired connection on thread (17984, 15456), using default compute resource
[0m00:07:24.278001 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Checking idleness
[0m00:07:24.279001 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Retrieving connection
[0m00:07:24.279001 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:07:24.280001 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:07:24.280001 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:07:24.525970 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Connection created
[0m00:07:24.526973 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=Unknown) - Created cursor
[0m00:07:24.649006 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m00:07:24.654007 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=197698db-e554-459a-8494-a9692ebb3978) - Closing cursor
[0m00:07:24.669010 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_saleslt, idle-time=0.14204001426696777s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Checking idleness
[0m00:07:24.669010 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_saleslt, idle-time=0.14303946495056152s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Retrieving connection
[0m00:07:24.670011 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_saleslt, idle-time=0.14404034614562988s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Checking idleness
[0m00:07:24.671011 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_saleslt, idle-time=0.14404034614562988s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Retrieving connection
[0m00:07:24.671011 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:24.672012 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:07:24.673012 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:07:24.673012 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=Unknown) - Created cursor
[0m00:07:24.897914 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m00:07:24.899915 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=fb72aed9-1bbf-427d-992b-925ca97fd86c) - Closing cursor
[0m00:07:24.906917 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_saleslt, idle-time=0.3799464702606201s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Checking idleness
[0m00:07:24.906917 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_saleslt, idle-time=0.3809468746185303s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Retrieving connection
[0m00:07:24.907917 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:07:24.907917 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:07:24.908917 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=Unknown) - Created cursor
[0m00:07:25.058667 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:07:25.061667 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=190d4427-8898-4e00-b8fe-7accb0e865e7) - Closing cursor
[0m00:07:25.062668 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17984, 15456), compute-name=) - Released connection
[0m00:07:25.063670 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17984, 15456), compute-name=) - Checking idleness
[0m00:07:25.065668 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:07:25.065668 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_snapshots, idle-time=0.0030002593994140625s, acquire-count=0, language=None, thread-identifier=(17984, 15456), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m00:07:25.066669 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_snapshots, idle-time=0.0040013790130615234s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Acquired connection on thread (17984, 15456), using default compute resource
[0m00:07:25.066669 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_snapshots, idle-time=0.0040013790130615234s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Checking idleness
[0m00:07:25.067670 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_snapshots, idle-time=0.005002260208129883s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Retrieving connection
[0m00:07:25.067670 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:07:25.068670 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:07:25.068670 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=Unknown) - Created cursor
[0m00:07:25.173048 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m00:07:25.176049 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=1019f205-7ff9-4591-94a2-4029983ac010) - Closing cursor
[0m00:07:25.179050 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_snapshots, idle-time=0.11538124084472656s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Checking idleness
[0m00:07:25.179050 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_snapshots, idle-time=0.11638188362121582s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Retrieving connection
[0m00:07:25.180050 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:07:25.180050 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:07:25.181051 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=Unknown) - Created cursor
[0m00:07:25.291065 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:07:25.293064 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=4c9d6404-c8df-49a7-82df-abaa79be45e5) - Closing cursor
[0m00:07:25.297065 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_snapshots, idle-time=0.2333970069885254s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Checking idleness
[0m00:07:25.297065 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_snapshots, idle-time=0.23439669609069824s, acquire-count=1, language=None, thread-identifier=(17984, 15456), compute-name=) - Retrieving connection
[0m00:07:25.298065 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:07:25.299065 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:07:25.299065 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=Unknown) - Created cursor
[0m00:07:25.450079 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:07:25.453079 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, command-id=62877286-42b6-487b-9b47-349740049dbb) - Closing cursor
[0m00:07:25.454080 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2033358748896, session-id=40adc9ab-bb46-4252-8220-e679ae9384ba, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17984, 15456), compute-name=) - Released connection
[0m00:07:25.457079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f68a2fc-b646-4232-b3e7-7ec86a732694', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D96D7EE420>]}
[0m00:07:25.458080 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=2.061734437942505s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Checking idleness
[0m00:07:25.459080 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=2.062734365463257s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Retrieving connection
[0m00:07:25.460080 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=2.0637340545654297s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Checking idleness
[0m00:07:25.461080 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=2.06473445892334s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Retrieving connection
[0m00:07:25.462080 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:25.463080 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:07:25.463080 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17984, 18100), compute-name=) - Released connection
[0m00:07:25.464082 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:07:25.464082 [info ] [MainThread]: 
[0m00:07:25.468082 [debug] [Thread-1 (]: Began running node model.medallion_spark.customer
[0m00:07:25.469081 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.customer .................................. [RUN]
[0m00:07:25.470082 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=None, name=model.medallion_spark.customer, idle-time=0s, acquire-count=0, language=None, thread-identifier=(17984, 13080), compute-name=) - Creating connection
[0m00:07:25.471082 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_spark.customer'
[0m00:07:25.471082 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=None, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Acquired connection on thread (17984, 13080), using default compute resource for model '`hive_metastore`.`saleslt`.`customer`'
[0m00:07:25.472082 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.customer
[0m00:07:25.486086 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.customer"
[0m00:07:25.488087 [debug] [Thread-1 (]: Began executing node model.medallion_spark.customer
[0m00:07:25.513093 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:07:25.523094 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=None, name=model.medallion_spark.customer, idle-time=0.05101346969604492s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Checking idleness
[0m00:07:25.523094 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=None, name=model.medallion_spark.customer, idle-time=0.05201220512390137s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Retrieving connection
[0m00:07:25.524095 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=None, name=model.medallion_spark.customer, idle-time=0.053012847900390625s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Checking idleness
[0m00:07:25.524095 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=None, name=model.medallion_spark.customer, idle-time=0.053012847900390625s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Retrieving connection
[0m00:07:25.525095 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:25.525095 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.customer"
[0m00:07:25.526095 [debug] [Thread-1 (]: On model.medallion_spark.customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.customer"} */

      describe extended `hive_metastore`.`saleslt`.`customer`
  
[0m00:07:25.526095 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:07:25.703109 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Connection created
[0m00:07:25.704108 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, command-id=Unknown) - Created cursor
[0m00:07:26.044168 [debug] [Thread-1 (]: SQL status: OK in 0.5199999809265137 seconds
[0m00:07:26.047168 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, command-id=e696dd05-5553-4f08-ae58-0cef4137c8bc) - Closing cursor
[0m00:07:26.097179 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.customer"
[0m00:07:26.099179 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.customer, idle-time=0.3960704803466797s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Checking idleness
[0m00:07:26.100181 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.customer, idle-time=0.3960704803466797s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Retrieving connection
[0m00:07:26.100181 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.customer"
[0m00:07:26.101181 [debug] [Thread-1 (]: On model.medallion_spark.customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m00:07:26.102180 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, command-id=Unknown) - Created cursor
[0m00:07:29.901737 [debug] [Thread-1 (]: SQL status: OK in 3.799999952316284 seconds
[0m00:07:29.902737 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, command-id=a58c6019-4958-40a4-9b0e-a09d89c84887) - Closing cursor
[0m00:07:29.946748 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17984, 13080), compute-name=) - Released connection
[0m00:07:29.947747 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17984, 13080), compute-name=) - Released connection
[0m00:07:29.949749 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f68a2fc-b646-4232-b3e7-7ec86a732694', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D96DEE7800>]}
[0m00:07:29.950749 [info ] [Thread-1 (]: 1 of 3 OK created sql table model saleslt.customer ............................. [[32mOK[0m in 4.48s]
[0m00:07:29.951749 [debug] [Thread-1 (]: Finished running node model.medallion_spark.customer
[0m00:07:29.952749 [debug] [Thread-1 (]: Began running node model.medallion_spark.product
[0m00:07:29.953749 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.product ................................... [RUN]
[0m00:07:29.954748 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.customer, idle-time=0.007001638412475586s, acquire-count=0, language=sql, thread-identifier=(17984, 13080), compute-name=) - Checking idleness
[0m00:07:29.954748 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.customer, now model.medallion_spark.product)
[0m00:07:29.955749 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.product, idle-time=0.00800180435180664s, acquire-count=0, language=sql, thread-identifier=(17984, 13080), compute-name=) - Reusing connection previously named model.medallion_spark.customer
[0m00:07:29.956749 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.product, idle-time=0.009002685546875s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Acquired connection on thread (17984, 13080), using default compute resource for model '`hive_metastore`.`saleslt`.`product`'
[0m00:07:29.957749 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.product
[0m00:07:29.961751 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.product"
[0m00:07:29.963751 [debug] [Thread-1 (]: Began executing node model.medallion_spark.product
[0m00:07:29.967752 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:07:29.972754 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.product, idle-time=0.025007247924804688s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Checking idleness
[0m00:07:29.973754 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.product, idle-time=0.026007413864135742s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Retrieving connection
[0m00:07:29.974755 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.product"
[0m00:07:29.975753 [debug] [Thread-1 (]: On model.medallion_spark.product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.product"} */

      describe extended `hive_metastore`.`saleslt`.`product`
  
[0m00:07:29.975753 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, command-id=Unknown) - Created cursor
[0m00:07:30.242814 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m00:07:30.245814 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, command-id=37873ab7-ccc5-4f10-8761-6c23b15dad4f) - Closing cursor
[0m00:07:30.248815 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.product"
[0m00:07:30.250816 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.product, idle-time=0.3030698299407959s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Checking idleness
[0m00:07:30.251817 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.product, idle-time=0.3030698299407959s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Retrieving connection
[0m00:07:30.251817 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.product"
[0m00:07:30.252817 [debug] [Thread-1 (]: On model.medallion_spark.product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m00:07:30.252817 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, command-id=Unknown) - Created cursor
[0m00:07:33.959095 [debug] [Thread-1 (]: SQL status: OK in 3.7100000381469727 seconds
[0m00:07:33.961097 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, command-id=3a961fa6-6fdc-43ee-81d7-ed1c2b5c34b3) - Closing cursor
[0m00:07:33.964096 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17984, 13080), compute-name=) - Released connection
[0m00:07:33.965097 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17984, 13080), compute-name=) - Released connection
[0m00:07:33.965097 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f68a2fc-b646-4232-b3e7-7ec86a732694', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D96DF5BE30>]}
[0m00:07:33.966097 [info ] [Thread-1 (]: 2 of 3 OK created sql table model saleslt.product .............................. [[32mOK[0m in 4.01s]
[0m00:07:33.967097 [debug] [Thread-1 (]: Finished running node model.medallion_spark.product
[0m00:07:33.968098 [debug] [Thread-1 (]: Began running node model.medallion_spark.sales
[0m00:07:33.968098 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m00:07:33.969097 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.product, idle-time=0.004000425338745117s, acquire-count=0, language=sql, thread-identifier=(17984, 13080), compute-name=) - Checking idleness
[0m00:07:33.970098 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.product, now model.medallion_spark.sales)
[0m00:07:33.970098 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.sales, idle-time=0.005001544952392578s, acquire-count=0, language=sql, thread-identifier=(17984, 13080), compute-name=) - Reusing connection previously named model.medallion_spark.product
[0m00:07:33.971098 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.sales, idle-time=0.005001544952392578s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Acquired connection on thread (17984, 13080), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m00:07:33.972099 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.sales
[0m00:07:33.979098 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.sales"
[0m00:07:33.980099 [debug] [Thread-1 (]: Began executing node model.medallion_spark.sales
[0m00:07:33.982101 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:07:33.985101 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.sales"
[0m00:07:33.986100 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.sales, idle-time=0.02100372314453125s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Checking idleness
[0m00:07:33.987101 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.sales, idle-time=0.02200460433959961s, acquire-count=1, language=sql, thread-identifier=(17984, 13080), compute-name=) - Retrieving connection
[0m00:07:33.988101 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.sales"
[0m00:07:33.988101 [debug] [Thread-1 (]: On model.medallion_spark.sales: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m00:07:33.989101 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, command-id=Unknown) - Created cursor
[0m00:07:34.313145 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, command-id=Unknown) - Closing cursor
[0m00:07:34.314145 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=c5aa6af9-c62f-4703-aad8-0478be277ecd
[0m00:07:34.315145 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17984, 13080), compute-name=) - Released connection
[0m00:07:34.322147 [debug] [Thread-1 (]: Runtime Error in model sales (models\marts\sales\sales.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
[0m00:07:34.323146 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2033363875456, session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17984, 13080), compute-name=) - Released connection
[0m00:07:34.323146 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f68a2fc-b646-4232-b3e7-7ec86a732694', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D96D6FAA50>]}
[0m00:07:34.324145 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model saleslt.sales ............................ [[31mERROR[0m in 0.35s]
[0m00:07:34.325147 [debug] [Thread-1 (]: Finished running node model.medallion_spark.sales
[0m00:07:34.327147 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=8.864067077636719s, acquire-count=0, language=None, thread-identifier=(17984, 18100), compute-name=) - Checking idleness
[0m00:07:34.328147 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=8.865066528320312s, acquire-count=0, language=None, thread-identifier=(17984, 18100), compute-name=) - Reusing connection previously named master
[0m00:07:34.328147 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=8.865066528320312s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Acquired connection on thread (17984, 18100), using default compute resource
[0m00:07:34.329147 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=8.866066932678223s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Checking idleness
[0m00:07:34.329147 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=None, name=master, idle-time=8.866066932678223s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Retrieving connection
[0m00:07:34.330147 [debug] [MainThread]: On master: ROLLBACK
[0m00:07:34.330147 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:07:34.492166 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=083c88ea-c32b-47f5-a8cf-419be21f784b, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Connection created
[0m00:07:34.493171 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:07:34.493171 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=083c88ea-c32b-47f5-a8cf-419be21f784b, name=master, idle-time=0.0010044574737548828s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Checking idleness
[0m00:07:34.494171 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=083c88ea-c32b-47f5-a8cf-419be21f784b, name=master, idle-time=0.0020046234130859375s, acquire-count=1, language=None, thread-identifier=(17984, 18100), compute-name=) - Retrieving connection
[0m00:07:34.494171 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:34.495169 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:07:34.495169 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2033359929568, session-id=083c88ea-c32b-47f5-a8cf-419be21f784b, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17984, 18100), compute-name=) - Released connection
[0m00:07:34.496170 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:07:34.496170 [debug] [MainThread]: On master: ROLLBACK
[0m00:07:34.497170 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:07:34.497170 [debug] [MainThread]: On master: Close
[0m00:07:34.498170 [debug] [MainThread]: Databricks adapter: Connection(session-id=083c88ea-c32b-47f5-a8cf-419be21f784b) - Closing connection
[0m00:07:34.567197 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:07:34.568200 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:07:34.568200 [debug] [MainThread]: Databricks adapter: Connection(session-id=efec0200-1fec-4b44-bcc0-5d39d85ecbdd) - Closing connection
[0m00:07:34.628222 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:07:34.629222 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:07:34.629222 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:07:34.630223 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:07:34.631223 [debug] [MainThread]: Databricks adapter: Connection(session-id=40adc9ab-bb46-4252-8220-e679ae9384ba) - Closing connection
[0m00:07:34.703801 [debug] [MainThread]: Connection 'model.medallion_spark.sales' was properly closed.
[0m00:07:34.704801 [debug] [MainThread]: On model.medallion_spark.sales: ROLLBACK
[0m00:07:34.704801 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:07:34.704801 [debug] [MainThread]: On model.medallion_spark.sales: Close
[0m00:07:34.705801 [debug] [MainThread]: Databricks adapter: Connection(session-id=1fe78812-3dba-4bba-a976-8b801b00cc0c) - Closing connection
[0m00:07:34.804856 [info ] [MainThread]: 
[0m00:07:34.805857 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 11.41 seconds (11.41s).
[0m00:07:34.807858 [debug] [MainThread]: Command end result
[0m00:07:34.856868 [info ] [MainThread]: 
[0m00:07:34.857870 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:07:34.858869 [info ] [MainThread]: 
[0m00:07:34.859869 [error] [MainThread]:   Runtime Error in model sales (models\marts\sales\sales.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
[0m00:07:34.860869 [info ] [MainThread]: 
[0m00:07:34.860869 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m00:07:34.862870 [debug] [MainThread]: Command `dbt run` failed at 00:07:34.861870 after 13.90 seconds
[0m00:07:34.862870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D96D8AA630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D95280E8A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D95230CEF0>]}
[0m00:07:34.863870 [debug] [MainThread]: Flushing usage events
[0m00:09:39.479377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB45787680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB48289730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB48289610>]}


============================== 00:09:39.483378 | 9be05b15-8d17-43d8-80a9-2903309701f3 ==============================
[0m00:09:39.483378 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:09:39.484379 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m00:09:39.650416 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:09:39.651416 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:09:39.651416 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:09:41.175761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9be05b15-8d17-43d8-80a9-2903309701f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB47F3E7E0>]}
[0m00:09:41.232773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9be05b15-8d17-43d8-80a9-2903309701f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB47EA8C20>]}
[0m00:09:41.233773 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:09:41.246776 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:09:41.465826 [debug] [MainThread]: Partial parsing enabled: 4 files deleted, 4 files added, 0 files changed.
[0m00:09:41.466826 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\marts\customer\dim_customer.sql
[0m00:09:41.466826 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\marts\product\dim_product.sql
[0m00:09:41.467827 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\marts\product\dim_product.yml
[0m00:09:41.467827 [debug] [MainThread]: Partial parsing: added file: medallion_spark://models\marts\customer\dim_customer.yml
[0m00:09:41.468826 [debug] [MainThread]: Partial parsing: deleted file: medallion_spark://models\marts\product\product.sql
[0m00:09:41.468826 [debug] [MainThread]: Partial parsing: deleted file: medallion_spark://models\marts\customer\customer.sql
[0m00:09:41.775896 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `tests` config has been renamed to `data_tests`. Please see
https://docs.getdbt.com/docs/build/data-tests#new-data_tests-syntax for more
information.
[0m00:09:41.776897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '9be05b15-8d17-43d8-80a9-2903309701f3', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB62D26480>]}
[0m00:09:41.794901 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models\marts\product\dim_product.yml'
[0m00:09:41.896923 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\dim_customer.yml'
[0m00:09:41.910927 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.unique_dim_products_product_sk.8f20ac7c5b' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m00:09:41.912927 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_products_product_sk.2a2df3e1b9' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m00:09:41.913928 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_products_product_name.991aec73f3' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m00:09:41.913928 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_products_sellstartdate.f97a265a0f' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m00:09:41.914928 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m00:09:41.915927 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m00:09:41.917930 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m00:09:41.918929 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark.not_null_dim_customers_AddressId.86b771f63e' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m00:09:42.008948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9be05b15-8d17-43d8-80a9-2903309701f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB63238440>]}
[0m00:09:42.151981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9be05b15-8d17-43d8-80a9-2903309701f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB631E2600>]}
[0m00:09:42.152981 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:09:42.153982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9be05b15-8d17-43d8-80a9-2903309701f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB62C2FAA0>]}
[0m00:09:42.155982 [info ] [MainThread]: 
[0m00:09:42.157982 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(18088, 16768), compute-name=) - Creating connection
[0m00:09:42.157982 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:09:42.158982 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Acquired connection on thread (18088, 16768), using default compute resource
[0m00:09:42.164983 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489350016, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(18088, 17744), compute-name=) - Creating connection
[0m00:09:42.165985 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:09:42.166984 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489350016, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(18088, 17744), compute-name=) - Acquired connection on thread (18088, 17744), using default compute resource
[0m00:09:42.166984 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489350016, session-id=None, name=list_hive_metastore, idle-time=0.00099945068359375s, acquire-count=1, language=None, thread-identifier=(18088, 17744), compute-name=) - Checking idleness
[0m00:09:42.167985 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489350016, session-id=None, name=list_hive_metastore, idle-time=0.002000093460083008s, acquire-count=1, language=None, thread-identifier=(18088, 17744), compute-name=) - Retrieving connection
[0m00:09:42.167985 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:09:42.167985 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:09:42.168985 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:09:42.472625 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489350016, session-id=f512157b-716b-42a9-9d5e-ccb183cce219, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(18088, 17744), compute-name=) - Connection created
[0m00:09:42.473625 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f512157b-716b-42a9-9d5e-ccb183cce219, command-id=Unknown) - Created cursor
[0m00:09:43.019540 [debug] [ThreadPool]: SQL status: OK in 0.8500000238418579 seconds
[0m00:09:43.022542 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f512157b-716b-42a9-9d5e-ccb183cce219, command-id=1378f185-3c7a-48ad-9eeb-94497efcce91) - Closing cursor
[0m00:09:43.023541 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489350016, session-id=f512157b-716b-42a9-9d5e-ccb183cce219, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(18088, 17744), compute-name=) - Released connection
[0m00:09:43.026542 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, acquire-count=0, language=None, thread-identifier=(18088, 16444), compute-name=) - Creating connection
[0m00:09:43.026542 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:09:43.027542 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Acquired connection on thread (18088, 16444), using default compute resource
[0m00:09:43.027542 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Checking idleness
[0m00:09:43.028542 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Retrieving connection
[0m00:09:43.028542 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:09:43.029543 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:09:43.029543 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:09:43.270571 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Connection created
[0m00:09:43.271574 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=Unknown) - Created cursor
[0m00:09:43.394812 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m00:09:43.399816 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=a32d809b-facb-4aac-a019-fff2e1cdff03) - Closing cursor
[0m00:09:43.410816 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_snapshots, idle-time=0.140244722366333s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Checking idleness
[0m00:09:43.411817 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_snapshots, idle-time=0.140244722366333s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Retrieving connection
[0m00:09:43.411817 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_snapshots, idle-time=0.14124608039855957s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Checking idleness
[0m00:09:43.412817 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_snapshots, idle-time=0.14224576950073242s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Retrieving connection
[0m00:09:43.412817 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:09:43.413817 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:09:43.413817 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:09:43.414818 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=Unknown) - Created cursor
[0m00:09:43.572822 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:09:43.574824 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=66014c1a-917f-4a64-ad91-1f2f9703db85) - Closing cursor
[0m00:09:43.580824 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_snapshots, idle-time=0.31025242805480957s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Checking idleness
[0m00:09:43.581824 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_snapshots, idle-time=0.31125330924987793s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Retrieving connection
[0m00:09:43.582825 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:09:43.582825 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:09:43.583824 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=Unknown) - Created cursor
[0m00:09:43.756833 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:09:43.759836 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=2691c76a-cf6c-4d2c-bd6a-6a79ce54b1e6) - Closing cursor
[0m00:09:43.760837 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(18088, 16444), compute-name=) - Released connection
[0m00:09:43.760837 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_snapshots, idle-time=0.0010004043579101562s, acquire-count=0, language=None, thread-identifier=(18088, 16444), compute-name=) - Checking idleness
[0m00:09:43.763835 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:09:43.763835 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_saleslt, idle-time=0.003999233245849609s, acquire-count=0, language=None, thread-identifier=(18088, 16444), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m00:09:43.764835 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_saleslt, idle-time=0.004998683929443359s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Acquired connection on thread (18088, 16444), using default compute resource
[0m00:09:43.764835 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_saleslt, idle-time=0.004998683929443359s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Checking idleness
[0m00:09:43.765835 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_saleslt, idle-time=0.005998849868774414s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Retrieving connection
[0m00:09:43.765835 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:09:43.766837 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:09:43.766837 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=Unknown) - Created cursor
[0m00:09:43.881873 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:09:43.885873 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=666c06ee-d4f9-40fa-b7f3-3580ae9d237d) - Closing cursor
[0m00:09:43.888874 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_saleslt, idle-time=0.12903785705566406s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Checking idleness
[0m00:09:43.889874 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_saleslt, idle-time=0.13003778457641602s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Retrieving connection
[0m00:09:43.889874 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:09:43.890874 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:09:43.891874 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=Unknown) - Created cursor
[0m00:09:44.051570 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:09:44.053573 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=87456ee9-2a4a-4dcc-8564-7ffc28b9d82c) - Closing cursor
[0m00:09:44.056571 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_saleslt, idle-time=0.2967345714569092s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Checking idleness
[0m00:09:44.056571 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_saleslt, idle-time=0.2967345714569092s, acquire-count=1, language=None, thread-identifier=(18088, 16444), compute-name=) - Retrieving connection
[0m00:09:44.057571 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:09:44.057571 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:09:44.058572 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=Unknown) - Created cursor
[0m00:09:44.247421 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m00:09:44.250422 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, command-id=eaa3f8cf-d719-419e-b7a4-1641fd6db7fc) - Closing cursor
[0m00:09:44.251422 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2110489351696, session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(18088, 16444), compute-name=) - Released connection
[0m00:09:44.253421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9be05b15-8d17-43d8-80a9-2903309701f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB6323B170>]}
[0m00:09:44.254422 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=2.0954396724700928s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Checking idleness
[0m00:09:44.255423 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=2.0954396724700928s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Retrieving connection
[0m00:09:44.255423 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=2.096440553665161s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Checking idleness
[0m00:09:44.256422 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=2.097440004348755s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Retrieving connection
[0m00:09:44.256422 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:09:44.257423 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:09:44.257423 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(18088, 16768), compute-name=) - Released connection
[0m00:09:44.258422 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:09:44.259422 [info ] [MainThread]: 
[0m00:09:44.262423 [debug] [Thread-1 (]: Began running node model.medallion_spark.dim_customer
[0m00:09:44.263424 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m00:09:44.265425 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0s, acquire-count=0, language=None, thread-identifier=(18088, 12564), compute-name=) - Creating connection
[0m00:09:44.266425 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_spark.dim_customer'
[0m00:09:44.267425 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Acquired connection on thread (18088, 12564), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m00:09:44.268425 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.dim_customer
[0m00:09:44.278427 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.dim_customer"
[0m00:09:44.280429 [debug] [Thread-1 (]: Began executing node model.medallion_spark.dim_customer
[0m00:09:44.292430 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:09:44.345443 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.dim_customer"
[0m00:09:44.347443 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.07901740074157715s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Checking idleness
[0m00:09:44.347443 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.0800173282623291s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Retrieving connection
[0m00:09:44.348443 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.08101820945739746s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Checking idleness
[0m00:09:44.348443 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.08101820945739746s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Retrieving connection
[0m00:09:44.349443 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:09:44.350444 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.dim_customer"
[0m00:09:44.351444 [debug] [Thread-1 (]: On model.medallion_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m00:09:44.351444 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:09:44.531879 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Connection created
[0m00:09:44.531879 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=5c42b495-be06-44a1-95c1-135c67a67278, command-id=Unknown) - Created cursor
[0m00:09:48.662302 [debug] [Thread-1 (]: SQL status: OK in 4.309999942779541 seconds
[0m00:09:48.663296 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=5c42b495-be06-44a1-95c1-135c67a67278, command-id=cca178ab-a7b0-4b75-b38f-020471a2572f) - Closing cursor
[0m00:09:48.700302 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(18088, 12564), compute-name=) - Released connection
[0m00:09:48.701303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(18088, 12564), compute-name=) - Released connection
[0m00:09:48.703304 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9be05b15-8d17-43d8-80a9-2903309701f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB4797FFE0>]}
[0m00:09:48.704303 [info ] [Thread-1 (]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 4.44s]
[0m00:09:48.705304 [debug] [Thread-1 (]: Finished running node model.medallion_spark.dim_customer
[0m00:09:48.706303 [debug] [Thread-1 (]: Began running node model.medallion_spark.dim_product
[0m00:09:48.706303 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m00:09:48.707303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_customer, idle-time=0.006000995635986328s, acquire-count=0, language=sql, thread-identifier=(18088, 12564), compute-name=) - Checking idleness
[0m00:09:48.708304 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.dim_customer, now model.medallion_spark.dim_product)
[0m00:09:48.708304 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_product, idle-time=0.007001399993896484s, acquire-count=0, language=sql, thread-identifier=(18088, 12564), compute-name=) - Reusing connection previously named model.medallion_spark.dim_customer
[0m00:09:48.709304 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_product, idle-time=0.008001327514648438s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Acquired connection on thread (18088, 12564), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m00:09:48.709304 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.dim_product
[0m00:09:48.713305 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.dim_product"
[0m00:09:48.714305 [debug] [Thread-1 (]: Began executing node model.medallion_spark.dim_product
[0m00:09:48.717306 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:09:48.719306 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.dim_product"
[0m00:09:48.721307 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_product, idle-time=0.020004987716674805s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Checking idleness
[0m00:09:48.722307 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_product, idle-time=0.020004987716674805s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Retrieving connection
[0m00:09:48.722307 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.dim_product"
[0m00:09:48.723308 [debug] [Thread-1 (]: On model.medallion_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m00:09:48.723308 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=5c42b495-be06-44a1-95c1-135c67a67278, command-id=Unknown) - Created cursor
[0m00:09:53.030278 [debug] [Thread-1 (]: SQL status: OK in 4.309999942779541 seconds
[0m00:09:53.031278 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=5c42b495-be06-44a1-95c1-135c67a67278, command-id=b67f170d-7c2e-42d3-9b2c-69ec11ad1d2e) - Closing cursor
[0m00:09:53.034279 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(18088, 12564), compute-name=) - Released connection
[0m00:09:53.034279 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(18088, 12564), compute-name=) - Released connection
[0m00:09:53.035280 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9be05b15-8d17-43d8-80a9-2903309701f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB632C0290>]}
[0m00:09:53.036280 [info ] [Thread-1 (]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 4.33s]
[0m00:09:53.037279 [debug] [Thread-1 (]: Finished running node model.medallion_spark.dim_product
[0m00:09:53.037279 [debug] [Thread-1 (]: Began running node model.medallion_spark.sales
[0m00:09:53.038280 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m00:09:53.039281 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.dim_product, idle-time=0.005001544952392578s, acquire-count=0, language=sql, thread-identifier=(18088, 12564), compute-name=) - Checking idleness
[0m00:09:53.039281 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.dim_product, now model.medallion_spark.sales)
[0m00:09:53.040281 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.sales, idle-time=0.006001949310302734s, acquire-count=0, language=sql, thread-identifier=(18088, 12564), compute-name=) - Reusing connection previously named model.medallion_spark.dim_product
[0m00:09:53.040281 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.sales, idle-time=0.006001949310302734s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Acquired connection on thread (18088, 12564), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m00:09:53.041281 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.sales
[0m00:09:53.045282 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.sales"
[0m00:09:53.047282 [debug] [Thread-1 (]: Began executing node model.medallion_spark.sales
[0m00:09:53.049283 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:09:53.052283 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.sales"
[0m00:09:53.053283 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.sales, idle-time=0.019004344940185547s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Checking idleness
[0m00:09:53.054283 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.sales, idle-time=0.0200042724609375s, acquire-count=1, language=sql, thread-identifier=(18088, 12564), compute-name=) - Retrieving connection
[0m00:09:53.054283 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.sales"
[0m00:09:53.055285 [debug] [Thread-1 (]: On model.medallion_spark.sales: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m00:09:53.056286 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=5c42b495-be06-44a1-95c1-135c67a67278, command-id=Unknown) - Created cursor
[0m00:09:53.389371 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=5c42b495-be06-44a1-95c1-135c67a67278, command-id=Unknown) - Closing cursor
[0m00:09:53.390370 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=14a15d46-0293-402e-b656-63b202f1c1df
[0m00:09:53.392372 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(18088, 12564), compute-name=) - Released connection
[0m00:09:53.401374 [debug] [Thread-1 (]: Runtime Error in model sales (models\marts\sales\sales.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
[0m00:09:53.402374 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2110493008256, session-id=5c42b495-be06-44a1-95c1-135c67a67278, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(18088, 12564), compute-name=) - Released connection
[0m00:09:53.403373 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9be05b15-8d17-43d8-80a9-2903309701f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB633588C0>]}
[0m00:09:53.404373 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model saleslt.sales ............................ [[31mERROR[0m in 0.37s]
[0m00:09:53.405375 [debug] [Thread-1 (]: Finished running node model.medallion_spark.sales
[0m00:09:53.407377 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=9.149954795837402s, acquire-count=0, language=None, thread-identifier=(18088, 16768), compute-name=) - Checking idleness
[0m00:09:53.408376 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=9.150953769683838s, acquire-count=0, language=None, thread-identifier=(18088, 16768), compute-name=) - Reusing connection previously named master
[0m00:09:53.409377 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=9.151954174041748s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Acquired connection on thread (18088, 16768), using default compute resource
[0m00:09:53.410377 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=9.1529541015625s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Checking idleness
[0m00:09:53.411377 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=None, name=master, idle-time=9.15395450592041s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Retrieving connection
[0m00:09:53.412377 [debug] [MainThread]: On master: ROLLBACK
[0m00:09:53.413378 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:09:53.591752 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=01443fb6-3afb-42ca-ae04-59b50ee6ea6d, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Connection created
[0m00:09:53.591752 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:09:53.592753 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=01443fb6-3afb-42ca-ae04-59b50ee6ea6d, name=master, idle-time=0.0020020008087158203s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Checking idleness
[0m00:09:53.592753 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=01443fb6-3afb-42ca-ae04-59b50ee6ea6d, name=master, idle-time=0.0020020008087158203s, acquire-count=1, language=None, thread-identifier=(18088, 16768), compute-name=) - Retrieving connection
[0m00:09:53.593753 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:09:53.593753 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:09:53.594753 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2110488987072, session-id=01443fb6-3afb-42ca-ae04-59b50ee6ea6d, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(18088, 16768), compute-name=) - Released connection
[0m00:09:53.594753 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:09:53.595753 [debug] [MainThread]: On master: ROLLBACK
[0m00:09:53.595753 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:09:53.596753 [debug] [MainThread]: On master: Close
[0m00:09:53.596753 [debug] [MainThread]: Databricks adapter: Connection(session-id=01443fb6-3afb-42ca-ae04-59b50ee6ea6d) - Closing connection
[0m00:09:53.663785 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:09:53.664785 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:09:53.664785 [debug] [MainThread]: Databricks adapter: Connection(session-id=f512157b-716b-42a9-9d5e-ccb183cce219) - Closing connection
[0m00:09:53.720561 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:09:53.720561 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:09:53.721562 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:09:53.721562 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:09:53.722562 [debug] [MainThread]: Databricks adapter: Connection(session-id=66cc2aca-52aa-49a4-9149-8e670a936f9f) - Closing connection
[0m00:09:53.782575 [debug] [MainThread]: Connection 'model.medallion_spark.sales' was properly closed.
[0m00:09:53.783575 [debug] [MainThread]: On model.medallion_spark.sales: ROLLBACK
[0m00:09:53.783575 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:09:53.783575 [debug] [MainThread]: On model.medallion_spark.sales: Close
[0m00:09:53.784576 [debug] [MainThread]: Databricks adapter: Connection(session-id=5c42b495-be06-44a1-95c1-135c67a67278) - Closing connection
[0m00:09:53.844286 [info ] [MainThread]: 
[0m00:09:53.845287 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 11.69 seconds (11.69s).
[0m00:09:53.846286 [debug] [MainThread]: Command end result
[0m00:09:53.896298 [info ] [MainThread]: 
[0m00:09:53.897298 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:09:53.898299 [info ] [MainThread]: 
[0m00:09:53.899299 [error] [MainThread]:   Runtime Error in model sales (models\marts\sales\sales.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
[0m00:09:53.900299 [info ] [MainThread]: 
[0m00:09:53.900299 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m00:09:53.902299 [debug] [MainThread]: Command `dbt run` failed at 00:09:53.902299 after 14.57 seconds
[0m00:09:53.902299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB45787680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB478EA150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB47E66360>]}
[0m00:09:53.903300 [debug] [MainThread]: Flushing usage events
[0m00:15:08.563722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236AAA45340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236AAA46DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236AA7646E0>]}


============================== 00:15:08.568722 | c98ba56f-efa7-49c9-ace2-5a11fa05dc30 ==============================
[0m00:15:08.568722 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:15:08.569723 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt snapshot', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m00:15:08.738762 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:15:08.739761 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:15:08.739761 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:15:10.304217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236AB7CCA10>]}
[0m00:15:10.365232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236AB753AA0>]}
[0m00:15:10.366232 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:15:10.381235 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:15:10.610287 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:15:10.611287 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:15:10.667300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C6217590>]}
[0m00:15:10.883349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C64819A0>]}
[0m00:15:10.883349 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:15:10.884349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C6431F10>]}
[0m00:15:10.886349 [info ] [MainThread]: 
[0m00:15:10.887349 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(17052, 16480), compute-name=) - Creating connection
[0m00:15:10.888350 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:15:10.888350 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Acquired connection on thread (17052, 16480), using default compute resource
[0m00:15:10.895351 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434277241008, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(17052, 15788), compute-name=) - Creating connection
[0m00:15:10.896353 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:15:10.896353 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434277241008, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17052, 15788), compute-name=) - Acquired connection on thread (17052, 15788), using default compute resource
[0m00:15:10.897354 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434277241008, session-id=None, name=list_hive_metastore, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(17052, 15788), compute-name=) - Checking idleness
[0m00:15:10.897354 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434277241008, session-id=None, name=list_hive_metastore, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(17052, 15788), compute-name=) - Retrieving connection
[0m00:15:10.898354 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:15:10.898354 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:15:10.899353 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:15:11.261770 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434277241008, session-id=c9cf61b3-0873-471a-aaf5-68c3a4f62ed5, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17052, 15788), compute-name=) - Connection created
[0m00:15:11.262770 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c9cf61b3-0873-471a-aaf5-68c3a4f62ed5, command-id=Unknown) - Created cursor
[0m00:15:11.811820 [debug] [ThreadPool]: SQL status: OK in 0.9100000262260437 seconds
[0m00:15:11.815820 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c9cf61b3-0873-471a-aaf5-68c3a4f62ed5, command-id=7e9b174a-286f-4dc5-b49f-639b6b7abb5c) - Closing cursor
[0m00:15:11.816821 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434277241008, session-id=c9cf61b3-0873-471a-aaf5-68c3a4f62ed5, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17052, 15788), compute-name=) - Released connection
[0m00:15:11.819822 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(17052, 1812), compute-name=) - Creating connection
[0m00:15:11.820823 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:15:11.820823 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Acquired connection on thread (17052, 1812), using default compute resource
[0m00:15:11.821824 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0010006427764892578s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Checking idleness
[0m00:15:11.822821 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0019979476928710938s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Retrieving connection
[0m00:15:11.822821 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:15:11.823823 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:15:11.823823 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:15:12.015650 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Connection created
[0m00:15:12.016650 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=Unknown) - Created cursor
[0m00:15:12.156686 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m00:15:12.160687 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=a5330728-74c5-4f4f-bfae-f1c74794ba5a) - Closing cursor
[0m00:15:12.173690 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_saleslt, idle-time=0.15803956985473633s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Checking idleness
[0m00:15:12.173690 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_saleslt, idle-time=0.15803956985473633s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Retrieving connection
[0m00:15:12.174690 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_saleslt, idle-time=0.15903997421264648s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Checking idleness
[0m00:15:12.174690 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_saleslt, idle-time=0.15903997421264648s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Retrieving connection
[0m00:15:12.175691 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:15:12.175691 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:15:12.176690 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:15:12.176690 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=Unknown) - Created cursor
[0m00:15:12.423897 [debug] [ThreadPool]: SQL status: OK in 0.25 seconds
[0m00:15:12.425897 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=5c888af6-4499-47ad-ae68-5af2547d770c) - Closing cursor
[0m00:15:12.431899 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_saleslt, idle-time=0.41624879837036133s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Checking idleness
[0m00:15:12.432899 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_saleslt, idle-time=0.4172487258911133s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Retrieving connection
[0m00:15:12.432899 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:15:12.433899 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:15:12.433899 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=Unknown) - Created cursor
[0m00:15:12.675925 [debug] [ThreadPool]: SQL status: OK in 0.23999999463558197 seconds
[0m00:15:12.679925 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=176a184c-3135-4f6b-9e79-f71e2e56e514) - Closing cursor
[0m00:15:12.681926 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17052, 1812), compute-name=) - Released connection
[0m00:15:12.682926 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_saleslt, idle-time=0.0009999275207519531s, acquire-count=0, language=None, thread-identifier=(17052, 1812), compute-name=) - Checking idleness
[0m00:15:12.685926 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:15:12.686927 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_snapshots, idle-time=0.005001068115234375s, acquire-count=0, language=None, thread-identifier=(17052, 1812), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m00:15:12.687927 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_snapshots, idle-time=0.005001068115234375s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Acquired connection on thread (17052, 1812), using default compute resource
[0m00:15:12.688928 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_snapshots, idle-time=0.007002115249633789s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Checking idleness
[0m00:15:12.689928 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_snapshots, idle-time=0.008002042770385742s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Retrieving connection
[0m00:15:12.690928 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:15:12.691929 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:15:12.691929 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=Unknown) - Created cursor
[0m00:15:12.786937 [debug] [ThreadPool]: SQL status: OK in 0.09000000357627869 seconds
[0m00:15:12.789938 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=2da3c75d-ae5b-4e52-963e-b6c400ad7f9e) - Closing cursor
[0m00:15:12.792939 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_snapshots, idle-time=0.11101269721984863s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Checking idleness
[0m00:15:12.792939 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_snapshots, idle-time=0.11101269721984863s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Retrieving connection
[0m00:15:12.793939 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:15:12.793939 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:15:12.794937 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=Unknown) - Created cursor
[0m00:15:13.003970 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m00:15:13.006969 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=03633331-94f9-45c8-9f86-d1b72760ffe4) - Closing cursor
[0m00:15:13.009970 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_snapshots, idle-time=0.3270447254180908s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Checking idleness
[0m00:15:13.009970 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_snapshots, idle-time=0.32804393768310547s, acquire-count=1, language=None, thread-identifier=(17052, 1812), compute-name=) - Retrieving connection
[0m00:15:13.010970 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:15:13.010970 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:15:13.011971 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=Unknown) - Created cursor
[0m00:15:13.175989 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:13.180989 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, command-id=afdccc92-0a6e-4153-9543-dde4f7e73974) - Closing cursor
[0m00:15:13.182989 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2434272063616, session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17052, 1812), compute-name=) - Released connection
[0m00:15:13.185989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C64C5EE0>]}
[0m00:15:13.186990 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=2.2986392974853516s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Checking idleness
[0m00:15:13.187990 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=2.2996394634246826s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Retrieving connection
[0m00:15:13.188989 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=2.2996394634246826s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Checking idleness
[0m00:15:13.188989 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=2.3006389141082764s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Retrieving connection
[0m00:15:13.188989 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:15:13.189990 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:15:13.189990 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17052, 16480), compute-name=) - Released connection
[0m00:15:13.190990 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:15:13.191990 [info ] [MainThread]: 
[0m00:15:13.194991 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m00:15:13.196992 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m00:15:13.197992 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(17052, 17804), compute-name=) - Creating connection
[0m00:15:13.198991 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m00:15:13.198991 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Acquired connection on thread (17052, 17804), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m00:15:13.199992 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m00:15:13.208996 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m00:15:13.244002 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.045011281967163086s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:13.245002 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04601120948791504s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:13.245002 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04601120948791504s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:13.246003 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.047011613845825195s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:13.246003 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:15:13.247003 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:13.248003 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:15:13.248003 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:15:13.424022 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Connection created
[0m00:15:13.425024 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:13.761066 [debug] [Thread-1 (]: SQL status: OK in 0.5099999904632568 seconds
[0m00:15:13.764068 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=95bfcdc3-7f34-4c2b-9384-04d3cec2413c) - Closing cursor
[0m00:15:13.801076 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.3760530948638916s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:13.801076 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.37705349922180176s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:13.802076 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:13.803076 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
select * from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:15:13.803076 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:13.986741 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:15:13.990742 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=ae27913a-d023-40ef-b503-9c30d35a5341) - Closing cursor
[0m00:15:13.996744 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5727219581604004s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:13.997744 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5737216472625732s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:13.998744 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:13.998744 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:15:13.999744 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:14.173111 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:14.175111 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=9f5d63cb-5807-4a7c-a27c-7b50e07f5a17) - Closing cursor
[0m00:15:14.181114 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.7570919990539551s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:14.181114 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.7570919990539551s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:14.182114 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:14.183113 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:15:14.183113 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:14.345999 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:15:14.349000 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=213208af-9587-4d3b-aa7b-91240096285e) - Closing cursor
[0m00:15:14.374007 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.9499845504760742s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:14.375007 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.9509844779968262s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:14.375007 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:14.376008 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`address_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:15:14.377008 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:15.051216 [debug] [Thread-1 (]: SQL status: OK in 0.6700000166893005 seconds
[0m00:15:15.052216 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=d0381162-b6a0-4a0a-912b-95e1e41c1dc6) - Closing cursor
[0m00:15:15.055216 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=1.6311941146850586s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:15.056217 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=1.632194995880127s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:15.057217 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:15.058218 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:15:15.059217 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:15.235030 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:15:15.238031 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=d6ed9f96-6ca4-47fd-a01f-9693440b5b7f) - Closing cursor
[0m00:15:15.240032 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=1.816009521484375s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:15.241032 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=1.817009449005127s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:15.241032 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:15.242033 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:15:15.242033 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:15.408096 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:15.411097 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=9f3f457a-fbc7-4b89-86b2-a71f9781ab82) - Closing cursor
[0m00:15:15.415098 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9910755157470703s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:15.416098 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9920759201049805s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:15.416098 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:15.417099 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:15:15.417099 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:15.597961 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:15:15.600960 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=db3c68df-e62d-448c-89a9-46687d85cc4a) - Closing cursor
[0m00:15:15.603960 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=2.1799378395080566s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:15.605961 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=2.180938243865967s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:15.605961 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:15.606961 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:15:15.606961 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:15.777294 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:15.781296 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=d977ccc1-6c08-453d-bce2-7e116285a4f2) - Closing cursor
[0m00:15:15.788297 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=2.364274740219116s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:15.789297 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=2.365274667739868s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:15.789297 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:15.790298 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:15:15.790298 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:15.939321 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:15:15.942323 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=20e5c45e-72d1-4a27-a90d-7bbbd68eb16c) - Closing cursor
[0m00:15:15.952325 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.address_snapshot"
[0m00:15:15.953325 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=2.5293030738830566s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:15.954326 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=2.530303955078125s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:15.954326 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:15.955325 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`address_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:15:15.955325 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:23.073275 [debug] [Thread-1 (]: SQL status: OK in 7.119999885559082 seconds
[0m00:15:23.074275 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=d6e2c0a9-4df1-4382-b897-a765ba4f7ee4) - Closing cursor
[0m00:15:23.142606 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:15:23.148609 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=9.724586486816406s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:23.148609 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=9.724586486816406s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:23.149609 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:15:23.149609 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:15:23.150610 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:23.705242 [debug] [Thread-1 (]: SQL status: OK in 0.550000011920929 seconds
[0m00:15:23.706242 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=3ca1a612-6bca-4216-b438-6a3aa4af7ed3) - Closing cursor
[0m00:15:23.728247 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:15:23.730248 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:23.731248 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:23.732248 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C5E83B90>]}
[0m00:15:23.733248 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 10.53s]
[0m00:15:23.734249 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m00:15:23.734249 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m00:15:23.735249 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m00:15:23.736249 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.address_snapshot, idle-time=0.006001710891723633s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:23.736249 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m00:15:23.737250 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.007002592086791992s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m00:15:23.737250 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.007002592086791992s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Acquired connection on thread (17052, 17804), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m00:15:23.738250 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m00:15:23.742250 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m00:15:23.746253 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.01600503921508789s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:23.747254 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.017006397247314453s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:23.748255 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:15:23.748255 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m00:15:23.749254 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:24.074302 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m00:15:24.077303 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=6c58f241-c444-43ba-ba19-ff76165268eb) - Closing cursor
[0m00:15:24.079303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.34905552864074707s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:24.080303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.3500556945800781s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:24.080303 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:15:24.081304 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:15:24.082304 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:24.469339 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Closing cursor
[0m00:15:24.471340 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`customer` doesn't exist.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_TABLE_NOT_FOUND] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`customer` doesn't exist.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`customer` doesn't exist.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable(DeltaErrors.scala:1450)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable$(DeltaErrors.scala:1449)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.nonExistentDeltaTable(DeltaErrors.scala:3323)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:336)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:321)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.$anonfun$fromV2Relation$1(DeltaAnalysis.scala:1630)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.recordFrameProfile(DeltaAnalysis.scala:1615)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.fromV2Relation(DeltaAnalysis.scala:1629)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:649)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.CTERelationDef.mapChildren(basicLogicalOperators.scala:944)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:716)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:343)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:1917)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1896)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:39)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:107)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:101)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:415)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:408)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:322)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:408)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:341)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:393)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:392)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:576)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1097)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:576)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:572)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:572)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:552)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:522)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:512)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:522)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:595)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:595)
	... 35 more
, operation-id=be82bc6e-85ce-407f-959a-00ced68c7013
[0m00:15:24.474340 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:24.484342 [debug] [Thread-1 (]: Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`customer` doesn't exist.
[0m00:15:24.485343 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:24.486343 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C6599280>]}
[0m00:15:24.487343 [error] [Thread-1 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 0.75s]
[0m00:15:24.488343 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m00:15:24.488343 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:15:24.490344 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m00:15:24.491344 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:24.491344 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m00:15:24.492344 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.007001399993896484s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m00:15:24.492344 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.007001399993896484s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Acquired connection on thread (17052, 17804), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m00:15:24.493344 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m00:15:24.497345 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m00:15:24.502346 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.017003774642944336s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:24.503347 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.018004894256591797s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:24.503347 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:24.504347 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:15:24.505347 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:24.749376 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m00:15:24.753375 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=c1e67c8b-dbf2-4282-b7e3-3f5fd3ebe316) - Closing cursor
[0m00:15:24.757375 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.2720324993133545s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:24.758376 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.27303290367126465s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:24.759377 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:24.759377 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:15:24.760376 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:24.936419 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:15:24.939419 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=fe91e889-0185-4efe-a816-09dcdb36af91) - Closing cursor
[0m00:15:24.942420 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.45607709884643555s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:24.942420 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.4570772647857666s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:24.943419 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:24.943419 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:15:24.944419 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:25.120545 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:15:25.123546 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=5fe44550-b91b-44b2-8cb5-c6541132216f) - Closing cursor
[0m00:15:25.126547 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.6412038803100586s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:25.126547 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.6412038803100586s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:25.127547 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:25.127547 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:15:25.128546 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:25.308606 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:15:25.311607 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=92a7f2e9-ac85-47cb-b51f-509c2f038a7f) - Closing cursor
[0m00:15:25.314607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.8292646408081055s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:25.315607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.8302645683288574s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:25.315607 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:25.316607 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId||'-'||AddressId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customeraddress_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:15:25.317608 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:25.779841 [debug] [Thread-1 (]: SQL status: OK in 0.46000000834465027 seconds
[0m00:15:25.780841 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=3d6873ca-f4aa-4ad9-9558-936db87f8d97) - Closing cursor
[0m00:15:25.782841 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.2974989414215088s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:25.783841 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.2974989414215088s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:25.783841 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:25.784842 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:15:25.784842 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:25.950896 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:25.954897 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=7b2d37fc-f2e4-4d2f-b4a1-0d2b7c08e76e) - Closing cursor
[0m00:15:25.957897 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.4725546836853027s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:25.958897 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.4735548496246338s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:25.959898 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:25.960898 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:15:25.960898 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:26.159209 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:15:26.162211 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=d907c394-e655-4eeb-901d-0d5fa0c5a057) - Closing cursor
[0m00:15:26.164211 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.6788687705993652s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:26.165212 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.6798691749572754s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:26.165212 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:26.166212 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:15:26.167211 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:26.348569 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:15:26.351571 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=edc5817a-bc97-4105-817c-8ee75d0fcfa7) - Closing cursor
[0m00:15:26.354571 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.8682284355163574s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:26.354571 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.8692286014556885s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:26.355572 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:26.356571 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:15:26.356571 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:26.519577 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:15:26.523578 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=25666677-ea7f-4666-8466-fb9e8208d28c) - Closing cursor
[0m00:15:26.526579 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.041236162185669s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:26.527578 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.0422356128692627s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:26.527578 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:26.528579 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:15:26.528579 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:26.696618 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:26.698619 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=4d535b1b-32b2-4fb8-b75e-a6520188d4c3) - Closing cursor
[0m00:15:26.700619 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:26.701619 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.2162768840789795s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:26.702620 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.2162768840789795s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:26.702620 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:26.703620 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customeraddress_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:15:26.703620 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:30.664109 [debug] [Thread-1 (]: SQL status: OK in 3.9600000381469727 seconds
[0m00:15:30.665109 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=42267f51-4c70-49cb-88de-81c6fe0b0104) - Closing cursor
[0m00:15:30.668110 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:15:30.671110 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=6.185767412185669s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:30.672109 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=6.186766862869263s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:30.673110 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:15:30.674111 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:15:30.675111 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:31.142084 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m00:15:31.144084 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=c2818cbc-b846-4007-947d-1cee4d3f9b2f) - Closing cursor
[0m00:15:31.145085 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:15:31.148088 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:31.150086 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:31.151086 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C6975070>]}
[0m00:15:31.153086 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 6.66s]
[0m00:15:31.154087 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:15:31.155087 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m00:15:31.156087 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m00:15:31.157088 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.007001638412475586s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:31.157088 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m00:15:31.158087 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.product_snapshot, idle-time=0.00800180435180664s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m00:15:31.158087 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.product_snapshot, idle-time=0.009001493453979492s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Acquired connection on thread (17052, 17804), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m00:15:31.159087 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m00:15:31.163088 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m00:15:31.169090 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0200045108795166s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:31.170089 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0200045108795166s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:31.170089 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:15:31.171090 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m00:15:31.171090 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:31.520126 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m00:15:31.522126 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=08c32c17-a1e0-4714-8216-708c6a272002) - Closing cursor
[0m00:15:31.525127 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.product_snapshot, idle-time=0.37504076957702637s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:31.525127 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.product_snapshot, idle-time=0.3760416507720947s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:31.526127 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:15:31.526127 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:15:31.527129 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:31.838212 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Closing cursor
[0m00:15:31.840212 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_TABLE_NOT_FOUND] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable(DeltaErrors.scala:1450)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable$(DeltaErrors.scala:1449)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.nonExistentDeltaTable(DeltaErrors.scala:3323)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:336)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:321)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.$anonfun$fromV2Relation$1(DeltaAnalysis.scala:1630)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.recordFrameProfile(DeltaAnalysis.scala:1615)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.fromV2Relation(DeltaAnalysis.scala:1629)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:649)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.CTERelationDef.mapChildren(basicLogicalOperators.scala:944)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:716)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:343)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:1917)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1896)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:39)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:107)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:101)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:415)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:408)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:322)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:408)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:341)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:393)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:392)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:576)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1097)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:576)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:572)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:572)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:552)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:522)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:512)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:522)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:595)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:595)
	... 35 more
, operation-id=089ac13b-3a30-48f6-9518-508c718d7754
[0m00:15:31.842213 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:31.847214 [debug] [Thread-1 (]: Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
[0m00:15:31.848214 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:31.848214 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C6AC0DA0>]}
[0m00:15:31.849215 [error] [Thread-1 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.69s]
[0m00:15:31.850215 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m00:15:31.851215 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m00:15:31.852215 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m00:15:31.853215 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.product_snapshot, idle-time=0.00400090217590332s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:31.853215 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m00:15:31.854216 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0050008296966552734s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m00:15:31.854216 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.00600123405456543s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Acquired connection on thread (17052, 17804), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m00:15:31.855216 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m00:15:31.861219 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m00:15:31.867220 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.019005775451660156s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:31.868220 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.019005775451660156s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:31.868220 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:31.869219 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:15:31.869219 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:32.101247 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:15:32.104249 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=30b38357-e343-4d30-9401-5e8449d9daf9) - Closing cursor
[0m00:15:32.106249 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.25803518295288086s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:32.107249 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.2590346336364746s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:32.107249 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:32.108250 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:15:32.108250 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:32.272263 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:15:32.275264 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=84f0c3ad-8b99-49d8-9cb1-e5ce63d52769) - Closing cursor
[0m00:15:32.277264 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.4290499687194824s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:32.278265 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.4290499687194824s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:32.278265 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:32.279265 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:15:32.279265 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:32.444284 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:32.447285 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=0ba63add-3d24-48fa-baad-c7083592b840) - Closing cursor
[0m00:15:32.450286 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.6020715236663818s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:32.450286 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.6020715236663818s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:32.451286 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:32.451286 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:15:32.452286 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:32.617307 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:32.619306 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=f75d8fe1-2f14-4919-90f1-2aac24822941) - Closing cursor
[0m00:15:32.623307 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.774092435836792s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:32.623307 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.775092601776123s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:32.624308 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:32.625308 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductModelID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productmodel_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:15:32.625308 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:33.436209 [debug] [Thread-1 (]: SQL status: OK in 0.8100000023841858 seconds
[0m00:15:33.437210 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=0a4def27-8bc5-437e-9106-114a5897bbc5) - Closing cursor
[0m00:15:33.440210 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.5919954776763916s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:33.440210 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.5919954776763916s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:33.441209 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:33.441209 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:15:33.442211 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:33.609456 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:33.612457 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=d8407c2d-defe-454c-9ead-564acd2a7614) - Closing cursor
[0m00:15:33.614457 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.7662429809570312s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:33.615457 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.7672429084777832s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:33.615457 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:33.616458 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:15:33.616458 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:33.802454 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:15:33.806454 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=8a39c8af-51be-4f91-b50d-a9bfcf14d300) - Closing cursor
[0m00:15:33.810455 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.9622409343719482s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:33.811456 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.9632415771484375s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:33.811456 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:33.812456 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:15:33.813456 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:33.969062 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:15:33.972063 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=497c393e-9088-4da9-b3d7-dde2fc986b56) - Closing cursor
[0m00:15:33.974064 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.1258492469787598s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:33.975063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.1268489360809326s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:33.975063 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:33.976064 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:15:33.976064 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:34.179519 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:15:34.182521 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=8764896b-6d2a-48e8-a0cb-c9d93eb2c46f) - Closing cursor
[0m00:15:34.184521 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.33630633354187s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:34.185521 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.337306022644043s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:34.185521 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:34.186521 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:15:34.186521 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:34.362516 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:15:34.366516 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=69705e17-8833-43c7-aba3-227015054948) - Closing cursor
[0m00:15:34.368516 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:34.369516 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.521301507949829s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:34.370518 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.522303581237793s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:34.370518 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:34.371517 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productmodel_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:15:34.371517 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:38.256994 [debug] [Thread-1 (]: SQL status: OK in 3.880000114440918 seconds
[0m00:15:38.257995 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=4cca2bca-6634-4b11-a355-e065ebef1ded) - Closing cursor
[0m00:15:38.260996 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:15:38.261996 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=6.413781642913818s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:38.261996 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=6.413781642913818s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:38.262997 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:15:38.262997 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:15:38.263996 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:38.753058 [debug] [Thread-1 (]: SQL status: OK in 0.49000000953674316 seconds
[0m00:15:38.754058 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=6c1b7b28-d781-4a24-9e24-62635ef23dd8) - Closing cursor
[0m00:15:38.756058 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:15:38.757060 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:38.757060 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:38.758059 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C69D00E0>]}
[0m00:15:38.759059 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 6.91s]
[0m00:15:38.760059 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m00:15:38.761059 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:15:38.761059 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m00:15:38.762062 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.005002498626708984s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:38.763060 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m00:15:38.763060 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.00599980354309082s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m00:15:38.764061 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0070018768310546875s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Acquired connection on thread (17052, 17804), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m00:15:38.765060 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:15:38.769062 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:15:38.773063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.01600360870361328s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:38.773063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.01600360870361328s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:38.774063 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:38.775064 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:15:38.775064 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:39.009076 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:15:39.012076 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=c299c7b9-1fda-4be5-b354-16cb8cbbe2a1) - Closing cursor
[0m00:15:39.014076 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.257016658782959s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:39.015076 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.25801682472229004s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:39.015076 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:39.016077 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
select * from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:15:39.017076 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:39.322120 [debug] [Thread-1 (]: SQL status: OK in 0.3100000023841858 seconds
[0m00:15:39.329121 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=a7fc4b51-356e-44fc-9c05-dc751e156104) - Closing cursor
[0m00:15:39.331122 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.5740628242492676s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:39.332123 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.5750629901885986s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:39.333122 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:39.333122 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:15:39.334123 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:39.562979 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:15:39.565979 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=3b548d83-25f5-4352-98b4-0b7ee329deac) - Closing cursor
[0m00:15:39.568980 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.8119204044342041s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:39.569980 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.812920331954956s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:39.569980 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:39.570980 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:15:39.571981 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:39.754033 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:15:39.757034 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=cd0e1b22-2b0f-4927-8406-33234dac5ce2) - Closing cursor
[0m00:15:39.762036 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.004976749420166s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:39.763036 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.0059762001037598s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:39.763036 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:39.764036 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderDetailID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:15:39.765036 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:40.186169 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m00:15:40.187170 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=d55303ee-4e4b-4c75-b5ff-018f81e5b97b) - Closing cursor
[0m00:15:40.189170 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.43211030960083s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:40.190170 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.4331107139587402s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:40.191171 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:40.191171 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:15:40.192171 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:40.362226 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:40.364227 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=8a2407a6-d58f-4992-9c3a-7748b0379479) - Closing cursor
[0m00:15:40.367227 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.6101670265197754s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:40.368227 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.6101670265197754s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:40.368227 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:40.369227 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:15:40.369227 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:40.543260 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:40.546260 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=7b4e1d4c-12ad-4988-bdb7-46b94c96df0d) - Closing cursor
[0m00:15:40.549262 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7922019958496094s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:40.549262 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7922019958496094s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:40.550261 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:40.550261 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:15:40.551262 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:40.724282 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:15:40.727283 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=a57094e8-2eda-470f-9cbc-dc2f7700aa4b) - Closing cursor
[0m00:15:40.731284 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.9732246398925781s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:40.731284 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.9742238521575928s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:40.732284 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:40.732284 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:15:40.733284 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:40.918302 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:15:40.921302 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=b6ffb7ce-d8bb-4836-9845-580b6c33cd63) - Closing cursor
[0m00:15:40.925302 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=2.1672427654266357s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:40.925302 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=2.1682426929473877s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:40.926302 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:40.926302 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:15:40.927303 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:41.076342 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:15:41.079343 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=2d95b9cd-7850-495f-bf31-9bab50b1a46d) - Closing cursor
[0m00:15:41.081343 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:41.082343 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=2.3252828121185303s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:41.083343 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=2.3262832164764404s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:41.083343 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:41.084343 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:15:41.084343 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:45.269823 [debug] [Thread-1 (]: SQL status: OK in 4.190000057220459 seconds
[0m00:15:45.271825 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=f965e16e-427d-4b31-a797-ba42ea544cf9) - Closing cursor
[0m00:15:45.274823 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:15:45.275823 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=6.518763780593872s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:45.275823 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=6.518763780593872s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:45.276823 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:15:45.276823 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:15:45.277824 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:45.742485 [debug] [Thread-1 (]: SQL status: OK in 0.46000000834465027 seconds
[0m00:15:45.743485 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=2be3964d-b730-40d7-a5e5-40e785daab9e) - Closing cursor
[0m00:15:45.744486 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:15:45.745486 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:45.746486 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:45.747486 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C69A0320>]}
[0m00:15:45.747486 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 6.98s]
[0m00:15:45.749487 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:15:45.749487 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:15:45.750487 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m00:15:45.751487 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.00400090217590332s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:45.751487 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m00:15:45.752488 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0050013065338134766s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:15:45.752488 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.006001710891723633s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Acquired connection on thread (17052, 17804), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m00:15:45.753488 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:15:45.757491 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:15:45.761491 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.015005111694335938s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:45.762491 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.016005516052246094s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:45.763492 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:45.763492 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:15:45.764491 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:46.126931 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m00:15:46.129931 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=f2b360ec-4446-4332-a14d-5288fcc582e2) - Closing cursor
[0m00:15:46.132932 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.3864459991455078s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:46.132932 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.3864459991455078s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:46.133932 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:46.133932 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
select * from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:15:46.134932 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:46.334958 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:15:46.340960 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=5e3a6a81-6530-4a6e-b2b7-f2988c7dfac1) - Closing cursor
[0m00:15:46.344962 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.5984761714935303s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:46.345963 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.5984761714935303s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:46.345963 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:46.346961 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:15:46.346961 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:46.526977 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:15:46.529978 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=e1f74d49-0bd4-4728-b499-939316fe73ae) - Closing cursor
[0m00:15:46.532978 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.786492109298706s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:46.533978 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.786492109298706s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:46.533978 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:46.534978 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:15:46.534978 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:46.738027 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:15:46.741028 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=1bd7b13e-de49-4548-b51d-70633dfdd918) - Closing cursor
[0m00:15:46.744028 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.997542142868042s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:46.745029 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.9985427856445312s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:46.746029 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:46.747028 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:15:46.748029 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:47.300087 [debug] [Thread-1 (]: SQL status: OK in 0.550000011920929 seconds
[0m00:15:47.302087 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=908a4f05-c55e-4c7b-9392-e054a895931c) - Closing cursor
[0m00:15:47.304088 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.5576021671295166s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:47.304088 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.5576021671295166s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:47.305088 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:47.305088 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:15:47.306089 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:47.492086 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:15:47.495086 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=903e06a2-55d4-4bcd-bcf5-d21916b4544c) - Closing cursor
[0m00:15:47.498088 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.7516014575958252s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:47.498088 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.7516014575958252s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:47.499087 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:47.499087 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:15:47.500088 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:47.713120 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:15:47.715121 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=d7791280-bbda-4e0f-81ca-2675fab1cee4) - Closing cursor
[0m00:15:47.718121 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.9716355800628662s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:47.719122 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.9716355800628662s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:47.719122 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:47.720122 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:15:47.720122 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:47.905132 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:15:47.908133 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=7b3d1c94-35d2-44ec-9e38-5f871f7212e4) - Closing cursor
[0m00:15:47.911134 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.1646478176116943s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:47.911134 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.1646478176116943s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:47.912134 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:47.912134 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:15:47.913135 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:48.110157 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:15:48.113158 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=0ba2a539-5409-41bc-b9ea-dcec26b0b189) - Closing cursor
[0m00:15:48.116159 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.3686726093292236s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:48.116159 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.3696727752685547s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:48.117159 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:48.117159 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:15:48.118159 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:48.323195 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:15:48.327197 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=edb74b48-4c39-4ba7-96e6-e0aba1181937) - Closing cursor
[0m00:15:48.329197 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:48.330198 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.583712100982666s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:48.330198 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.583712100982666s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:48.331198 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:48.331198 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderheader_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:15:48.332198 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:52.683736 [debug] [Thread-1 (]: SQL status: OK in 4.349999904632568 seconds
[0m00:15:52.685734 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=1e7eeeab-8f40-4d9f-a4b6-40f8f06c93d8) - Closing cursor
[0m00:15:52.689735 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:15:52.690736 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=6.944249629974365s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Checking idleness
[0m00:15:52.691736 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=6.945249795913696s, acquire-count=1, language=sql, thread-identifier=(17052, 17804), compute-name=) - Retrieving connection
[0m00:15:52.691736 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:15:52.692736 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:15:52.693736 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=Unknown) - Created cursor
[0m00:15:53.208714 [debug] [Thread-1 (]: SQL status: OK in 0.5199999809265137 seconds
[0m00:15:53.209715 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, command-id=b2a3f65d-4505-4d77-a2f4-eb087243b93b) - Closing cursor
[0m00:15:53.211716 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:15:53.212715 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:53.212715 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2434282986512, session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(17052, 17804), compute-name=) - Released connection
[0m00:15:53.213715 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c98ba56f-efa7-49c9-ace2-5a11fa05dc30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C6A7F320>]}
[0m00:15:53.214716 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 7.46s]
[0m00:15:53.215716 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:15:53.216715 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=40.02672600746155s, acquire-count=0, language=None, thread-identifier=(17052, 16480), compute-name=) - Checking idleness
[0m00:15:53.217716 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=40.027727127075195s, acquire-count=0, language=None, thread-identifier=(17052, 16480), compute-name=) - Reusing connection previously named master
[0m00:15:53.217716 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=40.027727127075195s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Acquired connection on thread (17052, 16480), using default compute resource
[0m00:15:53.218717 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=40.028727293014526s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Checking idleness
[0m00:15:53.218717 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=None, name=master, idle-time=40.028727293014526s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Retrieving connection
[0m00:15:53.219717 [debug] [MainThread]: On master: ROLLBACK
[0m00:15:53.219717 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:15:53.384157 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=5b4f1c98-1158-468c-b8a9-935cbfa1e16e, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Connection created
[0m00:15:53.384157 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:15:53.385157 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=5b4f1c98-1158-468c-b8a9-935cbfa1e16e, name=master, idle-time=0.0010006427764892578s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Checking idleness
[0m00:15:53.386157 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=5b4f1c98-1158-468c-b8a9-935cbfa1e16e, name=master, idle-time=0.0010006427764892578s, acquire-count=1, language=None, thread-identifier=(17052, 16480), compute-name=) - Retrieving connection
[0m00:15:53.386157 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:15:53.386157 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:15:53.387157 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2434278927408, session-id=5b4f1c98-1158-468c-b8a9-935cbfa1e16e, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17052, 16480), compute-name=) - Released connection
[0m00:15:53.388158 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:15:53.388158 [debug] [MainThread]: On master: ROLLBACK
[0m00:15:53.388158 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:15:53.389158 [debug] [MainThread]: On master: Close
[0m00:15:53.389158 [debug] [MainThread]: Databricks adapter: Connection(session-id=5b4f1c98-1158-468c-b8a9-935cbfa1e16e) - Closing connection
[0m00:15:53.451162 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:15:53.452163 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:15:53.453163 [debug] [MainThread]: Databricks adapter: Connection(session-id=c9cf61b3-0873-471a-aaf5-68c3a4f62ed5) - Closing connection
[0m00:15:53.508803 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:15:53.508803 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:15:53.509803 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:15:53.509803 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:15:53.510803 [debug] [MainThread]: Databricks adapter: Connection(session-id=be632ff5-1d7e-42bf-aa65-732ad2424c94) - Closing connection
[0m00:15:53.563175 [debug] [MainThread]: Connection 'snapshot.medallion_spark.salesorderheader_snapshot' was properly closed.
[0m00:15:53.564175 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: ROLLBACK
[0m00:15:53.564175 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:15:53.565176 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: Close
[0m00:15:53.565176 [debug] [MainThread]: Databricks adapter: Connection(session-id=f4a66af3-3d1c-4887-aedd-12eb9feb3472) - Closing connection
[0m00:15:53.622171 [info ] [MainThread]: 
[0m00:15:53.623171 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 42.73 seconds (42.73s).
[0m00:15:53.625173 [debug] [MainThread]: Command end result
[0m00:15:53.673182 [info ] [MainThread]: 
[0m00:15:53.674184 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m00:15:53.675184 [info ] [MainThread]: 
[0m00:15:53.676186 [error] [MainThread]:   Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`customer` doesn't exist.
[0m00:15:53.677183 [info ] [MainThread]: 
[0m00:15:53.677183 [error] [MainThread]:   Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
[0m00:15:53.678183 [info ] [MainThread]: 
[0m00:15:53.679183 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=0 TOTAL=7
[0m00:15:53.680183 [debug] [MainThread]: Command `dbt snapshot` failed at 00:15:53.680183 after 45.26 seconds
[0m00:15:53.681185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236AB32FDD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C607DD60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236C626BE30>]}
[0m00:15:53.681185 [debug] [MainThread]: Flushing usage events
[0m00:18:25.601257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6D1227710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6D344A150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6D3C55CA0>]}


============================== 00:18:25.606258 | f7018e43-39b4-4a9b-a882-6805cd28ac20 ==============================
[0m00:18:25.606258 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:18:25.607258 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m00:18:25.780297 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:18:25.780297 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:18:25.781298 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:18:27.320646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EE3DE300>]}
[0m00:18:27.377659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6D322C080>]}
[0m00:18:27.378659 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:18:27.392662 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:18:27.617713 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:18:27.617713 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:18:27.674726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EE7C6390>]}
[0m00:18:27.892776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EE9D9CA0>]}
[0m00:18:27.893776 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:18:27.894775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EE779CD0>]}
[0m00:18:27.896776 [info ] [MainThread]: 
[0m00:18:27.897776 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(7300, 9444), compute-name=) - Creating connection
[0m00:18:27.898776 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:18:27.898776 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Acquired connection on thread (7300, 9444), using default compute resource
[0m00:18:27.905780 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091357114144, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(7300, 15960), compute-name=) - Creating connection
[0m00:18:27.906780 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:18:27.906780 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091357114144, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7300, 15960), compute-name=) - Acquired connection on thread (7300, 15960), using default compute resource
[0m00:18:27.907780 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091357114144, session-id=None, name=list_hive_metastore, idle-time=0.0009999275207519531s, acquire-count=1, language=None, thread-identifier=(7300, 15960), compute-name=) - Checking idleness
[0m00:18:27.907780 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091357114144, session-id=None, name=list_hive_metastore, idle-time=0.0009999275207519531s, acquire-count=1, language=None, thread-identifier=(7300, 15960), compute-name=) - Retrieving connection
[0m00:18:27.908781 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:18:27.908781 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:18:27.909780 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:18:28.135298 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091357114144, session-id=aa208cf1-6fb3-42b6-a86a-5fcdf527b6e8, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7300, 15960), compute-name=) - Connection created
[0m00:18:28.137297 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=aa208cf1-6fb3-42b6-a86a-5fcdf527b6e8, command-id=Unknown) - Created cursor
[0m00:18:28.270844 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m00:18:28.274844 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=aa208cf1-6fb3-42b6-a86a-5fcdf527b6e8, command-id=90f75c6c-df40-4409-a1b4-6bb96d72a5c5) - Closing cursor
[0m00:18:28.274844 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091357114144, session-id=aa208cf1-6fb3-42b6-a86a-5fcdf527b6e8, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(7300, 15960), compute-name=) - Released connection
[0m00:18:28.276846 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(7300, 540), compute-name=) - Creating connection
[0m00:18:28.277846 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:18:28.277846 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Acquired connection on thread (7300, 540), using default compute resource
[0m00:18:28.278846 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0010006427764892578s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Checking idleness
[0m00:18:28.279846 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.002000570297241211s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Retrieving connection
[0m00:18:28.280846 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:18:28.280846 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:18:28.281846 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:18:28.525335 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Connection created
[0m00:18:28.526334 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=Unknown) - Created cursor
[0m00:18:28.640349 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m00:18:28.644350 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=c1ef8c70-6711-493a-8eea-d5d186a06c94) - Closing cursor
[0m00:18:28.656352 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_saleslt, idle-time=0.13101744651794434s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Checking idleness
[0m00:18:28.657352 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_saleslt, idle-time=0.1320176124572754s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Retrieving connection
[0m00:18:28.658353 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_saleslt, idle-time=0.1320176124572754s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Checking idleness
[0m00:18:28.658353 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_saleslt, idle-time=0.13301801681518555s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Retrieving connection
[0m00:18:28.658353 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:18:28.659353 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:18:28.659353 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:18:28.660353 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=Unknown) - Created cursor
[0m00:18:28.812476 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:18:28.816477 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=c22f062b-26f4-49e6-80ed-47b5cc1a753c) - Closing cursor
[0m00:18:28.823479 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_saleslt, idle-time=0.29714226722717285s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Checking idleness
[0m00:18:28.823479 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_saleslt, idle-time=0.2981443405151367s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Retrieving connection
[0m00:18:28.824478 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:18:28.824478 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:18:28.825478 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=Unknown) - Created cursor
[0m00:18:29.037399 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m00:18:29.040400 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=a6765984-e236-4478-ae24-111c34899521) - Closing cursor
[0m00:18:29.041401 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(7300, 540), compute-name=) - Released connection
[0m00:18:29.042400 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_saleslt, idle-time=0.0009996891021728516s, acquire-count=0, language=None, thread-identifier=(7300, 540), compute-name=) - Checking idleness
[0m00:18:29.046401 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:18:29.047402 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_snapshots, idle-time=0.00600123405456543s, acquire-count=0, language=None, thread-identifier=(7300, 540), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m00:18:29.047402 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_snapshots, idle-time=0.00600123405456543s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Acquired connection on thread (7300, 540), using default compute resource
[0m00:18:29.048402 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_snapshots, idle-time=0.0070018768310546875s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Checking idleness
[0m00:18:29.048402 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_snapshots, idle-time=0.0070018768310546875s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Retrieving connection
[0m00:18:29.049403 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:18:29.049403 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:18:29.050402 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=Unknown) - Created cursor
[0m00:18:29.161411 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:18:29.166412 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=81330410-3ac2-4afd-bc44-f6c4897457c7) - Closing cursor
[0m00:18:29.169413 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_snapshots, idle-time=0.12801194190979004s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Checking idleness
[0m00:18:29.170413 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_snapshots, idle-time=0.1290121078491211s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Retrieving connection
[0m00:18:29.171413 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:18:29.171413 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:18:29.172413 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=Unknown) - Created cursor
[0m00:18:29.265441 [debug] [ThreadPool]: SQL status: OK in 0.09000000357627869 seconds
[0m00:18:29.268442 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=c4e20a1e-c66e-4566-ae5e-6246e67160aa) - Closing cursor
[0m00:18:29.271442 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_snapshots, idle-time=0.2300417423248291s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Checking idleness
[0m00:18:29.271442 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_snapshots, idle-time=0.2300417423248291s, acquire-count=1, language=None, thread-identifier=(7300, 540), compute-name=) - Retrieving connection
[0m00:18:29.272442 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:18:29.272442 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:18:29.273443 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=Unknown) - Created cursor
[0m00:18:29.499483 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m00:18:29.501483 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, command-id=42582fd4-b31f-4e78-a3d9-4e16f3c363d2) - Closing cursor
[0m00:18:29.502483 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2091356736160, session-id=6cdd144b-4df8-4000-bf59-27acc5e96088, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(7300, 540), compute-name=) - Released connection
[0m00:18:29.504484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6D1227710>]}
[0m00:18:29.505484 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=1.6067078113555908s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Checking idleness
[0m00:18:29.505484 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=1.6067078113555908s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Retrieving connection
[0m00:18:29.506485 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=1.607708215713501s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Checking idleness
[0m00:18:29.506485 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=1.607708215713501s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Retrieving connection
[0m00:18:29.507485 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:18:29.507485 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:18:29.508485 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(7300, 9444), compute-name=) - Released connection
[0m00:18:29.508485 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:18:29.509485 [info ] [MainThread]: 
[0m00:18:29.513486 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m00:18:29.513486 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m00:18:29.514486 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(7300, 16996), compute-name=) - Creating connection
[0m00:18:29.515491 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m00:18:29.515491 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Acquired connection on thread (7300, 16996), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m00:18:29.516489 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m00:18:29.525488 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m00:18:29.559498 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04400753974914551s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:29.560497 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04500627517700195s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Retrieving connection
[0m00:18:29.560497 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04500627517700195s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:29.561497 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04600667953491211s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Retrieving connection
[0m00:18:29.562497 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:18:29.562497 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:18:29.562497 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:18:29.563497 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:18:29.722492 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Connection created
[0m00:18:29.723492 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Created cursor
[0m00:18:29.915531 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Closing cursor
[0m00:18:29.916530 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=30f1cfbe-15a8-4ea6-84fe-a2d2a109d90d
[0m00:18:29.917531 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Runtime Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m00:18:29.918531 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:29.930534 [debug] [Thread-1 (]: Runtime Error in snapshot address_snapshot (snapshots\address.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m00:18:29.931535 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:29.932535 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EEE8EB40>]}
[0m00:18:29.933535 [error] [Thread-1 (]: 1 of 7 ERROR snapshotting snapshots.address_snapshot ........................... [[31mERROR[0m in 0.42s]
[0m00:18:29.934535 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m00:18:29.935535 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m00:18:29.937535 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m00:18:29.938536 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.address_snapshot, idle-time=0.008002042770385742s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:29.938536 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m00:18:29.939536 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.009002447128295898s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m00:18:29.940536 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.009002447128295898s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Acquired connection on thread (7300, 16996), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m00:18:29.940536 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m00:18:29.944537 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m00:18:29.948538 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.01800370216369629s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:29.949538 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.019003868103027344s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Retrieving connection
[0m00:18:29.949538 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:18:29.950538 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m00:18:29.951539 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Created cursor
[0m00:18:30.083575 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Closing cursor
[0m00:18:30.084575 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=19a970c4-341d-4bf4-a4ca-68e17b67065a
[0m00:18:30.085576 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Runtime Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.086575 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.091577 [debug] [Thread-1 (]: Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.092577 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.093577 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EE993F20>]}
[0m00:18:30.094578 [error] [Thread-1 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 0.16s]
[0m00:18:30.095578 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m00:18:30.095578 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:18:30.096578 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m00:18:30.097579 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0050013065338134766s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:30.097579 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m00:18:30.098578 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.00600123405456543s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m00:18:30.098578 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.00600123405456543s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Acquired connection on thread (7300, 16996), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m00:18:30.099579 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m00:18:30.102578 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m00:18:30.107581 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.014003276824951172s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:30.107581 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.015003442764282227s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Retrieving connection
[0m00:18:30.108581 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:18:30.108581 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:18:30.109581 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Created cursor
[0m00:18:30.256602 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Closing cursor
[0m00:18:30.257602 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=39702b58-b275-4db5-b876-971af1e265d6
[0m00:18:30.258603 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Runtime Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.259603 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.264604 [debug] [Thread-1 (]: Runtime Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.264604 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.265604 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EEFCCF50>]}
[0m00:18:30.266605 [error] [Thread-1 (]: 3 of 7 ERROR snapshotting snapshots.customeraddress_snapshot ................... [[31mERROR[0m in 0.17s]
[0m00:18:30.267605 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:18:30.268606 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m00:18:30.268606 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m00:18:30.269607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.005002737045288086s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:30.270606 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m00:18:30.270606 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0060024261474609375s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m00:18:30.271607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0060024261474609375s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Acquired connection on thread (7300, 16996), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m00:18:30.271607 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m00:18:30.275607 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m00:18:30.281610 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.product_snapshot, idle-time=0.01700568199157715s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:30.282609 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.product_snapshot, idle-time=0.01700568199157715s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Retrieving connection
[0m00:18:30.282609 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:18:30.283609 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m00:18:30.283609 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Created cursor
[0m00:18:30.420568 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Closing cursor
[0m00:18:30.421569 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=c64a2ceb-2e50-4ddc-be99-f0c97c507512
[0m00:18:30.422568 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Runtime Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.422568 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.428570 [debug] [Thread-1 (]: Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.428570 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.429570 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EEFCC770>]}
[0m00:18:30.430571 [error] [Thread-1 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.16s]
[0m00:18:30.431570 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m00:18:30.432571 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m00:18:30.432571 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m00:18:30.433571 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.product_snapshot, idle-time=0.005001544952392578s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:30.434571 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m00:18:30.434571 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.006001710891723633s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m00:18:30.435572 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.006001710891723633s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Acquired connection on thread (7300, 16996), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m00:18:30.435572 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m00:18:30.439573 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m00:18:30.443574 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.015004158020019531s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:30.444574 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.016004323959350586s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Retrieving connection
[0m00:18:30.444574 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:18:30.445576 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:18:30.445576 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Created cursor
[0m00:18:30.584587 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Closing cursor
[0m00:18:30.585587 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=d989de15-cfc7-47f0-9115-24b6f1e1b5af
[0m00:18:30.585587 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Runtime Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.586587 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.592588 [debug] [Thread-1 (]: Runtime Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.592588 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.593589 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EEB1C7D0>]}
[0m00:18:30.594589 [error] [Thread-1 (]: 5 of 7 ERROR snapshotting snapshots.productmodel_snapshot ...................... [[31mERROR[0m in 0.16s]
[0m00:18:30.595590 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m00:18:30.595590 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:18:30.596590 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m00:18:30.597590 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.00500178337097168s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:30.597590 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m00:18:30.598590 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.00600123405456543s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m00:18:30.598590 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.00600123405456543s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Acquired connection on thread (7300, 16996), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m00:18:30.599591 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:18:30.603592 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:18:30.607593 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.014002561569213867s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:30.607593 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.015004396438598633s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Retrieving connection
[0m00:18:30.608593 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:18:30.608593 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:18:30.609593 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Created cursor
[0m00:18:30.771605 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Closing cursor
[0m00:18:30.772605 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=84dc91d0-2da8-41a3-ae04-d517559ad3f0
[0m00:18:30.773607 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Runtime Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.773607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.779607 [debug] [Thread-1 (]: Runtime Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.780608 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.780608 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EEFB6930>]}
[0m00:18:30.781608 [error] [Thread-1 (]: 6 of 7 ERROR snapshotting snapshots.salesorderdetail_snapshot .................. [[31mERROR[0m in 0.18s]
[0m00:18:30.782608 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:18:30.783608 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:18:30.783608 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m00:18:30.784609 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.005002260208129883s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:30.785608 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m00:18:30.785608 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.006000995635986328s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:18:30.786608 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.007001399993896484s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Acquired connection on thread (7300, 16996), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m00:18:30.786608 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:18:30.790610 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:18:30.796611 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.01700448989868164s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Checking idleness
[0m00:18:30.797610 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.01800370216369629s, acquire-count=1, language=sql, thread-identifier=(7300, 16996), compute-name=) - Retrieving connection
[0m00:18:30.797610 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:18:30.798614 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:18:30.799612 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Created cursor
[0m00:18:30.937088 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, command-id=Unknown) - Closing cursor
[0m00:18:30.938087 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=b7fc564f-8000-45d1-8ea3-38a1b586100a
[0m00:18:30.939088 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Runtime Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.940089 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.945089 [debug] [Thread-1 (]: Runtime Error in snapshot salesorderheader_snapshot (snapshots\saleorderheader.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m00:18:30.946090 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2091362378064, session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7300, 16996), compute-name=) - Released connection
[0m00:18:30.947091 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7018e43-39b4-4a9b-a882-6805cd28ac20', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EEFC5130>]}
[0m00:18:30.947091 [error] [Thread-1 (]: 7 of 7 ERROR snapshotting snapshots.salesorderheader_snapshot .................. [[31mERROR[0m in 0.16s]
[0m00:18:30.949090 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:18:30.950089 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=1.4416038990020752s, acquire-count=0, language=None, thread-identifier=(7300, 9444), compute-name=) - Checking idleness
[0m00:18:30.950089 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=1.4416038990020752s, acquire-count=0, language=None, thread-identifier=(7300, 9444), compute-name=) - Reusing connection previously named master
[0m00:18:30.951090 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=1.4426054954528809s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Acquired connection on thread (7300, 9444), using default compute resource
[0m00:18:30.951090 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=1.4426054954528809s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Checking idleness
[0m00:18:30.952090 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=None, name=master, idle-time=1.4436051845550537s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Retrieving connection
[0m00:18:30.952090 [debug] [MainThread]: On master: ROLLBACK
[0m00:18:30.953090 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:18:31.127599 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=a6bc8c88-0baa-425e-be6e-74d7e3fa6859, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Connection created
[0m00:18:31.127599 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:18:31.128597 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=a6bc8c88-0baa-425e-be6e-74d7e3fa6859, name=master, idle-time=0.002001047134399414s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Checking idleness
[0m00:18:31.129597 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=a6bc8c88-0baa-425e-be6e-74d7e3fa6859, name=master, idle-time=0.002001047134399414s, acquire-count=1, language=None, thread-identifier=(7300, 9444), compute-name=) - Retrieving connection
[0m00:18:31.129597 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:18:31.130597 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:18:31.131597 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2091357409824, session-id=a6bc8c88-0baa-425e-be6e-74d7e3fa6859, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(7300, 9444), compute-name=) - Released connection
[0m00:18:31.131597 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:18:31.132597 [debug] [MainThread]: On master: ROLLBACK
[0m00:18:31.133597 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:18:31.133597 [debug] [MainThread]: On master: Close
[0m00:18:31.134598 [debug] [MainThread]: Databricks adapter: Connection(session-id=a6bc8c88-0baa-425e-be6e-74d7e3fa6859) - Closing connection
[0m00:18:31.201627 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:18:31.201627 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:18:31.202627 [debug] [MainThread]: Databricks adapter: Connection(session-id=aa208cf1-6fb3-42b6-a86a-5fcdf527b6e8) - Closing connection
[0m00:18:31.261654 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:18:31.262655 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:18:31.263654 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:18:31.264654 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:18:31.264654 [debug] [MainThread]: Databricks adapter: Connection(session-id=6cdd144b-4df8-4000-bf59-27acc5e96088) - Closing connection
[0m00:18:31.329670 [debug] [MainThread]: Connection 'snapshot.medallion_spark.salesorderheader_snapshot' was properly closed.
[0m00:18:31.330669 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: ROLLBACK
[0m00:18:31.331670 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:18:31.332671 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: Close
[0m00:18:31.332671 [debug] [MainThread]: Databricks adapter: Connection(session-id=8928d85e-78a2-4303-9d51-e18c486e8e1e) - Closing connection
[0m00:18:31.383684 [info ] [MainThread]: 
[0m00:18:31.384685 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 3.49 seconds (3.49s).
[0m00:18:31.386684 [debug] [MainThread]: Command end result
[0m00:18:31.430694 [info ] [MainThread]: 
[0m00:18:31.431694 [info ] [MainThread]: [31mCompleted with 7 errors and 0 warnings:[0m
[0m00:18:31.432696 [info ] [MainThread]: 
[0m00:18:31.433695 [error] [MainThread]:   Runtime Error in snapshot address_snapshot (snapshots\address.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m00:18:31.433695 [info ] [MainThread]: 
[0m00:18:31.434695 [error] [MainThread]:   Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m00:18:31.435696 [info ] [MainThread]: 
[0m00:18:31.435696 [error] [MainThread]:   Runtime Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m00:18:31.436695 [info ] [MainThread]: 
[0m00:18:31.437696 [error] [MainThread]:   Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m00:18:31.438696 [info ] [MainThread]: 
[0m00:18:31.439697 [error] [MainThread]:   Runtime Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m00:18:31.440697 [info ] [MainThread]: 
[0m00:18:31.441698 [error] [MainThread]:   Runtime Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m00:18:31.442697 [info ] [MainThread]: 
[0m00:18:31.443697 [error] [MainThread]:   Runtime Error in snapshot salesorderheader_snapshot (snapshots\saleorderheader.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m00:18:31.444698 [info ] [MainThread]: 
[0m00:18:31.445698 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=7 SKIP=0 TOTAL=7
[0m00:18:31.446698 [debug] [MainThread]: Command `dbt snapshot` failed at 00:18:31.446698 after 5.99 seconds
[0m00:18:31.447699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6D39063C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6D3906270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6EEEB9670>]}
[0m00:18:31.447699 [debug] [MainThread]: Flushing usage events
[0m00:20:37.291993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57DEC3500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5004B6510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5004B5970>]}


============================== 00:20:37.296993 | c9cd655a-9037-40e4-b5cf-42b72f4f2f0a ==============================
[0m00:20:37.296993 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:20:37.297994 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt snapshot', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:20:37.467033 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:20:37.467033 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:20:37.468033 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:20:39.105403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57FD802C0>]}
[0m00:20:39.162416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A500EC68A0>]}
[0m00:20:39.163416 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:20:39.176418 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:20:39.400470 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:20:39.401469 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:20:39.461484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51BB038F0>]}
[0m00:20:39.714540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51BA3A120>]}
[0m00:20:39.715540 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:20:39.716540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51B9DF260>]}
[0m00:20:39.719542 [info ] [MainThread]: 
[0m00:20:39.720542 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16480, 11912), compute-name=) - Creating connection
[0m00:20:39.720542 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:20:39.721541 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Acquired connection on thread (16480, 11912), using default compute resource
[0m00:20:39.727543 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644574048, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16480, 18372), compute-name=) - Creating connection
[0m00:20:39.728543 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:20:39.728543 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644574048, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16480, 18372), compute-name=) - Acquired connection on thread (16480, 18372), using default compute resource
[0m00:20:39.729544 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644574048, session-id=None, name=list_hive_metastore, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(16480, 18372), compute-name=) - Checking idleness
[0m00:20:39.729544 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644574048, session-id=None, name=list_hive_metastore, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(16480, 18372), compute-name=) - Retrieving connection
[0m00:20:39.730544 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:20:39.730544 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:20:39.731544 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:20:40.045058 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644574048, session-id=21318b34-53b2-4046-b6e0-5a4d474006cf, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16480, 18372), compute-name=) - Connection created
[0m00:20:40.046058 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=21318b34-53b2-4046-b6e0-5a4d474006cf, command-id=Unknown) - Created cursor
[0m00:20:40.590114 [debug] [ThreadPool]: SQL status: OK in 0.8600000143051147 seconds
[0m00:20:40.595116 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=21318b34-53b2-4046-b6e0-5a4d474006cf, command-id=f8b60f28-293b-4c0f-8d02-ba0a81652837) - Closing cursor
[0m00:20:40.596116 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644574048, session-id=21318b34-53b2-4046-b6e0-5a4d474006cf, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16480, 18372), compute-name=) - Released connection
[0m00:20:40.598116 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16480, 18308), compute-name=) - Creating connection
[0m00:20:40.598116 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:20:40.599118 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Acquired connection on thread (16480, 18308), using default compute resource
[0m00:20:40.599118 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Checking idleness
[0m00:20:40.600118 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Retrieving connection
[0m00:20:40.600118 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:20:40.601118 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:20:40.601118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:20:40.816145 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Connection created
[0m00:20:40.817146 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=Unknown) - Created cursor
[0m00:20:40.984526 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m00:20:40.989527 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=67f2d6b8-5d0d-4746-accc-11eb48f4b9ee) - Closing cursor
[0m00:20:41.002530 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_saleslt, idle-time=0.18738460540771484s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Checking idleness
[0m00:20:41.002530 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_saleslt, idle-time=0.18738460540771484s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Retrieving connection
[0m00:20:41.003530 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_saleslt, idle-time=0.1883852481842041s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Checking idleness
[0m00:20:41.003530 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_saleslt, idle-time=0.1883852481842041s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Retrieving connection
[0m00:20:41.004531 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:20:41.004531 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:20:41.005531 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:20:41.005531 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=Unknown) - Created cursor
[0m00:20:41.134581 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m00:20:41.137582 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=730eee4f-fe9f-4dc8-8607-018e29f4a2f2) - Closing cursor
[0m00:20:41.143583 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_saleslt, idle-time=0.3274385929107666s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Checking idleness
[0m00:20:41.143583 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_saleslt, idle-time=0.32843828201293945s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Retrieving connection
[0m00:20:41.144584 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:20:41.144584 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:20:41.145584 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=Unknown) - Created cursor
[0m00:20:41.383013 [debug] [ThreadPool]: SQL status: OK in 0.23999999463558197 seconds
[0m00:20:41.386014 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=39d7aec6-dfaa-453f-a78b-bab2f219eff0) - Closing cursor
[0m00:20:41.387014 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16480, 18308), compute-name=) - Released connection
[0m00:20:41.388015 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16480, 18308), compute-name=) - Checking idleness
[0m00:20:41.392016 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:20:41.393016 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_snapshots, idle-time=0.005001544952392578s, acquire-count=0, language=None, thread-identifier=(16480, 18308), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m00:20:41.393016 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_snapshots, idle-time=0.006001472473144531s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Acquired connection on thread (16480, 18308), using default compute resource
[0m00:20:41.394016 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_snapshots, idle-time=0.006001472473144531s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Checking idleness
[0m00:20:41.394016 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_snapshots, idle-time=0.0070018768310546875s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Retrieving connection
[0m00:20:41.394016 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:20:41.395016 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:20:41.395016 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=Unknown) - Created cursor
[0m00:20:41.501065 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:20:41.506067 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=56e9b038-bcdd-4c30-a5b7-38dbda800657) - Closing cursor
[0m00:20:41.509066 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_snapshots, idle-time=0.12205219268798828s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Checking idleness
[0m00:20:41.510067 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_snapshots, idle-time=0.12205219268798828s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Retrieving connection
[0m00:20:41.510067 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:20:41.511068 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:20:41.511068 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=Unknown) - Created cursor
[0m00:20:41.608098 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m00:20:41.610101 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=971a7353-6d77-4830-a40b-0ea134efcf13) - Closing cursor
[0m00:20:41.613101 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_snapshots, idle-time=0.22608661651611328s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Checking idleness
[0m00:20:41.613101 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_snapshots, idle-time=0.22608661651611328s, acquire-count=1, language=None, thread-identifier=(16480, 18308), compute-name=) - Retrieving connection
[0m00:20:41.614102 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:20:41.614102 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:20:41.615101 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=Unknown) - Created cursor
[0m00:20:41.779258 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:20:41.782259 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, command-id=d6d5e3a1-21de-4531-8de9-a0a4d800b804) - Closing cursor
[0m00:20:41.783260 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1808644571792, session-id=861e0cf0-70a0-442f-9f66-c9fb58667614, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16480, 18308), compute-name=) - Released connection
[0m00:20:41.786261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51BB109E0>]}
[0m00:20:41.786261 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=2.0647192001342773s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Checking idleness
[0m00:20:41.787261 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=2.0657196044921875s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Retrieving connection
[0m00:20:41.788261 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=2.0667192935943604s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Checking idleness
[0m00:20:41.789260 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=2.067718982696533s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Retrieving connection
[0m00:20:41.790260 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:20:41.791261 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:20:41.792261 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16480, 11912), compute-name=) - Released connection
[0m00:20:41.793261 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:20:41.794262 [info ] [MainThread]: 
[0m00:20:41.797263 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m00:20:41.798264 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m00:20:41.798264 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16480, 16648), compute-name=) - Creating connection
[0m00:20:41.799265 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m00:20:41.800265 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Acquired connection on thread (16480, 16648), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m00:20:41.800265 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m00:20:41.810266 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m00:20:41.844273 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.045007944107055664s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:41.845273 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.045007944107055664s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:41.845273 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.046007633209228516s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:41.846273 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04700803756713867s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:41.846273 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:20:41.847274 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:41.847274 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:20:41.848274 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:20:42.012293 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Connection created
[0m00:20:42.012293 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:42.458361 [debug] [Thread-1 (]: SQL status: OK in 0.6100000143051147 seconds
[0m00:20:42.462362 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=3b6a03be-1d32-4382-85ad-6e672b19a503) - Closing cursor
[0m00:20:42.510373 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=0.49907922744750977s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:42.511374 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=0.500079870223999s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:42.512374 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:42.512374 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
select * from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:20:42.513374 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:42.672398 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:20:42.677402 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=81888803-10f3-4968-8054-4d415b45eca7) - Closing cursor
[0m00:20:42.681401 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=0.6701071262359619s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:42.682401 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=0.6711077690124512s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:42.682401 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:42.683402 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:20:42.684402 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:42.859392 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:20:42.862392 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=e5fba4b4-a33f-4894-9482-cad9a389e101) - Closing cursor
[0m00:20:42.867394 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=0.8560998439788818s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:42.868393 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=0.8570995330810547s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:42.868393 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:42.869394 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:20:42.870394 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:43.052419 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:20:43.055420 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=4ad57f00-ee56-4a90-b451-d1c4d7b8d5bd) - Closing cursor
[0m00:20:43.080425 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=1.069131851196289s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:43.081426 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=1.0701320171356201s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:43.082426 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:43.083426 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`address_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:20:43.084426 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:43.497462 [debug] [Thread-1 (]: SQL status: OK in 0.4099999964237213 seconds
[0m00:20:43.499463 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=54efb606-1709-454a-bb49-51620f111859) - Closing cursor
[0m00:20:43.501463 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=1.490168809890747s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:43.502463 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=1.491168737411499s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:43.502463 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:43.503463 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:20:43.503463 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:43.650348 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:20:43.654348 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=f8fdce42-d04b-40a4-a51a-c8bd3c26a5a6) - Closing cursor
[0m00:20:43.657349 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=1.6460554599761963s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:43.658349 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=1.6470553874969482s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:43.659350 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:43.660351 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:20:43.661350 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:43.830512 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:20:43.834512 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=c72fb652-8033-4d59-955a-aabaafd40635) - Closing cursor
[0m00:20:43.839514 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=1.8282198905944824s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:43.840514 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=1.8292198181152344s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:43.841514 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:43.842514 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:20:43.843514 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:43.981523 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:20:43.984524 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=4546ef3f-b8a1-4ba9-8afb-2660297afea7) - Closing cursor
[0m00:20:43.988525 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9772310256958008s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:43.989525 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=1.978231430053711s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:43.989525 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:43.990525 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:20:43.990525 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:44.147540 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:20:44.150540 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=13eac47c-7ef8-483c-b09f-8d00a0d231a5) - Closing cursor
[0m00:20:44.157541 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=2.1462478637695312s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:44.158542 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=2.1462478637695312s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:44.158542 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:44.159542 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:20:44.159542 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:44.305383 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:20:44.308384 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=7371708f-8607-4539-8db1-b8d68705819f) - Closing cursor
[0m00:20:44.317385 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.address_snapshot"
[0m00:20:44.319386 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=2.3080923557281494s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:44.319386 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=2.3080923557281494s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:44.320386 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:44.320386 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`address_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:20:44.321386 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:48.120480 [debug] [Thread-1 (]: SQL status: OK in 3.799999952316284 seconds
[0m00:20:48.122480 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=6ea38dfe-9a62-43b5-b3e8-c1ff34db9198) - Closing cursor
[0m00:20:48.129482 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:20:48.134483 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=6.123189210891724s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:48.135483 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=6.124189853668213s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:48.135483 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:20:48.136484 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:20:48.137484 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:48.571085 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m00:20:48.573087 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=1a801597-09b7-4d01-85a7-7592d582b4ba) - Closing cursor
[0m00:20:48.594092 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:20:48.596092 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:20:48.597092 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:20:48.598092 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51B432240>]}
[0m00:20:48.599092 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 6.80s]
[0m00:20:48.600094 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m00:20:48.601093 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m00:20:48.601093 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m00:20:48.602093 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0050013065338134766s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:48.603093 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m00:20:48.603093 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0060007572174072266s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m00:20:48.604094 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.007001638412475586s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Acquired connection on thread (16480, 16648), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m00:20:48.604094 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m00:20:48.608095 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m00:20:48.612096 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.015003442764282227s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:48.613095 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.016003131866455078s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:48.614096 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:20:48.614096 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m00:20:48.615096 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:48.958128 [debug] [Thread-1 (]: SQL status: OK in 0.3400000035762787 seconds
[0m00:20:48.960129 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=207f3ec8-7609-4f03-8b21-fb6a57775c5c) - Closing cursor
[0m00:20:48.963129 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.36603665351867676s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:48.963129 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.36603665351867676s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:48.964130 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:20:48.964130 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:20:48.965130 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:49.255151 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Closing cursor
[0m00:20:49.257152 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`customer` doesn't exist.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_TABLE_NOT_FOUND] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`customer` doesn't exist.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`customer` doesn't exist.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable(DeltaErrors.scala:1450)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable$(DeltaErrors.scala:1449)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.nonExistentDeltaTable(DeltaErrors.scala:3323)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:336)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:321)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.$anonfun$fromV2Relation$1(DeltaAnalysis.scala:1630)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.recordFrameProfile(DeltaAnalysis.scala:1615)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.fromV2Relation(DeltaAnalysis.scala:1629)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:649)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.CTERelationDef.mapChildren(basicLogicalOperators.scala:944)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:716)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:343)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:1917)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1896)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:39)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:107)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:101)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:415)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:408)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:322)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:408)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:341)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:393)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:392)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:576)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1097)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:576)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:572)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:572)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:552)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:522)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:512)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:522)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:595)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:595)
	... 35 more
, operation-id=f903a2a3-5eb2-46c4-8c67-330df7c9362e
[0m00:20:49.259152 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:20:49.267154 [debug] [Thread-1 (]: Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`customer` doesn't exist.
[0m00:20:49.268155 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:20:49.269155 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51BEE1640>]}
[0m00:20:49.269155 [error] [Thread-1 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 0.67s]
[0m00:20:49.271155 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m00:20:49.271155 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:20:49.273156 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m00:20:49.274155 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.006000518798828125s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:49.274155 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m00:20:49.275156 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.007000923156738281s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m00:20:49.276156 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.007000923156738281s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Acquired connection on thread (16480, 16648), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m00:20:49.276156 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m00:20:49.280157 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m00:20:49.286158 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.017003536224365234s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:49.286158 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.018003463745117188s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:49.287159 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:49.287159 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:20:49.288159 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:49.532151 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m00:20:49.535152 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=8d2a59aa-1c2b-4687-a2d1-7f497c8a3e3e) - Closing cursor
[0m00:20:49.537152 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.2689976692199707s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:49.538153 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.26999759674072266s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:49.538153 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:49.539153 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:20:49.540153 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:50.755380 [debug] [Thread-1 (]: SQL status: OK in 1.2200000286102295 seconds
[0m00:20:50.759381 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=d392c535-31a6-47ac-8637-f6bf41721ac7) - Closing cursor
[0m00:20:50.763382 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.4952270984649658s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:50.764383 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.496227741241455s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:50.764383 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:50.765383 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:20:50.765383 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:50.923363 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:20:50.925357 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=5f23a2c8-d8d6-4d91-a72e-90528ab847be) - Closing cursor
[0m00:20:50.928359 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.6602041721343994s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:50.929358 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.661203145980835s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:50.930359 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:50.930359 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:20:50.931358 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:51.091233 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:20:51.095234 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=34bc4bc2-37b5-475f-a322-6c4fe528589b) - Closing cursor
[0m00:20:51.099235 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.8310799598693848s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:51.100236 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.8320808410644531s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:51.101235 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:51.102237 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId||'-'||AddressId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customeraddress_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:20:51.103236 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:51.466625 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m00:20:51.467626 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=a19a7821-5cad-434a-8a97-89ec860b550c) - Closing cursor
[0m00:20:51.470626 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.2024712562561035s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:51.470626 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.2024712562561035s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:51.471627 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:51.471627 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:20:51.472627 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:51.619238 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:20:51.622239 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=bc00f166-4945-4dfb-9523-6efb9148fc96) - Closing cursor
[0m00:20:51.624239 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.3560843467712402s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:51.625239 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.3570845127105713s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:51.625239 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:51.626240 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:20:51.627240 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:51.805469 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:20:51.810471 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=a9635e39-16d7-4776-af73-2921f96f83b4) - Closing cursor
[0m00:20:51.814472 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.5463171005249023s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:51.814472 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.5463171005249023s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:51.815472 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:51.815472 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:20:51.816472 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:51.951474 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:20:51.954475 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=7e150f20-4169-4a14-abe6-8a88c1737421) - Closing cursor
[0m00:20:51.957475 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.6883206367492676s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:51.957475 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.6893205642700195s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:51.958476 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:51.958476 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:20:51.959476 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:52.118434 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:20:52.121435 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=d8284079-5001-4b93-8cdb-7abd5b5928b7) - Closing cursor
[0m00:20:52.124436 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.856281042098999s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:52.125436 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.8572816848754883s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:52.125436 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:52.126437 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:20:52.126437 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:52.275490 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:20:52.279494 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=db37bf0e-758c-434d-9596-9172f558f4e1) - Closing cursor
[0m00:20:52.282493 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:52.284494 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=3.01633882522583s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:52.285494 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=3.017338752746582s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:52.286495 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:52.287495 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customeraddress_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:20:52.288495 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:55.966952 [debug] [Thread-1 (]: SQL status: OK in 3.680000066757202 seconds
[0m00:20:55.967952 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=4638ce65-ddcd-47cc-b8bd-36e5df3ebdb4) - Closing cursor
[0m00:20:55.970952 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:20:55.972953 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=6.703797340393066s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:55.972953 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=6.704798221588135s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:55.973953 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:20:55.973953 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:20:55.974953 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:56.408014 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m00:20:56.409015 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=7c34b8f2-791f-434f-953e-f3a6e3bbb465) - Closing cursor
[0m00:20:56.411016 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:20:56.412016 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:20:56.413016 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:20:56.414016 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51C071850>]}
[0m00:20:56.415016 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 7.14s]
[0m00:20:56.416017 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:20:56.416017 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m00:20:56.417017 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m00:20:56.418017 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.005001068115234375s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:56.418017 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m00:20:56.419017 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.product_snapshot, idle-time=0.00600123405456543s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m00:20:56.419017 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.product_snapshot, idle-time=0.00600123405456543s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Acquired connection on thread (16480, 16648), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m00:20:56.420017 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m00:20:56.424019 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m00:20:56.430023 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.product_snapshot, idle-time=0.017006874084472656s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:56.431022 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.product_snapshot, idle-time=0.018005847930908203s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:56.431022 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:20:56.432021 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m00:20:56.433020 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:56.762054 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m00:20:56.765054 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=9dda71a3-347f-4e8f-818f-40d8b8f7ed58) - Closing cursor
[0m00:20:56.767055 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.product_snapshot, idle-time=0.35403871536254883s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:56.768055 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.product_snapshot, idle-time=0.3550386428833008s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:56.768055 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:20:56.769055 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:20:56.769055 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:57.040095 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Closing cursor
[0m00:20:57.042096 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_TABLE_NOT_FOUND] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable(DeltaErrors.scala:1450)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable$(DeltaErrors.scala:1449)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.nonExistentDeltaTable(DeltaErrors.scala:3323)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:336)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:321)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.$anonfun$fromV2Relation$1(DeltaAnalysis.scala:1630)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.recordFrameProfile(DeltaAnalysis.scala:1615)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.fromV2Relation(DeltaAnalysis.scala:1629)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:649)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.CTERelationDef.mapChildren(basicLogicalOperators.scala:944)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:716)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:343)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:1917)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1896)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:39)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:107)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:101)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:415)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:408)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:322)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:408)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:341)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:393)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:392)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:576)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1097)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:576)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:572)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:572)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:552)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:522)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:512)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:522)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:595)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:595)
	... 35 more
, operation-id=1cb036da-124e-442f-8866-005563f0878c
[0m00:20:57.044096 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:20:57.049097 [debug] [Thread-1 (]: Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
[0m00:20:57.050097 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:20:57.050097 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51C06DB50>]}
[0m00:20:57.051098 [error] [Thread-1 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.63s]
[0m00:20:57.052098 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m00:20:57.053098 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m00:20:57.053098 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m00:20:57.054098 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.product_snapshot, idle-time=0.005000591278076172s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:57.055098 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m00:20:57.055098 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0060007572174072266s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m00:20:57.056099 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.007001161575317383s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Acquired connection on thread (16480, 16648), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m00:20:57.056099 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m00:20:57.060100 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m00:20:57.065102 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.016004085540771484s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:57.066101 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.017003774642944336s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:57.066101 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:57.067102 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:20:57.068102 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:57.332126 [debug] [Thread-1 (]: SQL status: OK in 0.25999999046325684 seconds
[0m00:20:57.335124 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=ab4ad741-56ab-4f11-89c2-6bfcd033ff36) - Closing cursor
[0m00:20:57.337124 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.2880268096923828s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:57.338124 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.28902673721313477s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:57.338124 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:57.339125 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:20:57.339125 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:57.503471 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:20:57.506472 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=7a22d4f8-a1df-48bc-8c36-ea46b9b3d735) - Closing cursor
[0m00:20:57.509473 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.45937514305114746s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:57.509473 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.4603750705718994s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:57.510473 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:57.510473 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:20:57.511473 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:57.672523 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:20:57.675524 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=1b6da07b-8cef-4228-935d-27b0ef0a2cab) - Closing cursor
[0m00:20:57.678525 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.6294269561767578s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:57.678525 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.6294269561767578s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:57.679525 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:57.679525 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:20:57.680525 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:57.842912 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:20:57.845913 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=b8bb8967-f76b-4bee-b5d3-c40f141112d3) - Closing cursor
[0m00:20:57.848915 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7998173236846924s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:57.849914 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.8008160591125488s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:57.849914 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:57.850916 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductModelID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productmodel_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:20:57.851915 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:58.570912 [debug] [Thread-1 (]: SQL status: OK in 0.7200000286102295 seconds
[0m00:20:58.572915 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=d07f1210-0f3d-4d7a-a237-dcb67abfdca0) - Closing cursor
[0m00:20:58.574913 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.525815725326538s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:58.575913 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.526815414428711s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:58.575913 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:58.576914 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:20:58.576914 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:58.710650 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:20:58.713651 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=5990dcdf-9e87-4703-9d76-09f26f148578) - Closing cursor
[0m00:20:58.715651 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.6665534973144531s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:58.716652 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.6675543785095215s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:58.716652 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:58.717652 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:20:58.718652 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:58.881303 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:20:58.883304 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=2ce7b8fd-a730-4d5c-85e6-eee4bcc06705) - Closing cursor
[0m00:20:58.886304 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.8372068405151367s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:58.887305 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.8372068405151367s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:58.887305 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:58.888305 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:20:58.888305 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:59.028696 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:20:59.031697 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=6aa74866-1991-4129-8b9f-304e5b804a45) - Closing cursor
[0m00:20:59.033698 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.9846000671386719s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:59.034697 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.9855995178222656s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:59.034697 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:59.035699 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:20:59.035699 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:59.189202 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:20:59.193203 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=c8ca814b-4616-40e2-a721-2a7caf3e5231) - Closing cursor
[0m00:20:59.196204 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.147106170654297s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:59.197204 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.147106170654297s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:59.197204 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:59.198204 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:20:59.198204 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:20:59.327027 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:20:59.330028 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=eeadb749-0ab5-4d93-88a1-60f452be7d00) - Closing cursor
[0m00:20:59.331028 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:59.332029 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.282930850982666s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:20:59.333029 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.283931255340576s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:20:59.333029 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:20:59.334029 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productmodel_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:20:59.334029 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:02.875691 [debug] [Thread-1 (]: SQL status: OK in 3.5399999618530273 seconds
[0m00:21:02.876691 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=9c15e9dd-ed24-477e-9ba2-88f877ca97e6) - Closing cursor
[0m00:21:02.879692 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:21:02.880692 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.831594705581665s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:02.881692 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.832594871520996s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:02.881692 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:21:02.882693 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:21:02.883693 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:03.306739 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m00:21:03.308740 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=1ae0f801-e7bf-4ba6-a7ea-f868ca982650) - Closing cursor
[0m00:21:03.310740 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:21:03.312740 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:21:03.312740 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:21:03.313740 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51C0A2090>]}
[0m00:21:03.314741 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 6.26s]
[0m00:21:03.315741 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m00:21:03.315741 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:21:03.316741 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m00:21:03.317741 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.005001544952392578s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:03.318742 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m00:21:03.318742 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0060024261474609375s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m00:21:03.319741 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.007001638412475586s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Acquired connection on thread (16480, 16648), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m00:21:03.319741 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:21:03.324743 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:21:03.329744 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.017004013061523438s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:03.330744 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.018004417419433594s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:03.331744 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:03.331744 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:21:03.332745 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:03.682756 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m00:21:03.686757 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=a2cecfe1-4b8a-4401-8f92-26bc8616917e) - Closing cursor
[0m00:21:03.689758 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.3770182132720947s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:03.690759 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.3780186176300049s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:03.690759 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:03.691759 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
select * from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:21:03.692758 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:03.848815 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:21:03.854817 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=7a43411c-10c1-4a24-9971-f02047303981) - Closing cursor
[0m00:21:03.856818 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.5440783500671387s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:03.857818 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.5450778007507324s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:03.857818 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:03.858817 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:21:03.858817 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:04.073766 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:21:04.076767 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=1fbca661-098d-4c1e-8a78-ae200b3fce32) - Closing cursor
[0m00:21:04.079768 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7660274505615234s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:04.079768 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7670280933380127s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:04.080768 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:04.080768 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:21:04.081769 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:04.261797 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:21:04.264797 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=396e8a3f-6d60-4393-9209-e37982ce0d01) - Closing cursor
[0m00:21:04.269798 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.9570586681365967s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:04.270799 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.9580590724945068s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:04.271799 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:04.272799 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderDetailID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:21:04.273799 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:04.624824 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m00:21:04.626824 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=f6fa83fe-6e64-48a6-a63d-a39337307762) - Closing cursor
[0m00:21:04.629824 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.3170840740203857s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:04.630825 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.3180851936340332s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:04.631826 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:04.632825 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:21:04.632825 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:04.772808 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:21:04.775809 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=093af805-14d8-4413-8a67-dfa06dda5142) - Closing cursor
[0m00:21:04.778810 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.4650695323944092s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:04.778810 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.4660696983337402s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:04.779811 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:04.779811 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:21:04.780811 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:04.948851 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:21:04.954853 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=812550b1-0016-4d9e-a070-ccd551ba93e0) - Closing cursor
[0m00:21:04.959854 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.6471142768859863s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:04.960854 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.6481144428253174s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:04.961854 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:04.962855 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:21:04.963856 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:05.103887 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:21:05.105887 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=b3ff7bc0-2f82-4f77-86d3-731c0d02978d) - Closing cursor
[0m00:21:05.108888 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7961480617523193s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:05.109888 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7971482276916504s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:05.110888 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:05.110888 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:21:05.111888 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:05.271924 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:21:05.274925 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=ffb58a22-c6b4-432c-8a8f-b4755253cc07) - Closing cursor
[0m00:21:05.277927 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.965186595916748s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:05.278926 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.9661858081817627s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:05.278926 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:05.279926 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:21:05.279926 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:05.419958 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:21:05.423958 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=92d269bd-fa1f-41f4-b278-3601dea257dd) - Closing cursor
[0m00:21:05.424959 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:05.426959 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=2.1132187843322754s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:05.426959 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=2.1142189502716064s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:05.427959 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:05.427959 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:21:05.428959 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:09.012695 [debug] [Thread-1 (]: SQL status: OK in 3.5799999237060547 seconds
[0m00:21:09.013694 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=368cc961-521c-4f9e-9fa8-7c7e230705f3) - Closing cursor
[0m00:21:09.017696 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:21:09.018697 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.705957412719727s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:09.019698 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.706958532333374s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:09.020698 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:21:09.020698 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:21:09.021697 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:09.450216 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m00:21:09.451217 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=d2b2fbd7-59ed-429d-9b2b-5d4483061c04) - Closing cursor
[0m00:21:09.453217 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:21:09.454218 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:21:09.455235 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:21:09.455235 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51C0FDB80>]}
[0m00:21:09.456224 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 6.14s]
[0m00:21:09.458225 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:21:09.458225 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:21:09.459225 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m00:21:09.460225 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.004990100860595703s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:09.461226 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m00:21:09.462226 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.005991220474243164s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:21:09.462226 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.006991386413574219s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Acquired connection on thread (16480, 16648), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m00:21:09.463225 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:21:09.468226 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:21:09.474227 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.017992496490478516s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:09.474227 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.01899266242980957s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:09.475228 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:09.475228 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:21:09.476228 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:09.762293 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:21:09.767295 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=4f6ce670-51fd-4d36-b799-260df97fe904) - Closing cursor
[0m00:21:09.770295 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.3150608539581299s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:09.771296 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.31606125831604004s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:09.771296 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:09.772296 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
select * from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:21:09.773295 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:09.930140 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:21:09.935141 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=97707a97-31b3-418b-a907-9333bcbcc41f) - Closing cursor
[0m00:21:09.940146 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.4849112033843994s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:09.941144 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.48590922355651855s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:09.942145 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:09.942145 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:21:09.943144 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:10.116183 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:21:10.120183 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=83723f87-58da-4441-9417-cb68839eea41) - Closing cursor
[0m00:21:10.125185 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.669950008392334s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:10.127185 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.6719505786895752s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:10.128186 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:10.129186 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:21:10.131186 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:10.289226 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:21:10.292227 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=cecaf675-b185-4218-981f-17cef310aff7) - Closing cursor
[0m00:21:10.297228 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.8419935703277588s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:10.298228 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.8429932594299316s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:10.298228 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:10.300228 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:21:10.301228 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:10.801781 [debug] [Thread-1 (]: SQL status: OK in 0.5 seconds
[0m00:21:10.803778 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=69886f90-ef11-4711-aeea-eb601e2619b1) - Closing cursor
[0m00:21:10.808094 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.3518590927124023s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:10.809094 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.3528594970703125s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:10.809094 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:10.810094 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:21:10.810094 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:10.953064 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:21:10.956065 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=f61cdcac-9aa7-4554-adee-ebf0ed1174cb) - Closing cursor
[0m00:21:10.959066 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.503831148147583s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:10.959066 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.503831148147583s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:10.960066 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:10.961067 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:21:10.962068 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:11.122105 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:21:11.125106 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=8f4986f6-51dc-490a-8634-cf766f3189da) - Closing cursor
[0m00:21:11.128109 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.672874927520752s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:11.129108 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.6738731861114502s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:11.129108 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:11.130109 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:21:11.130109 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:11.294965 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:21:11.297967 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=e1acce6f-d41b-4c58-86c1-848ea77c5183) - Closing cursor
[0m00:21:11.301968 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.8457322120666504s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:11.301968 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.8467333316802979s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:11.302968 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:11.303968 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:21:11.303968 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:11.472007 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:21:11.475008 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=38d28b4f-4b7b-4895-898c-311d1c45c6e2) - Closing cursor
[0m00:21:11.479167 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.0229318141937256s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:11.479167 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.0239319801330566s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:11.480168 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:11.480168 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:21:11.481169 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:11.621199 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:21:11.624201 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=d72a2a2b-d497-4a42-b6e3-6619e76f8fab) - Closing cursor
[0m00:21:11.626200 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:11.628201 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.1719653606414795s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:11.628201 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.172966480255127s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:11.629202 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:11.630203 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderheader_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:21:11.631201 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:15.658884 [debug] [Thread-1 (]: SQL status: OK in 4.03000020980835 seconds
[0m00:21:15.660885 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=43128849-8594-4001-b15d-c267b53ad6cb) - Closing cursor
[0m00:21:15.663886 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:21:15.665886 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=6.21065092086792s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Checking idleness
[0m00:21:15.666885 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=6.21065092086792s, acquire-count=1, language=sql, thread-identifier=(16480, 16648), compute-name=) - Retrieving connection
[0m00:21:15.666885 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:21:15.667886 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:21:15.667886 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=Unknown) - Created cursor
[0m00:21:16.093009 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m00:21:16.095009 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b990f810-977f-4d36-8abe-b89db266cdd3, command-id=9d9cb8be-94ad-475c-98b9-f95d6f393541) - Closing cursor
[0m00:21:16.096010 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:21:16.098010 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:21:16.098010 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1808649837632, session-id=b990f810-977f-4d36-8abe-b89db266cdd3, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16480, 16648), compute-name=) - Released connection
[0m00:21:16.099011 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9cd655a-9037-40e4-b5cf-42b72f4f2f0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51C125B80>]}
[0m00:21:16.100011 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 6.64s]
[0m00:21:16.101012 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:21:16.103011 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=34.310750246047974s, acquire-count=0, language=None, thread-identifier=(16480, 11912), compute-name=) - Checking idleness
[0m00:21:16.104011 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=34.311750650405884s, acquire-count=0, language=None, thread-identifier=(16480, 11912), compute-name=) - Reusing connection previously named master
[0m00:21:16.104011 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=34.311750650405884s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Acquired connection on thread (16480, 11912), using default compute resource
[0m00:21:16.105012 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=34.31275129318237s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Checking idleness
[0m00:21:16.106011 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=None, name=master, idle-time=34.31375074386597s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Retrieving connection
[0m00:21:16.106011 [debug] [MainThread]: On master: ROLLBACK
[0m00:21:16.107012 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:21:16.296055 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=bfdebc41-71a3-48fd-90ac-0b9b37748933, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Connection created
[0m00:21:16.296055 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:21:16.297055 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=bfdebc41-71a3-48fd-90ac-0b9b37748933, name=master, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Checking idleness
[0m00:21:16.298057 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=bfdebc41-71a3-48fd-90ac-0b9b37748933, name=master, idle-time=0.0020020008087158203s, acquire-count=1, language=None, thread-identifier=(16480, 11912), compute-name=) - Retrieving connection
[0m00:21:16.298057 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:21:16.299055 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:21:16.299055 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1808645727984, session-id=bfdebc41-71a3-48fd-90ac-0b9b37748933, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16480, 11912), compute-name=) - Released connection
[0m00:21:16.300056 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:21:16.300056 [debug] [MainThread]: On master: ROLLBACK
[0m00:21:16.301056 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:21:16.301056 [debug] [MainThread]: On master: Close
[0m00:21:16.302056 [debug] [MainThread]: Databricks adapter: Connection(session-id=bfdebc41-71a3-48fd-90ac-0b9b37748933) - Closing connection
[0m00:21:16.365070 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:21:16.366071 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:21:16.366071 [debug] [MainThread]: Databricks adapter: Connection(session-id=21318b34-53b2-4046-b6e0-5a4d474006cf) - Closing connection
[0m00:21:16.427087 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:21:16.427087 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:21:16.428088 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:21:16.428088 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:21:16.429088 [debug] [MainThread]: Databricks adapter: Connection(session-id=861e0cf0-70a0-442f-9f66-c9fb58667614) - Closing connection
[0m00:21:16.496103 [debug] [MainThread]: Connection 'snapshot.medallion_spark.salesorderheader_snapshot' was properly closed.
[0m00:21:16.497104 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: ROLLBACK
[0m00:21:16.497104 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:21:16.498104 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: Close
[0m00:21:16.499105 [debug] [MainThread]: Databricks adapter: Connection(session-id=b990f810-977f-4d36-8abe-b89db266cdd3) - Closing connection
[0m00:21:16.564119 [info ] [MainThread]: 
[0m00:21:16.565119 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 36.84 seconds (36.84s).
[0m00:21:16.567120 [debug] [MainThread]: Command end result
[0m00:21:16.621131 [info ] [MainThread]: 
[0m00:21:16.622144 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m00:21:16.623140 [info ] [MainThread]: 
[0m00:21:16.624139 [error] [MainThread]:   Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`customer` doesn't exist.
[0m00:21:16.626139 [info ] [MainThread]: 
[0m00:21:16.627142 [error] [MainThread]:   Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
[0m00:21:16.628140 [info ] [MainThread]: 
[0m00:21:16.629141 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=0 TOTAL=7
[0m00:21:16.631141 [debug] [MainThread]: Command `dbt snapshot` failed at 00:21:16.631141 after 39.49 seconds
[0m00:21:16.632141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A50030B740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5003098E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A51BA76570>]}
[0m00:21:16.632141 [debug] [MainThread]: Flushing usage events
[0m00:28:07.848826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B592A975F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B59524CC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B595426420>]}


============================== 00:28:07.853828 | 5faa18ae-4ba1-4095-b4ec-30a55ca5b60a ==============================
[0m00:28:07.853828 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:28:07.855829 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:28:08.031867 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:28:08.031867 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:28:08.032868 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:28:09.584219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5faa18ae-4ba1-4095-b4ec-30a55ca5b60a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5945F0560>]}
[0m00:28:09.654234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5faa18ae-4ba1-4095-b4ec-30a55ca5b60a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B595A490D0>]}
[0m00:28:09.655234 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:28:09.669239 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:28:09.888287 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:28:09.889287 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:28:09.947302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5faa18ae-4ba1-4095-b4ec-30a55ca5b60a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5B0316BD0>]}
[0m00:28:10.174353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5faa18ae-4ba1-4095-b4ec-30a55ca5b60a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5B023FE90>]}
[0m00:28:10.175353 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:28:10.176353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5faa18ae-4ba1-4095-b4ec-30a55ca5b60a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5B01007A0>]}
[0m00:28:10.178353 [info ] [MainThread]: 
[0m00:28:10.179352 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14340, 13708), compute-name=) - Creating connection
[0m00:28:10.180354 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:28:10.180354 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Acquired connection on thread (14340, 13708), using default compute resource
[0m00:28:10.187355 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855154432, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14340, 16320), compute-name=) - Creating connection
[0m00:28:10.187355 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:28:10.188355 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855154432, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14340, 16320), compute-name=) - Acquired connection on thread (14340, 16320), using default compute resource
[0m00:28:10.188355 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855154432, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14340, 16320), compute-name=) - Checking idleness
[0m00:28:10.189355 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855154432, session-id=None, name=list_hive_metastore, idle-time=0.0009999275207519531s, acquire-count=1, language=None, thread-identifier=(14340, 16320), compute-name=) - Retrieving connection
[0m00:28:10.189355 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:28:10.190355 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:28:10.190355 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:28:10.525870 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855154432, session-id=2f36f4a2-98b5-4f6c-881e-14047d63b4c4, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14340, 16320), compute-name=) - Connection created
[0m00:28:10.525870 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=2f36f4a2-98b5-4f6c-881e-14047d63b4c4, command-id=Unknown) - Created cursor
[0m00:28:10.698256 [debug] [ThreadPool]: SQL status: OK in 0.5099999904632568 seconds
[0m00:28:10.701256 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=2f36f4a2-98b5-4f6c-881e-14047d63b4c4, command-id=6a1f88ba-5417-4f26-a4ff-1d09d97f1f16) - Closing cursor
[0m00:28:10.702256 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855154432, session-id=2f36f4a2-98b5-4f6c-881e-14047d63b4c4, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14340, 16320), compute-name=) - Released connection
[0m00:28:10.704258 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14340, 10132), compute-name=) - Creating connection
[0m00:28:10.704258 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:28:10.705259 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Acquired connection on thread (14340, 10132), using default compute resource
[0m00:28:10.705259 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Checking idleness
[0m00:28:10.706259 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Retrieving connection
[0m00:28:10.706259 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:28:10.707259 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:28:10.707259 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:28:10.954384 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Connection created
[0m00:28:10.955384 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=Unknown) - Created cursor
[0m00:28:11.059942 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m00:28:11.064945 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=7f2b9594-8c87-4059-83dd-738d83c226f5) - Closing cursor
[0m00:28:11.077948 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_saleslt, idle-time=0.12356376647949219s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Checking idleness
[0m00:28:11.077948 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_saleslt, idle-time=0.12456393241882324s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Retrieving connection
[0m00:28:11.078948 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_saleslt, idle-time=0.1255638599395752s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Checking idleness
[0m00:28:11.078948 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_saleslt, idle-time=0.1255638599395752s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Retrieving connection
[0m00:28:11.079948 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:28:11.079948 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:28:11.080949 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:28:11.080949 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=Unknown) - Created cursor
[0m00:28:11.218957 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:28:11.222959 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=36ba5d21-de47-4d01-8761-37f641ab63d3) - Closing cursor
[0m00:28:11.228960 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_saleslt, idle-time=0.2755758762359619s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Checking idleness
[0m00:28:11.229959 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_saleslt, idle-time=0.27657532691955566s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Retrieving connection
[0m00:28:11.229959 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:28:11.230960 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:28:11.230960 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=Unknown) - Created cursor
[0m00:28:11.471006 [debug] [ThreadPool]: SQL status: OK in 0.23999999463558197 seconds
[0m00:28:11.474007 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=cb25d7db-4de3-4b43-a25e-ced6e741ea22) - Closing cursor
[0m00:28:11.475009 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14340, 10132), compute-name=) - Released connection
[0m00:28:11.475009 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14340, 10132), compute-name=) - Checking idleness
[0m00:28:11.478009 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:28:11.479009 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_snapshots, idle-time=0.0039997100830078125s, acquire-count=0, language=None, thread-identifier=(14340, 10132), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m00:28:11.480010 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_snapshots, idle-time=0.0050013065338134766s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Acquired connection on thread (14340, 10132), using default compute resource
[0m00:28:11.481010 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_snapshots, idle-time=0.006000995635986328s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Checking idleness
[0m00:28:11.482010 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_snapshots, idle-time=0.007000923156738281s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Retrieving connection
[0m00:28:11.483009 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:28:11.484010 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:28:11.485011 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=Unknown) - Created cursor
[0m00:28:11.575001 [debug] [ThreadPool]: SQL status: OK in 0.09000000357627869 seconds
[0m00:28:11.579003 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=43fb2865-5fd0-4d5f-a42d-9ffcf6aaac55) - Closing cursor
[0m00:28:11.583004 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_snapshots, idle-time=0.10799551010131836s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Checking idleness
[0m00:28:11.584004 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_snapshots, idle-time=0.10799551010131836s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Retrieving connection
[0m00:28:11.584004 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:28:11.584004 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:28:11.585004 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=Unknown) - Created cursor
[0m00:28:11.733038 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:28:11.735040 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=390cbf04-0a54-406c-9273-e69be96b95cc) - Closing cursor
[0m00:28:11.738040 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_snapshots, idle-time=0.2630314826965332s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Checking idleness
[0m00:28:11.738040 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_snapshots, idle-time=0.2630314826965332s, acquire-count=1, language=None, thread-identifier=(14340, 10132), compute-name=) - Retrieving connection
[0m00:28:11.739040 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:28:11.739040 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:28:11.740039 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=Unknown) - Created cursor
[0m00:28:11.879573 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:28:11.882574 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, command-id=dd58c44e-1609-4fad-b078-9c46beb4515b) - Closing cursor
[0m00:28:11.883575 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1879855533616, session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14340, 10132), compute-name=) - Released connection
[0m00:28:11.886576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5faa18ae-4ba1-4095-b4ec-30a55ca5b60a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B59303EFC0>]}
[0m00:28:11.887576 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=1.7072217464447021s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Checking idleness
[0m00:28:11.888576 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=1.7082223892211914s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Retrieving connection
[0m00:28:11.889576 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=1.709221601486206s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Checking idleness
[0m00:28:11.890576 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=1.710221529006958s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Retrieving connection
[0m00:28:11.891577 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:28:11.892577 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:28:11.892577 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14340, 13708), compute-name=) - Released connection
[0m00:28:11.894577 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:28:11.894577 [info ] [MainThread]: 
[0m00:28:11.898578 [debug] [Thread-1 (]: Began running node model.medallion_spark.dim_customer
[0m00:28:11.899578 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m00:28:11.900578 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14340, 10808), compute-name=) - Creating connection
[0m00:28:11.900578 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_spark.dim_customer'
[0m00:28:11.901578 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Acquired connection on thread (14340, 10808), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m00:28:11.901578 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.dim_customer
[0m00:28:11.911581 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.dim_customer"
[0m00:28:11.912581 [debug] [Thread-1 (]: Began executing node model.medallion_spark.dim_customer
[0m00:28:11.926584 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:28:11.932585 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.031006813049316406s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Checking idleness
[0m00:28:11.933586 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.032007694244384766s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Retrieving connection
[0m00:28:11.934585 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.032007694244384766s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Checking idleness
[0m00:28:11.934585 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.033007144927978516s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Retrieving connection
[0m00:28:11.935586 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:28:11.935586 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.dim_customer"
[0m00:28:11.936586 [debug] [Thread-1 (]: On model.medallion_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
[0m00:28:11.936586 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:28:12.094635 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Connection created
[0m00:28:12.095637 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, command-id=Unknown) - Created cursor
[0m00:28:12.316085 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, command-id=Unknown) - Closing cursor
[0m00:28:12.317086 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=41855ced-c4fd-412d-9a99-2c4b364ebb2d
[0m00:28:12.318086 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Runtime Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m00:28:12.319086 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14340, 10808), compute-name=) - Released connection
[0m00:28:12.329088 [debug] [Thread-1 (]: Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m00:28:12.330088 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14340, 10808), compute-name=) - Released connection
[0m00:28:12.332089 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5faa18ae-4ba1-4095-b4ec-30a55ca5b60a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5B06F4B60>]}
[0m00:28:12.333089 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model saleslt.dim_customer ..................... [[31mERROR[0m in 0.43s]
[0m00:28:12.334090 [debug] [Thread-1 (]: Finished running node model.medallion_spark.dim_customer
[0m00:28:12.335089 [debug] [Thread-1 (]: Began running node model.medallion_spark.dim_product
[0m00:28:12.336090 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m00:28:12.337090 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_customer, idle-time=0.007001161575317383s, acquire-count=0, language=sql, thread-identifier=(14340, 10808), compute-name=) - Checking idleness
[0m00:28:12.337090 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.dim_customer, now model.medallion_spark.dim_product)
[0m00:28:12.338090 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_product, idle-time=0.008001565933227539s, acquire-count=0, language=sql, thread-identifier=(14340, 10808), compute-name=) - Reusing connection previously named model.medallion_spark.dim_customer
[0m00:28:12.338090 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_product, idle-time=0.008001565933227539s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Acquired connection on thread (14340, 10808), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m00:28:12.339090 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.dim_product
[0m00:28:12.343091 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.dim_product"
[0m00:28:12.344091 [debug] [Thread-1 (]: Began executing node model.medallion_spark.dim_product
[0m00:28:12.346093 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:28:12.349093 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_product, idle-time=0.01900482177734375s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Checking idleness
[0m00:28:12.350093 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_product, idle-time=0.0200045108795166s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Retrieving connection
[0m00:28:12.350093 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.dim_product"
[0m00:28:12.351094 [debug] [Thread-1 (]: On model.medallion_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m00:28:12.351094 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, command-id=Unknown) - Created cursor
[0m00:28:12.506138 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, command-id=Unknown) - Closing cursor
[0m00:28:12.508138 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=01a654a3-f40d-4e0e-8653-25707d5f86b6
[0m00:28:12.509138 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Runtime Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m00:28:12.510139 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14340, 10808), compute-name=) - Released connection
[0m00:28:12.516140 [debug] [Thread-1 (]: Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m00:28:12.517140 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14340, 10808), compute-name=) - Released connection
[0m00:28:12.517140 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5faa18ae-4ba1-4095-b4ec-30a55ca5b60a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5B06E0C80>]}
[0m00:28:12.518140 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 0.18s]
[0m00:28:12.520141 [debug] [Thread-1 (]: Finished running node model.medallion_spark.dim_product
[0m00:28:12.520141 [debug] [Thread-1 (]: Began running node model.medallion_spark.sales
[0m00:28:12.521141 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m00:28:12.523141 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.dim_product, idle-time=0.00600123405456543s, acquire-count=0, language=sql, thread-identifier=(14340, 10808), compute-name=) - Checking idleness
[0m00:28:12.523141 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.dim_product, now model.medallion_spark.sales)
[0m00:28:12.524142 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.sales, idle-time=0.007001399993896484s, acquire-count=0, language=sql, thread-identifier=(14340, 10808), compute-name=) - Reusing connection previously named model.medallion_spark.dim_product
[0m00:28:12.525142 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.sales, idle-time=0.008001565933227539s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Acquired connection on thread (14340, 10808), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m00:28:12.526142 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.sales
[0m00:28:12.531143 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.sales"
[0m00:28:12.532143 [debug] [Thread-1 (]: Began executing node model.medallion_spark.sales
[0m00:28:12.534143 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:28:12.589156 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.sales"
[0m00:28:12.590156 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.sales, idle-time=0.07301592826843262s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Checking idleness
[0m00:28:12.591157 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.sales, idle-time=0.07401680946350098s, acquire-count=1, language=sql, thread-identifier=(14340, 10808), compute-name=) - Retrieving connection
[0m00:28:12.591157 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.sales"
[0m00:28:12.592156 [debug] [Thread-1 (]: On model.medallion_spark.sales: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m00:28:12.593157 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, command-id=Unknown) - Created cursor
[0m00:28:13.133402 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, command-id=Unknown) - Closing cursor
[0m00:28:13.136403 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_TABLE_NOT_FOUND] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable(DeltaErrors.scala:1450)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable$(DeltaErrors.scala:1449)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.nonExistentDeltaTable(DeltaErrors.scala:3323)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:336)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:321)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.$anonfun$fromV2Relation$1(DeltaAnalysis.scala:1630)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.recordFrameProfile(DeltaAnalysis.scala:1615)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.fromV2Relation(DeltaAnalysis.scala:1629)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:649)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1985)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1279)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1278)
	at org.apache.spark.sql.catalyst.plans.logical.CTERelationDef.mapChildren(basicLogicalOperators.scala:944)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:716)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at scala.collection.immutable.List.map(List.scala:297)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:716)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:400)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:39)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:107)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:101)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:415)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:408)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:322)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:408)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:341)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:393)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:392)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:576)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1097)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:576)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:572)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:572)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:552)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:522)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:512)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:522)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:595)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:595)
	... 35 more
, operation-id=a1b86726-f42d-4a66-94fc-518ac1ad11c7
[0m00:28:13.140403 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14340, 10808), compute-name=) - Released connection
[0m00:28:13.145405 [debug] [Thread-1 (]: Runtime Error in model sales (models\marts\sales\sales.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
[0m00:28:13.146405 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1879860787440, session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14340, 10808), compute-name=) - Released connection
[0m00:28:13.146405 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5faa18ae-4ba1-4095-b4ec-30a55ca5b60a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5B0826510>]}
[0m00:28:13.147405 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model saleslt.sales ............................ [[31mERROR[0m in 0.62s]
[0m00:28:13.148405 [debug] [Thread-1 (]: Finished running node model.medallion_spark.sales
[0m00:28:13.150406 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=1.2578294277191162s, acquire-count=0, language=None, thread-identifier=(14340, 13708), compute-name=) - Checking idleness
[0m00:28:13.151406 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=1.2578294277191162s, acquire-count=0, language=None, thread-identifier=(14340, 13708), compute-name=) - Reusing connection previously named master
[0m00:28:13.151406 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=1.2588293552398682s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Acquired connection on thread (14340, 13708), using default compute resource
[0m00:28:13.152406 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=1.2598297595977783s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Checking idleness
[0m00:28:13.153406 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=None, name=master, idle-time=1.260829210281372s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Retrieving connection
[0m00:28:13.153406 [debug] [MainThread]: On master: ROLLBACK
[0m00:28:13.154407 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:28:13.343215 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=d6794ae9-8d72-4864-9758-31d569579d64, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Connection created
[0m00:28:13.344215 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:28:13.344215 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=d6794ae9-8d72-4864-9758-31d569579d64, name=master, idle-time=0.0009999275207519531s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Checking idleness
[0m00:28:13.345216 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=d6794ae9-8d72-4864-9758-31d569579d64, name=master, idle-time=0.002000093460083008s, acquire-count=1, language=None, thread-identifier=(14340, 13708), compute-name=) - Retrieving connection
[0m00:28:13.345216 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:28:13.346216 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:28:13.346216 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1879855849472, session-id=d6794ae9-8d72-4864-9758-31d569579d64, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14340, 13708), compute-name=) - Released connection
[0m00:28:13.347216 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:28:13.347216 [debug] [MainThread]: On master: ROLLBACK
[0m00:28:13.348216 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:28:13.348216 [debug] [MainThread]: On master: Close
[0m00:28:13.349217 [debug] [MainThread]: Databricks adapter: Connection(session-id=d6794ae9-8d72-4864-9758-31d569579d64) - Closing connection
[0m00:28:13.435227 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:28:13.436228 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:28:13.436228 [debug] [MainThread]: Databricks adapter: Connection(session-id=2f36f4a2-98b5-4f6c-881e-14047d63b4c4) - Closing connection
[0m00:28:13.498234 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:28:13.499236 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:28:13.500235 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:28:13.500235 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:28:13.501236 [debug] [MainThread]: Databricks adapter: Connection(session-id=6d9eb535-df7b-4c29-95eb-63088b135a3e) - Closing connection
[0m00:28:13.570724 [debug] [MainThread]: Connection 'model.medallion_spark.sales' was properly closed.
[0m00:28:13.571727 [debug] [MainThread]: On model.medallion_spark.sales: ROLLBACK
[0m00:28:13.571727 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:28:13.572727 [debug] [MainThread]: On model.medallion_spark.sales: Close
[0m00:28:13.572727 [debug] [MainThread]: Databricks adapter: Connection(session-id=18fbd129-3bd1-401f-b13d-b33356f56bc9) - Closing connection
[0m00:28:13.677129 [info ] [MainThread]: 
[0m00:28:13.678130 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 3.50 seconds (3.50s).
[0m00:28:13.679129 [debug] [MainThread]: Command end result
[0m00:28:13.726140 [info ] [MainThread]: 
[0m00:28:13.727140 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m00:28:13.728141 [info ] [MainThread]: 
[0m00:28:13.729140 [error] [MainThread]:   Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m00:28:13.730142 [info ] [MainThread]: 
[0m00:28:13.731141 [error] [MainThread]:   Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  Runtime Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m00:28:13.731141 [info ] [MainThread]: 
[0m00:28:13.732140 [error] [MainThread]:   Runtime Error in model sales (models\marts\sales\sales.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `saleslt`.`product` doesn't exist.
[0m00:28:13.733142 [info ] [MainThread]: 
[0m00:28:13.733142 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 TOTAL=3
[0m00:28:13.735142 [debug] [MainThread]: Command `dbt run` failed at 00:28:13.735142 after 6.03 seconds
[0m00:28:13.736142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5951763F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5951760F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5B0808410>]}
[0m00:28:13.736142 [debug] [MainThread]: Flushing usage events
[0m00:29:23.372899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CDDF06BA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CDDF4CBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CDDF4CAA0>]}


============================== 00:29:23.377900 | db10eab1-eb39-479f-9e8d-297185273794 ==============================
[0m00:29:23.377900 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:29:23.378903 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m00:29:23.545939 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:29:23.545939 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:29:23.546940 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:29:25.066282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'db10eab1-eb39-479f-9e8d-297185273794', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CDE3304A0>]}
[0m00:29:25.126295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'db10eab1-eb39-479f-9e8d-297185273794', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CDDDFE780>]}
[0m00:29:25.127295 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:29:25.141299 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:29:25.363349 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:29:25.364349 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:29:25.419361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'db10eab1-eb39-479f-9e8d-297185273794', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CF8DC4EF0>]}
[0m00:29:25.647413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'db10eab1-eb39-479f-9e8d-297185273794', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CF8F13A10>]}
[0m00:29:25.647413 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:29:25.648412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'db10eab1-eb39-479f-9e8d-297185273794', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CF8FE2960>]}
[0m00:29:25.650414 [info ] [MainThread]: 
[0m00:29:25.651414 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(10760, 17344), compute-name=) - Creating connection
[0m00:29:25.651414 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:29:25.652415 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Acquired connection on thread (10760, 17344), using default compute resource
[0m00:29:25.659415 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392177006336, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(10760, 17084), compute-name=) - Creating connection
[0m00:29:25.660416 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:29:25.661416 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392177006336, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(10760, 17084), compute-name=) - Acquired connection on thread (10760, 17084), using default compute resource
[0m00:29:25.661416 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392177006336, session-id=None, name=list_hive_metastore, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(10760, 17084), compute-name=) - Checking idleness
[0m00:29:25.662417 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392177006336, session-id=None, name=list_hive_metastore, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(10760, 17084), compute-name=) - Retrieving connection
[0m00:29:25.662417 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:29:25.662417 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:29:25.663417 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:29:25.969436 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392177006336, session-id=0326d166-2569-4ede-9e00-f980665a40ad, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(10760, 17084), compute-name=) - Connection created
[0m00:29:25.970435 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0326d166-2569-4ede-9e00-f980665a40ad, command-id=Unknown) - Created cursor
[0m00:29:26.076561 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m00:29:26.079561 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0326d166-2569-4ede-9e00-f980665a40ad, command-id=12678b76-d489-419f-b51d-c74744289477) - Closing cursor
[0m00:29:26.080561 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392177006336, session-id=0326d166-2569-4ede-9e00-f980665a40ad, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(10760, 17084), compute-name=) - Released connection
[0m00:29:26.082561 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, acquire-count=0, language=None, thread-identifier=(10760, 7304), compute-name=) - Creating connection
[0m00:29:26.083562 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:29:26.083562 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Acquired connection on thread (10760, 7304), using default compute resource
[0m00:29:26.084563 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0010006427764892578s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Checking idleness
[0m00:29:26.085563 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0020003318786621094s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Retrieving connection
[0m00:29:26.086563 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:29:26.086563 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:29:26.087564 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:29:26.256926 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Connection created
[0m00:29:26.257926 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=Unknown) - Created cursor
[0m00:29:26.354944 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m00:29:26.359944 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=19e054c8-90d0-405b-8e32-30e2d9454468) - Closing cursor
[0m00:29:26.374948 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_snapshots, idle-time=0.11802220344543457s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Checking idleness
[0m00:29:26.374948 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_snapshots, idle-time=0.11802220344543457s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Retrieving connection
[0m00:29:26.375949 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_snapshots, idle-time=0.11902332305908203s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Checking idleness
[0m00:29:26.375949 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_snapshots, idle-time=0.11902332305908203s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Retrieving connection
[0m00:29:26.376948 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:29:26.376948 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:29:26.377949 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:29:26.377949 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=Unknown) - Created cursor
[0m00:29:26.489954 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:29:26.491955 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=0a7ef241-5828-4c3c-a0a1-100d74401d04) - Closing cursor
[0m00:29:26.497957 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_snapshots, idle-time=0.24103116989135742s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Checking idleness
[0m00:29:26.498957 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_snapshots, idle-time=0.24203109741210938s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Retrieving connection
[0m00:29:26.498957 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:29:26.499957 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:29:26.499957 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=Unknown) - Created cursor
[0m00:29:26.652007 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:29:26.655007 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=700095da-4acb-44f4-8e31-ce9d1ee4c74f) - Closing cursor
[0m00:29:26.656007 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(10760, 7304), compute-name=) - Released connection
[0m00:29:26.657007 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_snapshots, idle-time=0.0009996891021728516s, acquire-count=0, language=None, thread-identifier=(10760, 7304), compute-name=) - Checking idleness
[0m00:29:26.659008 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:29:26.660009 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_saleslt, idle-time=0.004001617431640625s, acquire-count=0, language=None, thread-identifier=(10760, 7304), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m00:29:26.660009 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_saleslt, idle-time=0.004001617431640625s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Acquired connection on thread (10760, 7304), using default compute resource
[0m00:29:26.661008 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_saleslt, idle-time=0.005000591278076172s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Checking idleness
[0m00:29:26.661008 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_saleslt, idle-time=0.005000591278076172s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Retrieving connection
[0m00:29:26.662009 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:29:26.662009 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:29:26.663009 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=Unknown) - Created cursor
[0m00:29:26.782182 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m00:29:26.785182 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=89e336bc-ad2e-462a-b9f1-23ddd5a0504d) - Closing cursor
[0m00:29:26.788183 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_saleslt, idle-time=0.13217544555664062s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Checking idleness
[0m00:29:26.788183 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_saleslt, idle-time=0.13217544555664062s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Retrieving connection
[0m00:29:26.789184 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:29:26.789184 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:29:26.790184 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=Unknown) - Created cursor
[0m00:29:26.891231 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m00:29:26.894232 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=51ba83dd-0126-4234-8f03-302bc1c4391a) - Closing cursor
[0m00:29:26.897232 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_saleslt, idle-time=0.2412247657775879s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Checking idleness
[0m00:29:26.898232 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_saleslt, idle-time=0.24222469329833984s, acquire-count=1, language=None, thread-identifier=(10760, 7304), compute-name=) - Retrieving connection
[0m00:29:26.899234 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:29:26.899234 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:29:26.900233 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=Unknown) - Created cursor
[0m00:29:27.042034 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:29:27.045033 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, command-id=fac24028-30d2-48dc-807a-972e11099f99) - Closing cursor
[0m00:29:27.046034 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2392178659776, session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(10760, 7304), compute-name=) - Released connection
[0m00:29:27.050034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'db10eab1-eb39-479f-9e8d-297185273794', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CDE2E7C50>]}
[0m00:29:27.051035 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=1.3986201286315918s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Checking idleness
[0m00:29:27.052035 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=1.3996202945709229s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Retrieving connection
[0m00:29:27.052035 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=1.3996202945709229s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Checking idleness
[0m00:29:27.053034 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=1.4006195068359375s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Retrieving connection
[0m00:29:27.054035 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:29:27.055035 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:29:27.055035 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(10760, 17344), compute-name=) - Released connection
[0m00:29:27.056036 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:29:27.057036 [info ] [MainThread]: 
[0m00:29:27.060037 [debug] [Thread-1 (]: Began running node model.medallion_spark.dim_customer
[0m00:29:27.061037 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m00:29:27.062037 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0s, acquire-count=0, language=None, thread-identifier=(10760, 16276), compute-name=) - Creating connection
[0m00:29:27.063037 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_spark.dim_customer'
[0m00:29:27.064037 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Acquired connection on thread (10760, 16276), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m00:29:27.064037 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.dim_customer
[0m00:29:27.074041 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.dim_customer"
[0m00:29:27.075040 [debug] [Thread-1 (]: Began executing node model.medallion_spark.dim_customer
[0m00:29:27.089043 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:29:27.096045 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.03300786018371582s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Checking idleness
[0m00:29:27.097044 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.03400683403015137s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Retrieving connection
[0m00:29:27.097044 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.03400683403015137s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Checking idleness
[0m00:29:27.098046 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.03500843048095703s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Retrieving connection
[0m00:29:27.099045 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:29:27.099045 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.dim_customer"
[0m00:29:27.100046 [debug] [Thread-1 (]: On model.medallion_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
[0m00:29:27.100046 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:29:27.252383 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Connection created
[0m00:29:27.253383 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, command-id=Unknown) - Created cursor
[0m00:29:27.678024 [debug] [Thread-1 (]: SQL status: OK in 0.5799999833106995 seconds
[0m00:29:27.682024 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, command-id=8aebb021-ee26-450a-8dab-ca29c0270ee9) - Closing cursor
[0m00:29:27.732036 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.dim_customer"
[0m00:29:27.733036 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_customer, idle-time=0.4806528091430664s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Checking idleness
[0m00:29:27.734036 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_customer, idle-time=0.48165321350097656s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Retrieving connection
[0m00:29:27.734036 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.dim_customer"
[0m00:29:27.735036 [debug] [Thread-1 (]: On model.medallion_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m00:29:27.736037 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, command-id=Unknown) - Created cursor
[0m00:29:31.493821 [debug] [Thread-1 (]: SQL status: OK in 3.759999990463257 seconds
[0m00:29:31.495822 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, command-id=158b9b3b-559a-4809-b7a6-be6c37c4b516) - Closing cursor
[0m00:29:31.533830 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(10760, 16276), compute-name=) - Released connection
[0m00:29:31.534831 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(10760, 16276), compute-name=) - Released connection
[0m00:29:31.535831 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db10eab1-eb39-479f-9e8d-297185273794', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CF94D2D50>]}
[0m00:29:31.536831 [info ] [Thread-1 (]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 4.47s]
[0m00:29:31.537831 [debug] [Thread-1 (]: Finished running node model.medallion_spark.dim_customer
[0m00:29:31.538831 [debug] [Thread-1 (]: Began running node model.medallion_spark.dim_product
[0m00:29:31.538831 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m00:29:31.539832 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_customer, idle-time=0.005001068115234375s, acquire-count=0, language=sql, thread-identifier=(10760, 16276), compute-name=) - Checking idleness
[0m00:29:31.540832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.dim_customer, now model.medallion_spark.dim_product)
[0m00:29:31.540832 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_product, idle-time=0.006000995635986328s, acquire-count=0, language=sql, thread-identifier=(10760, 16276), compute-name=) - Reusing connection previously named model.medallion_spark.dim_customer
[0m00:29:31.541832 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_product, idle-time=0.007001161575317383s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Acquired connection on thread (10760, 16276), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m00:29:31.541832 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.dim_product
[0m00:29:31.545832 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.dim_product"
[0m00:29:31.546833 [debug] [Thread-1 (]: Began executing node model.medallion_spark.dim_product
[0m00:29:31.549833 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:29:31.551835 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_product, idle-time=0.017003774642944336s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Checking idleness
[0m00:29:31.552835 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_product, idle-time=0.018004417419433594s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Retrieving connection
[0m00:29:31.553835 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.dim_product"
[0m00:29:31.553835 [debug] [Thread-1 (]: On model.medallion_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m00:29:31.554835 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, command-id=Unknown) - Created cursor
[0m00:29:31.806682 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m00:29:31.809683 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, command-id=5a07e828-73e5-47e0-9b41-fbeb48c4497b) - Closing cursor
[0m00:29:31.812683 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.dim_product"
[0m00:29:31.813684 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_product, idle-time=0.278853178024292s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Checking idleness
[0m00:29:31.814684 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_product, idle-time=0.27985334396362305s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Retrieving connection
[0m00:29:31.814684 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.dim_product"
[0m00:29:31.815685 [debug] [Thread-1 (]: On model.medallion_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m00:29:31.816685 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, command-id=Unknown) - Created cursor
[0m00:29:35.385843 [debug] [Thread-1 (]: SQL status: OK in 3.569999933242798 seconds
[0m00:29:35.387844 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, command-id=4e4fb75c-9c5d-457d-8667-948681e00f0d) - Closing cursor
[0m00:29:35.389844 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(10760, 16276), compute-name=) - Released connection
[0m00:29:35.390845 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(10760, 16276), compute-name=) - Released connection
[0m00:29:35.391844 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db10eab1-eb39-479f-9e8d-297185273794', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CF913EF30>]}
[0m00:29:35.391844 [info ] [Thread-1 (]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 3.85s]
[0m00:29:35.392845 [debug] [Thread-1 (]: Finished running node model.medallion_spark.dim_product
[0m00:29:35.393845 [debug] [Thread-1 (]: Began running node model.medallion_spark.sales
[0m00:29:35.394846 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m00:29:35.395845 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.dim_product, idle-time=0.004001140594482422s, acquire-count=0, language=sql, thread-identifier=(10760, 16276), compute-name=) - Checking idleness
[0m00:29:35.395845 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.dim_product, now model.medallion_spark.sales)
[0m00:29:35.396846 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.sales, idle-time=0.005001068115234375s, acquire-count=0, language=sql, thread-identifier=(10760, 16276), compute-name=) - Reusing connection previously named model.medallion_spark.dim_product
[0m00:29:35.396846 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.sales, idle-time=0.00600123405456543s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Acquired connection on thread (10760, 16276), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m00:29:35.397846 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.sales
[0m00:29:35.401847 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.sales"
[0m00:29:35.403848 [debug] [Thread-1 (]: Began executing node model.medallion_spark.sales
[0m00:29:35.405848 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m00:29:35.409849 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.sales"
[0m00:29:35.410849 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.sales, idle-time=0.020004749298095703s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Checking idleness
[0m00:29:35.410849 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.sales, idle-time=0.020004749298095703s, acquire-count=1, language=sql, thread-identifier=(10760, 16276), compute-name=) - Retrieving connection
[0m00:29:35.411849 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.sales"
[0m00:29:35.412849 [debug] [Thread-1 (]: On model.medallion_spark.sales: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m00:29:35.412849 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, command-id=Unknown) - Created cursor
[0m00:29:35.708363 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, command-id=Unknown) - Closing cursor
[0m00:29:35.709364 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=71fdd8b0-ec7d-4fef-afec-64ded53d0500
[0m00:29:35.710365 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(10760, 16276), compute-name=) - Released connection
[0m00:29:35.717366 [debug] [Thread-1 (]: Runtime Error in model sales (models\marts\sales\sales.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
[0m00:29:35.718366 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2392184401520, session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(10760, 16276), compute-name=) - Released connection
[0m00:29:35.718366 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db10eab1-eb39-479f-9e8d-297185273794', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CF9553DD0>]}
[0m00:29:35.719366 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model saleslt.sales ............................ [[31mERROR[0m in 0.32s]
[0m00:29:35.720367 [debug] [Thread-1 (]: Finished running node model.medallion_spark.sales
[0m00:29:35.722366 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=8.667331218719482s, acquire-count=0, language=None, thread-identifier=(10760, 17344), compute-name=) - Checking idleness
[0m00:29:35.724367 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=8.668331623077393s, acquire-count=0, language=None, thread-identifier=(10760, 17344), compute-name=) - Reusing connection previously named master
[0m00:29:35.725367 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=8.669332027435303s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Acquired connection on thread (10760, 17344), using default compute resource
[0m00:29:35.726368 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=8.670331716537476s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Checking idleness
[0m00:29:35.726368 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=None, name=master, idle-time=8.671332836151123s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Retrieving connection
[0m00:29:35.728369 [debug] [MainThread]: On master: ROLLBACK
[0m00:29:35.729368 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:29:35.911207 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=0c272f3e-da5d-4ba1-aa6c-45dba515d6fc, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Connection created
[0m00:29:35.912208 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:29:35.912208 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=0c272f3e-da5d-4ba1-aa6c-45dba515d6fc, name=master, idle-time=0.0010006427764892578s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Checking idleness
[0m00:29:35.913208 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=0c272f3e-da5d-4ba1-aa6c-45dba515d6fc, name=master, idle-time=0.0020008087158203125s, acquire-count=1, language=None, thread-identifier=(10760, 17344), compute-name=) - Retrieving connection
[0m00:29:35.913208 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:29:35.914208 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:29:35.914208 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2392178375376, session-id=0c272f3e-da5d-4ba1-aa6c-45dba515d6fc, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(10760, 17344), compute-name=) - Released connection
[0m00:29:35.915209 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:29:35.915209 [debug] [MainThread]: On master: ROLLBACK
[0m00:29:35.916209 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:29:35.916209 [debug] [MainThread]: On master: Close
[0m00:29:35.917209 [debug] [MainThread]: Databricks adapter: Connection(session-id=0c272f3e-da5d-4ba1-aa6c-45dba515d6fc) - Closing connection
[0m00:29:35.978241 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:29:35.979241 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:29:35.980242 [debug] [MainThread]: Databricks adapter: Connection(session-id=0326d166-2569-4ede-9e00-f980665a40ad) - Closing connection
[0m00:29:36.061263 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:29:36.062263 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:29:36.062263 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:29:36.063263 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:29:36.063263 [debug] [MainThread]: Databricks adapter: Connection(session-id=f4d5a9a7-3f82-4818-a4b9-4b040a384e69) - Closing connection
[0m00:29:36.122299 [debug] [MainThread]: Connection 'model.medallion_spark.sales' was properly closed.
[0m00:29:36.123300 [debug] [MainThread]: On model.medallion_spark.sales: ROLLBACK
[0m00:29:36.123300 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:29:36.124300 [debug] [MainThread]: On model.medallion_spark.sales: Close
[0m00:29:36.124300 [debug] [MainThread]: Databricks adapter: Connection(session-id=0eee0db1-58ae-4ab7-9bb5-f8512281bc8b) - Closing connection
[0m00:29:36.181319 [info ] [MainThread]: 
[0m00:29:36.182319 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 10.53 seconds (10.53s).
[0m00:29:36.185320 [debug] [MainThread]: Command end result
[0m00:29:36.237332 [info ] [MainThread]: 
[0m00:29:36.237332 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:29:36.238332 [info ] [MainThread]: 
[0m00:29:36.239331 [error] [MainThread]:   Runtime Error in model sales (models\marts\sales\sales.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 33 pos 8
[0m00:29:36.239331 [info ] [MainThread]: 
[0m00:29:36.240332 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m00:29:36.242333 [debug] [MainThread]: Command `dbt run` failed at 00:29:36.242333 after 12.98 seconds
[0m00:29:36.242333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CDDEDD880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CDE23FD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022CDDDFECC0>]}
[0m00:29:36.243334 [debug] [MainThread]: Flushing usage events
[0m00:29:44.328356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EAD75E390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EAB2012B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EAD7CCE60>]}


============================== 00:29:44.333357 | 310f29b7-aabe-44e2-a3a5-2fe903d1088a ==============================
[0m00:29:44.333357 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:29:44.333357 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:29:44.502394 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:29:44.503394 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:29:44.503394 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:29:46.029740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EAE18B860>]}
[0m00:29:46.093754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EADC8D130>]}
[0m00:29:46.094754 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:29:46.107757 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:29:46.339810 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:29:46.340810 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:29:46.402824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EC8DF5FD0>]}
[0m00:29:46.629875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EC8BBEAE0>]}
[0m00:29:46.630876 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:29:46.631876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EC8DEB0E0>]}
[0m00:29:46.633876 [info ] [MainThread]: 
[0m00:29:46.634877 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15612, 9324), compute-name=) - Creating connection
[0m00:29:46.635877 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:29:46.635877 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Acquired connection on thread (15612, 9324), using default compute resource
[0m00:29:46.642878 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960637440, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15612, 14092), compute-name=) - Creating connection
[0m00:29:46.643881 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:29:46.644882 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960637440, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15612, 14092), compute-name=) - Acquired connection on thread (15612, 14092), using default compute resource
[0m00:29:46.644882 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960637440, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15612, 14092), compute-name=) - Checking idleness
[0m00:29:46.645881 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960637440, session-id=None, name=list_hive_metastore, idle-time=0.00099945068359375s, acquire-count=1, language=None, thread-identifier=(15612, 14092), compute-name=) - Retrieving connection
[0m00:29:46.646882 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:29:46.647879 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:29:46.647879 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:29:46.803842 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960637440, session-id=0fb20e05-996b-48ac-a1fe-a5b01b2580d4, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15612, 14092), compute-name=) - Connection created
[0m00:29:46.804843 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0fb20e05-996b-48ac-a1fe-a5b01b2580d4, command-id=Unknown) - Created cursor
[0m00:29:46.880751 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m00:29:46.883752 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0fb20e05-996b-48ac-a1fe-a5b01b2580d4, command-id=b003c8b8-ea20-461f-a431-1c4670e58e82) - Closing cursor
[0m00:29:46.884752 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960637440, session-id=0fb20e05-996b-48ac-a1fe-a5b01b2580d4, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15612, 14092), compute-name=) - Released connection
[0m00:29:46.886753 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15612, 5872), compute-name=) - Creating connection
[0m00:29:46.887754 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:29:46.887754 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Acquired connection on thread (15612, 5872), using default compute resource
[0m00:29:46.888754 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0009999275207519531s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Checking idleness
[0m00:29:46.888754 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0009999275207519531s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Retrieving connection
[0m00:29:46.889755 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:29:46.889755 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:29:46.890755 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:29:47.049775 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Connection created
[0m00:29:47.049775 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=Unknown) - Created cursor
[0m00:29:47.146797 [debug] [ThreadPool]: SQL status: OK in 0.25999999046325684 seconds
[0m00:29:47.151798 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=aba01383-892c-4fbd-bc52-7b94fb3e025f) - Closing cursor
[0m00:29:47.163801 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_snapshots, idle-time=0.11402535438537598s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Checking idleness
[0m00:29:47.164801 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_snapshots, idle-time=0.11502552032470703s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Retrieving connection
[0m00:29:47.164801 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_snapshots, idle-time=0.11502552032470703s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Checking idleness
[0m00:29:47.165801 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_snapshots, idle-time=0.11602568626403809s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Retrieving connection
[0m00:29:47.165801 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:29:47.166801 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:29:47.166801 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:29:47.167802 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=Unknown) - Created cursor
[0m00:29:47.316446 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:29:47.318445 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=b6a8767b-2d2e-4eb6-8195-1ca4d45708f2) - Closing cursor
[0m00:29:47.324446 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_snapshots, idle-time=0.2746710777282715s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Checking idleness
[0m00:29:47.325447 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_snapshots, idle-time=0.27567100524902344s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Retrieving connection
[0m00:29:47.325447 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:29:47.326447 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:29:47.326447 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=Unknown) - Created cursor
[0m00:29:47.479470 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:29:47.482472 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=40c979a5-4c3a-45ee-b02a-bc4940a0e654) - Closing cursor
[0m00:29:47.483471 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15612, 5872), compute-name=) - Released connection
[0m00:29:47.483471 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15612, 5872), compute-name=) - Checking idleness
[0m00:29:47.485472 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:29:47.486473 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_saleslt, idle-time=0.0030019283294677734s, acquire-count=0, language=None, thread-identifier=(15612, 5872), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m00:29:47.486473 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_saleslt, idle-time=0.0030019283294677734s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Acquired connection on thread (15612, 5872), using default compute resource
[0m00:29:47.487472 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_saleslt, idle-time=0.00400090217590332s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Checking idleness
[0m00:29:47.487472 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_saleslt, idle-time=0.00400090217590332s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Retrieving connection
[0m00:29:47.488473 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:29:47.488473 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:29:47.489474 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=Unknown) - Created cursor
[0m00:29:47.583478 [debug] [ThreadPool]: SQL status: OK in 0.09000000357627869 seconds
[0m00:29:47.588478 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=fb962d5b-bfa1-4f0e-bf3f-5cd71814bad0) - Closing cursor
[0m00:29:47.593479 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_saleslt, idle-time=0.1090078353881836s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Checking idleness
[0m00:29:47.593479 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_saleslt, idle-time=0.11000776290893555s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Retrieving connection
[0m00:29:47.594479 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:29:47.595479 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:29:47.596480 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=Unknown) - Created cursor
[0m00:29:47.717001 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m00:29:47.720003 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=13b39d95-cd67-4b39-8867-13ad79847563) - Closing cursor
[0m00:29:47.724003 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_saleslt, idle-time=0.24053215980529785s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Checking idleness
[0m00:29:47.725004 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_saleslt, idle-time=0.24053215980529785s, acquire-count=1, language=None, thread-identifier=(15612, 5872), compute-name=) - Retrieving connection
[0m00:29:47.725004 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:29:47.726004 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:29:47.727004 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=Unknown) - Created cursor
[0m00:29:47.871036 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:29:47.874037 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=91742751-3109-4140-870d-8e548bf39ecc, command-id=3e859a68-cb15-4ea1-92c0-e7649e69c2e3) - Closing cursor
[0m00:29:47.875038 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2399960642720, session-id=91742751-3109-4140-870d-8e548bf39ecc, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15612, 5872), compute-name=) - Released connection
[0m00:29:47.878038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EC8DAD820>]}
[0m00:29:47.878038 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=1.2421605587005615s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Checking idleness
[0m00:29:47.879038 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=1.2431614398956299s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Retrieving connection
[0m00:29:47.879038 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=1.2431614398956299s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Checking idleness
[0m00:29:47.880038 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=1.244161605834961s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Retrieving connection
[0m00:29:47.880038 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:29:47.881039 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:29:47.881039 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15612, 9324), compute-name=) - Released connection
[0m00:29:47.882039 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:29:47.883040 [info ] [MainThread]: 
[0m00:29:47.886039 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m00:29:47.887040 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m00:29:47.888040 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15612, 1456), compute-name=) - Creating connection
[0m00:29:47.889041 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m00:29:47.890041 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Acquired connection on thread (15612, 1456), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m00:29:47.890041 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m00:29:47.900043 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m00:29:47.938054 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04901289939880371s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:47.939052 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.05001091957092285s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:47.940052 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.05101156234741211s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:47.940052 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.05101156234741211s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:47.941052 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:29:47.941052 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:47.942053 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:29:47.942053 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:29:48.101547 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Connection created
[0m00:29:48.102548 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:48.415576 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m00:29:48.417576 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=f3340cfc-ddae-4267-894b-37a03c1a836a) - Closing cursor
[0m00:29:48.454585 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.3520374298095703s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:48.454585 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.35303735733032227s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:48.455585 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:48.455585 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
select * from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:29:48.456586 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:48.601606 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:29:48.604606 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=c60bd36e-a563-4b99-bcb8-4887b71f69f6) - Closing cursor
[0m00:29:48.609607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5080595016479492s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:48.609607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5080595016479492s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:48.610607 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:48.610607 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:29:48.611607 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:48.769643 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:29:48.772644 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=e4f2752f-847a-4722-a6e2-05d550145a39) - Closing cursor
[0m00:29:48.778645 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.6760971546173096s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:48.778645 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.6770973205566406s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:48.779645 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:48.779645 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:29:48.780645 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:48.940649 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:29:48.944648 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=ed00d7f0-d8a1-45e7-b985-b94888a9a0f7) - Closing cursor
[0m00:29:48.971654 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.8701066970825195s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:48.972654 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.8711066246032715s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:48.972654 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:48.973654 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`address_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:29:48.974654 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:49.761740 [debug] [Thread-1 (]: SQL status: OK in 0.7900000214576721 seconds
[0m00:29:49.763740 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=bc9d33c1-ab58-4f8a-a9d7-57f47d07eda0) - Closing cursor
[0m00:29:49.768742 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=1.6671946048736572s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:49.769742 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=1.6681945323944092s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:49.770742 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:49.771742 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:29:49.772743 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:49.911773 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:29:49.914775 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=3d1363b6-d6ff-462d-aed4-48f687d9f9f4) - Closing cursor
[0m00:29:49.916775 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=1.8152282238006592s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:49.917776 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=1.8162283897399902s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:49.918775 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:49.918775 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:29:49.919775 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:50.095826 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:29:50.098827 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=594c0316-96c8-467b-be06-9a2ef2abb891) - Closing cursor
[0m00:29:50.102826 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=2.00127911567688s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:50.102826 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=2.00127911567688s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:50.103827 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:50.103827 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:29:50.104827 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:50.249807 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:29:50.253808 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=d8f92e37-3e44-49c7-8837-465d41e05761) - Closing cursor
[0m00:29:50.257809 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=2.156261444091797s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:50.257809 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=2.156261444091797s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:50.258809 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:50.258809 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:29:50.259809 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:50.412816 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:29:50.416818 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=14f31b5f-e97c-4e18-8f13-88f1e9fbb521) - Closing cursor
[0m00:29:50.429820 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=2.328273296356201s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:50.431820 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=2.330272674560547s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:50.432820 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:50.432820 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:29:50.433821 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:50.571648 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:29:50.574649 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=875a4ee4-5d54-4c89-99e9-652d68f45334) - Closing cursor
[0m00:29:50.582651 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.address_snapshot"
[0m00:29:50.584651 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=2.4821035861968994s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:50.585651 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=2.4831039905548096s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:50.585651 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:50.586653 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`address_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:29:50.587652 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:53.820227 [debug] [Thread-1 (]: SQL status: OK in 3.2300000190734863 seconds
[0m00:29:53.822228 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=513e07e8-2d49-43c4-a6ad-a21cd3c45f05) - Closing cursor
[0m00:29:53.830229 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:29:53.835230 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=5.73368239402771s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:53.836230 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=5.73468279838562s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:53.837230 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:29:53.837230 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:29:53.838231 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:54.338294 [debug] [Thread-1 (]: SQL status: OK in 0.5 seconds
[0m00:29:54.339294 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=c9f3699e-7904-418b-951d-2590e4248e2f) - Closing cursor
[0m00:29:54.364300 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:29:54.366302 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:29:54.366302 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:29:54.368301 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EAD75FFB0>]}
[0m00:29:54.369302 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 6.48s]
[0m00:29:54.370301 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m00:29:54.371302 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m00:29:54.372301 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m00:29:54.373303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.address_snapshot, idle-time=0.007000923156738281s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:54.374303 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m00:29:54.375303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.009001016616821289s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m00:29:54.376303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.010001182556152344s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Acquired connection on thread (15612, 1456), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m00:29:54.377304 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m00:29:54.382305 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m00:29:54.387306 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.020003318786621094s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:54.387306 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.021004199981689453s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:54.388306 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:29:54.388306 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m00:29:54.389306 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:54.671346 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m00:29:54.675348 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=8220cbb7-4122-42d8-8c0f-e4a3740681b4) - Closing cursor
[0m00:29:54.679348 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.3130459785461426s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:54.679348 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.3130459785461426s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:54.680348 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:29:54.680348 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:29:54.681349 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:54.834363 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Closing cursor
[0m00:29:54.835364 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=b35920cd-cab0-48f6-bf37-3a38b1cb4db2
[0m00:29:54.836364 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:29:54.845366 [debug] [Thread-1 (]: Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
[0m00:29:54.845366 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:29:54.846367 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EC91C2750>]}
[0m00:29:54.847366 [error] [Thread-1 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 0.47s]
[0m00:29:54.849367 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m00:29:54.850367 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:29:54.851369 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m00:29:54.853368 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.008001327514648438s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:54.854368 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m00:29:54.856369 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.010003089904785156s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m00:29:54.857369 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.01200246810913086s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Acquired connection on thread (15612, 1456), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m00:29:54.858369 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m00:29:54.865370 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m00:29:54.875373 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0290067195892334s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:54.875373 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.030006885528564453s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:54.876373 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:54.876373 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:29:54.877373 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:55.090385 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:29:55.094386 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=ff8ec98a-3da2-49c9-bfec-cc0b445dac1d) - Closing cursor
[0m00:29:55.097387 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.25202083587646484s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:55.098387 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.2530205249786377s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:55.098387 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:55.099387 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:29:55.099387 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:55.246447 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:29:55.249446 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=f8898193-8678-4c73-aeba-d1f65e549fe0) - Closing cursor
[0m00:29:55.252447 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.40708041191101074s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:55.252447 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.40708041191101074s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:55.253447 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:55.253447 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:29:55.254447 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:55.429449 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:29:55.432449 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=825408ea-3ce0-4bed-b2ee-38c8bd07d0cb) - Closing cursor
[0m00:29:55.435450 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.589083194732666s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:55.435450 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.5900835990905762s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:55.436450 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:55.436450 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:29:55.437450 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:55.591440 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:29:55.594441 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=b2df46df-6a8a-44d3-a270-0d1aa0586c70) - Closing cursor
[0m00:29:55.599442 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.753075122833252s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:55.599442 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.7540760040283203s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:55.600442 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:55.601443 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId||'-'||AddressId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customeraddress_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:29:55.603443 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:55.958486 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m00:29:55.959486 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=fe6e47b3-25a2-453e-9616-2455763dac36) - Closing cursor
[0m00:29:55.962487 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.1161208152770996s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:55.962487 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.1171205043792725s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:55.963489 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:55.963489 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:29:55.964489 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:56.107523 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:29:56.110524 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=65205c47-d0c7-4f44-92a5-0bfd1e430794) - Closing cursor
[0m00:29:56.113525 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.2681584358215332s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:56.114524 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.269157886505127s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:56.114524 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:56.115525 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:29:56.115525 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:56.269520 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:29:56.272521 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=7e1f1f4f-3522-4a2f-8a39-6f0db90e1f12) - Closing cursor
[0m00:29:56.274521 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.429154396057129s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:56.275521 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.430154800415039s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:56.275521 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:56.276522 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:29:56.276522 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:56.438545 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:29:56.441547 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=da859234-3a2a-4eb9-9987-195b643bf368) - Closing cursor
[0m00:29:56.445546 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.599179744720459s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:56.446547 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.6001801490783691s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:56.446547 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:56.447547 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:29:56.448547 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:56.609562 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:29:56.613563 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=6ae432b9-d0e2-4154-b9e1-fd1c9daee65f) - Closing cursor
[0m00:29:56.616563 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.7711968421936035s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:56.616563 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.7711968421936035s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:56.617564 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:56.617564 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:29:56.618564 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:29:56.767575 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:29:56.772576 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=3228acdd-e7c2-4a7c-860b-b48e275535bc) - Closing cursor
[0m00:29:56.774577 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:56.777578 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.9312107563018799s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:29:56.778578 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.93221116065979s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:29:56.778578 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:29:56.779579 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customeraddress_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:29:56.780578 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:00.159000 [debug] [Thread-1 (]: SQL status: OK in 3.380000114440918 seconds
[0m00:30:00.161000 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=e3ba78c4-a33e-4df6-98ec-227f8b30faa9) - Closing cursor
[0m00:30:00.163001 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:30:00.165001 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=5.318634510040283s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:00.165001 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=5.3196351528167725s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:00.166002 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:30:00.166002 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:30:00.167001 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:00.594046 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m00:30:00.595046 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=90033378-75f8-47db-9bc4-40d2d4d8c43f) - Closing cursor
[0m00:30:00.597047 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:30:00.598047 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:30:00.599047 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:30:00.599047 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EC9258290>]}
[0m00:30:00.600048 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 5.75s]
[0m00:30:00.602049 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:30:00.603048 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m00:30:00.604049 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m00:30:00.606049 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.007002353668212891s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:00.606049 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m00:30:00.607049 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.product_snapshot, idle-time=0.008002281188964844s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m00:30:00.607049 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.product_snapshot, idle-time=0.008002281188964844s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Acquired connection on thread (15612, 1456), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m00:30:00.608049 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m00:30:00.612052 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m00:30:00.618052 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.product_snapshot, idle-time=0.018004894256591797s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:00.619055 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.product_snapshot, idle-time=0.01900458335876465s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:00.619055 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:30:00.620054 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m00:30:00.621052 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:00.833284 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:30:00.836285 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=4a205182-c9cd-4e2a-926b-7965c0941afa) - Closing cursor
[0m00:30:00.839285 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.product_snapshot, idle-time=0.23923754692077637s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:00.839285 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.product_snapshot, idle-time=0.24023771286010742s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:00.840286 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:30:00.840286 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:30:00.841286 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:00.955333 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Closing cursor
[0m00:30:00.957333 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=c4ee431e-aa82-4d85-ac82-13ab615f7e41
[0m00:30:00.959334 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:30:00.966336 [debug] [Thread-1 (]: Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
[0m00:30:00.966336 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:30:00.967336 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EC93B9040>]}
[0m00:30:00.968336 [error] [Thread-1 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.36s]
[0m00:30:00.969336 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m00:30:00.970336 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m00:30:00.971336 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m00:30:00.972336 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.product_snapshot, idle-time=0.006000518798828125s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:00.973337 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m00:30:00.974337 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.00700068473815918s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m00:30:00.974337 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.008000850677490234s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Acquired connection on thread (15612, 1456), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m00:30:00.975337 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m00:30:00.980338 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m00:30:00.985339 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.018002748489379883s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:00.985339 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.019002914428710938s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:00.986340 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:00.986340 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:30:00.987340 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:01.195408 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:30:01.198409 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=dcd8266b-cb1e-455b-a8e1-1e1493fd9615) - Closing cursor
[0m00:30:01.200409 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.23407268524169922s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:01.201408 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.23507237434387207s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:01.201408 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:01.202410 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:30:01.203410 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:01.340440 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:30:01.343442 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=531155e7-442d-4e1a-a936-a911c843e467) - Closing cursor
[0m00:30:01.346442 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.38010549545288086s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:01.347442 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.38010549545288086s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:01.347442 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:01.348442 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:30:01.348442 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:01.497148 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:30:01.500149 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=95b1403c-a476-4578-a400-985085f06dbd) - Closing cursor
[0m00:30:01.503150 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.536813497543335s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:01.504151 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.5378153324127197s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:01.504151 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:01.505150 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:30:01.506151 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:01.661159 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:30:01.664161 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=ea35df5c-64a7-4fe9-a9b8-449b6791ce76) - Closing cursor
[0m00:30:01.667161 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7008252143859863s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:01.668162 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7018258571624756s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:01.668162 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:01.669162 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductModelID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productmodel_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:30:01.670162 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:02.032237 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m00:30:02.034238 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=f032816a-ea97-4932-9cb8-813ef35a802d) - Closing cursor
[0m00:30:02.036237 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.0699009895324707s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:02.037238 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.070901870727539s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:02.038239 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:02.038239 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:30:02.039238 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:02.181014 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:30:02.185015 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=7a31f1c0-7a2a-40a5-bd44-a7eefecbae70) - Closing cursor
[0m00:30:02.190017 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.2226803302764893s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:02.191017 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.2236807346343994s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:02.192018 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:02.193018 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:30:02.193018 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:02.354244 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:30:02.358246 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=64dcf61e-96f3-40a8-9769-88138a2a7b9e) - Closing cursor
[0m00:30:02.360245 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.393909215927124s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:02.361246 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.3949105739593506s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:02.362247 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:02.362247 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:30:02.363247 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:02.509266 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:30:02.512267 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=dc510ed5-512d-4587-8576-79464733d70b) - Closing cursor
[0m00:30:02.514267 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.547931432723999s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:02.515268 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.54893159866333s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:02.515268 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:02.516268 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:30:02.516268 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:02.676095 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:30:02.678094 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=607e0c44-421a-4b1d-a3fd-fad1d82800a4) - Closing cursor
[0m00:30:02.681096 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.7147595882415771s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:02.682095 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.7157588005065918s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:02.682095 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:02.683096 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:30:02.684096 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:02.822128 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:30:02.826129 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=da81b829-a06d-4d14-bd82-cd07e654de41) - Closing cursor
[0m00:30:02.828129 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:02.829129 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.862793207168579s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:02.830130 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.8637938499450684s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:02.831130 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:02.831130 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productmodel_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:30:02.832130 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:06.281809 [debug] [Thread-1 (]: SQL status: OK in 3.450000047683716 seconds
[0m00:30:06.282810 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=95aeab44-9b4d-4490-b0e1-e4372b6f2347) - Closing cursor
[0m00:30:06.286810 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:30:06.287811 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.321475028991699s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:06.287811 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.321475028991699s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:06.288811 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:30:06.288811 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:30:06.289811 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:06.739994 [debug] [Thread-1 (]: SQL status: OK in 0.44999998807907104 seconds
[0m00:30:06.741996 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=bae6b494-5e62-48e4-8632-54c0201356ca) - Closing cursor
[0m00:30:06.742996 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:30:06.743996 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:30:06.744997 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:30:06.745996 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EC91C95E0>]}
[0m00:30:06.746997 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 5.77s]
[0m00:30:06.747997 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m00:30:06.747997 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:30:06.748997 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m00:30:06.749996 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.004999876022338867s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:06.749996 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m00:30:06.750997 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0060002803802490234s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m00:30:06.751997 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0060002803802490234s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Acquired connection on thread (15612, 1456), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m00:30:06.751997 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:30:06.755998 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:30:06.761000 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.016003131866455078s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:06.762000 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.016003131866455078s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:06.762000 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:06.763000 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:30:06.763000 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:07.049013 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:30:07.052014 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=7f1aa8f9-6131-4bf6-b347-b0f3f82453ee) - Closing cursor
[0m00:30:07.055015 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.3090176582336426s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:07.055015 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.31001782417297363s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:07.056015 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:07.057016 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
select * from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:30:07.057016 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:07.204042 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:30:07.210044 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=2666f731-9ca1-45c3-9b7a-59d8ea054163) - Closing cursor
[0m00:30:07.213044 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.4680471420288086s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:07.213044 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.4680471420288086s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:07.214045 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:07.214045 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:30:07.215045 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:07.366039 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:30:07.369040 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=63f5a03c-3477-4e07-8c46-4ce20e3429fb) - Closing cursor
[0m00:30:07.373041 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.6280443668365479s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:07.374041 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.6280443668365479s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:07.374041 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:07.375041 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:30:07.375041 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:07.531061 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:30:07.534063 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=94ac3503-c43a-424d-87a1-3e2b26f18b6b) - Closing cursor
[0m00:30:07.539063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7940661907196045s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:07.540064 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7950670719146729s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:07.540064 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:07.541064 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderDetailID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:30:07.542064 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:07.954073 [debug] [Thread-1 (]: SQL status: OK in 0.4099999964237213 seconds
[0m00:30:07.955073 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=d7410813-2bdf-4cc4-9808-0eaa4b7daf25) - Closing cursor
[0m00:30:07.957073 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.2120769023895264s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:07.958074 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.2130773067474365s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:07.958074 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:07.959074 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:30:07.959074 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:08.103080 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:30:08.106080 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=38596094-2de1-4b81-a546-2a9ca34c33f3) - Closing cursor
[0m00:30:08.109081 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.364084243774414s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:08.110082 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.364084243774414s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:08.110082 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:08.111082 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:30:08.111082 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:08.282099 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:30:08.285100 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=a6aa1293-53b8-4142-91f3-c659627bf894) - Closing cursor
[0m00:30:08.287100 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.5421035289764404s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:08.288101 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.5431044101715088s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:08.289101 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:08.289101 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:30:08.290101 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:08.444163 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:30:08.448164 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=97c50e9b-9ce1-447d-a147-f5878e0c328d) - Closing cursor
[0m00:30:08.453166 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.708169937133789s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:08.454166 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7091693878173828s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:08.455166 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:08.456167 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:30:08.457167 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:08.617994 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:30:08.620995 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=7ddb2a7d-479c-44ad-9507-a0a8ef6f82bc) - Closing cursor
[0m00:30:08.623995 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.8779985904693604s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:08.623995 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.8789982795715332s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:08.624996 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:08.624996 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:30:08.625995 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:08.761043 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:30:08.765043 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=1774eb5a-876e-47a4-ba96-d4538a4034af) - Closing cursor
[0m00:30:08.768044 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:08.769043 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=2.0240464210510254s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:08.770045 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=2.0250487327575684s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:08.771045 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:08.772044 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:30:08.773044 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:12.406368 [debug] [Thread-1 (]: SQL status: OK in 3.630000114440918 seconds
[0m00:30:12.407368 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=9eae47e8-df96-41da-908e-3e0a1aed07ed) - Closing cursor
[0m00:30:12.410369 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:30:12.411369 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.665372371673584s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:12.411369 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.666372537612915s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:12.412370 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:30:12.412370 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:30:12.413369 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:12.876462 [debug] [Thread-1 (]: SQL status: OK in 0.46000000834465027 seconds
[0m00:30:12.877463 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=d9776c2a-24da-4584-ad0f-1c9a77e8e19a) - Closing cursor
[0m00:30:12.879464 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:30:12.881464 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:30:12.881464 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:30:12.882464 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EC9391D60>]}
[0m00:30:12.883465 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 6.13s]
[0m00:30:12.884464 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:30:12.885464 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:30:12.885464 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m00:30:12.886465 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0050008296966552734s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:12.887465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m00:30:12.887465 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:30:12.888465 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.007001399993896484s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Acquired connection on thread (15612, 1456), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m00:30:12.888465 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:30:12.892466 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:30:12.898468 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.016003131866455078s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:12.898468 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.017003774642944336s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:12.899468 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:12.899468 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:30:12.900467 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:13.285556 [debug] [Thread-1 (]: SQL status: OK in 0.3799999952316284 seconds
[0m00:30:13.288555 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=538fb906-7586-4657-9315-12212ac4b304) - Closing cursor
[0m00:30:13.291556 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.4100921154022217s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:13.292556 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.41109228134155273s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:13.292556 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:13.293557 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
select * from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:30:13.293557 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:13.509608 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m00:30:13.514609 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=0dc6a4b1-0373-45ad-9f31-cf9ec530d4c2) - Closing cursor
[0m00:30:13.518610 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.6361448764801025s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:13.518610 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.63714599609375s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:13.519610 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:13.519610 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:30:13.520610 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:13.718608 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:30:13.721609 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=c1c3f1be-c568-487a-96bc-54b0c4ea866f) - Closing cursor
[0m00:30:13.726610 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.8441472053527832s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:13.726610 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.8451464176177979s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:13.727611 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:13.728611 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:30:13.728611 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:13.927672 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:30:13.930672 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=8b5ce6a7-e587-4cde-8200-f43a7cf3fb52) - Closing cursor
[0m00:30:13.935673 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.053208589553833s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:13.935673 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.054208755493164s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:13.936673 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:13.938673 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:30:13.939673 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:14.412498 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m00:30:14.414498 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=b19010f1-469e-4e90-bb57-ef2557683c85) - Closing cursor
[0m00:30:14.416499 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.5350351333618164s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:14.417499 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.5350351333618164s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:14.417499 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:14.418499 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:30:14.418499 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:14.560518 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:30:14.564517 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=cda748ee-1057-49af-9dfd-3b0b7b06cc1f) - Closing cursor
[0m00:30:14.567518 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.6860542297363281s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:14.568518 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.6870543956756592s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:14.569518 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:14.569518 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:30:14.570519 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:14.777901 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:30:14.780901 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=d28a56e5-2c30-4eb7-9f4c-af4aaf2fb3de) - Closing cursor
[0m00:30:14.784902 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.9034385681152344s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:14.785903 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.9044389724731445s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:14.785903 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:14.786903 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:30:14.787903 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:14.943958 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:30:14.946959 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=417634e7-2360-49af-a4b8-85e6f31341bb) - Closing cursor
[0m00:30:14.948960 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.0674960613250732s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:14.949960 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.068495750427246s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:14.949960 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:14.950960 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:30:14.951960 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:15.115018 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:30:15.118018 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=ca64b458-6f6e-495a-bc03-4ce18d2fe1c3) - Closing cursor
[0m00:30:15.121018 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.2395544052124023s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:15.122018 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.2405545711517334s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:15.122018 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:15.123019 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:30:15.124019 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:15.262084 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:30:15.265086 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=918a3422-1076-4bf3-821b-c1fb22799cec) - Closing cursor
[0m00:30:15.267086 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:15.269086 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.387622117996216s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:15.269086 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.387622117996216s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:15.270087 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:15.270087 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderheader_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:30:15.271087 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:19.462428 [debug] [Thread-1 (]: SQL status: OK in 4.190000057220459 seconds
[0m00:30:19.464429 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=0dbd693a-7f5a-4679-a352-5d902925ec4d) - Closing cursor
[0m00:30:19.467430 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:30:19.468431 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=6.5869667530059814s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Checking idleness
[0m00:30:19.469431 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=6.587967157363892s, acquire-count=1, language=sql, thread-identifier=(15612, 1456), compute-name=) - Retrieving connection
[0m00:30:19.470431 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:30:19.470431 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:30:19.471431 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=Unknown) - Created cursor
[0m00:30:19.979376 [debug] [Thread-1 (]: SQL status: OK in 0.5099999904632568 seconds
[0m00:30:19.981376 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, command-id=67b3d8f7-097b-4e23-ae04-469355c44a8d) - Closing cursor
[0m00:30:19.983376 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:30:19.985378 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:30:19.986377 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2399965898144, session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15612, 1456), compute-name=) - Released connection
[0m00:30:19.987378 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310f29b7-aabe-44e2-a3a5-2fe903d1088a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EC91D0A40>]}
[0m00:30:19.988378 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 7.10s]
[0m00:30:19.990379 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:30:19.992380 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=32.11134099960327s, acquire-count=0, language=None, thread-identifier=(15612, 9324), compute-name=) - Checking idleness
[0m00:30:19.993379 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=32.112340688705444s, acquire-count=0, language=None, thread-identifier=(15612, 9324), compute-name=) - Reusing connection previously named master
[0m00:30:19.994379 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=32.11334037780762s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Acquired connection on thread (15612, 9324), using default compute resource
[0m00:30:19.995379 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=32.11434030532837s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Checking idleness
[0m00:30:19.996380 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=None, name=master, idle-time=32.11534118652344s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Retrieving connection
[0m00:30:19.997380 [debug] [MainThread]: On master: ROLLBACK
[0m00:30:19.998380 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:30:20.170373 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=aab3364e-8fd3-481a-a963-0151fff196d1, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Connection created
[0m00:30:20.170373 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:30:20.171373 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=aab3364e-8fd3-481a-a963-0151fff196d1, name=master, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Checking idleness
[0m00:30:20.172373 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=aab3364e-8fd3-481a-a963-0151fff196d1, name=master, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(15612, 9324), compute-name=) - Retrieving connection
[0m00:30:20.172373 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:30:20.173374 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:30:20.173374 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2399959379664, session-id=aab3364e-8fd3-481a-a963-0151fff196d1, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15612, 9324), compute-name=) - Released connection
[0m00:30:20.174374 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:30:20.174374 [debug] [MainThread]: On master: ROLLBACK
[0m00:30:20.175375 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:30:20.175375 [debug] [MainThread]: On master: Close
[0m00:30:20.176374 [debug] [MainThread]: Databricks adapter: Connection(session-id=aab3364e-8fd3-481a-a963-0151fff196d1) - Closing connection
[0m00:30:20.249395 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:30:20.249395 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:30:20.250397 [debug] [MainThread]: Databricks adapter: Connection(session-id=0fb20e05-996b-48ac-a1fe-a5b01b2580d4) - Closing connection
[0m00:30:20.345395 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:30:20.346396 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:30:20.346396 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:30:20.347396 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:30:20.347396 [debug] [MainThread]: Databricks adapter: Connection(session-id=91742751-3109-4140-870d-8e548bf39ecc) - Closing connection
[0m00:30:20.413418 [debug] [MainThread]: Connection 'snapshot.medallion_spark.salesorderheader_snapshot' was properly closed.
[0m00:30:20.413418 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: ROLLBACK
[0m00:30:20.414419 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:30:20.414419 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: Close
[0m00:30:20.415419 [debug] [MainThread]: Databricks adapter: Connection(session-id=f4580a16-76d9-455c-9047-87ad7c8f8dd8) - Closing connection
[0m00:30:20.488458 [info ] [MainThread]: 
[0m00:30:20.489455 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 33.85 seconds (33.85s).
[0m00:30:20.491459 [debug] [MainThread]: Command end result
[0m00:30:20.538468 [info ] [MainThread]: 
[0m00:30:20.539468 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m00:30:20.540467 [info ] [MainThread]: 
[0m00:30:20.541468 [error] [MainThread]:   Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
[0m00:30:20.542468 [info ] [MainThread]: 
[0m00:30:20.542468 [error] [MainThread]:   Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
[0m00:30:20.544469 [info ] [MainThread]: 
[0m00:30:20.545470 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=0 TOTAL=7
[0m00:30:20.546470 [debug] [MainThread]: Command `dbt snapshot` failed at 00:30:20.546470 after 36.33 seconds
[0m00:30:20.547470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EADF506B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EADF1B260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EADDF5BB0>]}
[0m00:30:20.547470 [debug] [MainThread]: Flushing usage events
[0m00:34:26.551123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273DF17F2F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273DCD5E2A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273DF70AF00>]}


============================== 00:34:26.556123 | a1684a06-74be-4310-ad2c-cb02a4de0ca8 ==============================
[0m00:34:26.556123 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:34:26.557124 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt snapshot', 'send_anonymous_usage_stats': 'True'}
[0m00:34:26.744166 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:34:26.745166 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:34:26.745166 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:34:28.308519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FA1A4AD0>]}
[0m00:34:28.367533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273DF76A330>]}
[0m00:34:28.369534 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:34:28.382536 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:34:28.615589 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m00:34:28.616589 [debug] [MainThread]: Partial parsing: updated file: medallion_spark://snapshots\customer.sql
[0m00:34:28.617589 [debug] [MainThread]: Partial parsing: updated file: medallion_spark://snapshots\product.sql
[0m00:34:29.010678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FA8713A0>]}
[0m00:34:29.149710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FA8ADE80>]}
[0m00:34:29.150709 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:34:29.151711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FA882F30>]}
[0m00:34:29.153710 [info ] [MainThread]: 
[0m00:34:29.154710 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13520, 17828), compute-name=) - Creating connection
[0m00:34:29.155710 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:34:29.155710 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Acquired connection on thread (13520, 17828), using default compute resource
[0m00:34:29.162712 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697143837776, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13520, 16096), compute-name=) - Creating connection
[0m00:34:29.162712 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:34:29.163712 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697143837776, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13520, 16096), compute-name=) - Acquired connection on thread (13520, 16096), using default compute resource
[0m00:34:29.163712 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697143837776, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13520, 16096), compute-name=) - Checking idleness
[0m00:34:29.164712 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697143837776, session-id=None, name=list_hive_metastore, idle-time=0.0009999275207519531s, acquire-count=1, language=None, thread-identifier=(13520, 16096), compute-name=) - Retrieving connection
[0m00:34:29.164712 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:34:29.165713 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:34:29.165713 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:34:29.479189 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697143837776, session-id=86e9f837-01c9-4ef1-a374-e6291ab80749, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13520, 16096), compute-name=) - Connection created
[0m00:34:29.480190 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=86e9f837-01c9-4ef1-a374-e6291ab80749, command-id=Unknown) - Created cursor
[0m00:34:29.659215 [debug] [ThreadPool]: SQL status: OK in 0.49000000953674316 seconds
[0m00:34:29.664216 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=86e9f837-01c9-4ef1-a374-e6291ab80749, command-id=c0d1fa1a-2842-4449-bd24-2e782e3170aa) - Closing cursor
[0m00:34:29.665215 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697143837776, session-id=86e9f837-01c9-4ef1-a374-e6291ab80749, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(13520, 16096), compute-name=) - Released connection
[0m00:34:29.666217 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13520, 15428), compute-name=) - Creating connection
[0m00:34:29.667217 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:34:29.668217 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Acquired connection on thread (13520, 15428), using default compute resource
[0m00:34:29.668217 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0009999275207519531s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Checking idleness
[0m00:34:29.669216 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0009999275207519531s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Retrieving connection
[0m00:34:29.669216 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:34:29.669216 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:34:29.670217 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:34:29.836237 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Connection created
[0m00:34:29.837237 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=Unknown) - Created cursor
[0m00:34:30.014247 [debug] [ThreadPool]: SQL status: OK in 0.3400000035762787 seconds
[0m00:34:30.018248 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=a5c28175-7f1c-4084-99ba-1a5756d78965) - Closing cursor
[0m00:34:30.029250 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_saleslt, idle-time=0.19301342964172363s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Checking idleness
[0m00:34:30.030250 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_saleslt, idle-time=0.1940135955810547s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Retrieving connection
[0m00:34:30.030250 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_saleslt, idle-time=0.1940135955810547s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Checking idleness
[0m00:34:30.031251 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_saleslt, idle-time=0.19501423835754395s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Retrieving connection
[0m00:34:30.031251 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:34:30.032251 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:34:30.032251 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:34:30.033250 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=Unknown) - Created cursor
[0m00:34:30.266288 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m00:34:30.269287 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=a02c47a9-eebf-4bea-a993-3e7a223d1de8) - Closing cursor
[0m00:34:30.275289 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_saleslt, idle-time=0.4390528202056885s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Checking idleness
[0m00:34:30.276290 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_saleslt, idle-time=0.44005298614501953s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Retrieving connection
[0m00:34:30.277290 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:34:30.277290 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:34:30.278289 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=Unknown) - Created cursor
[0m00:34:30.429323 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:30.432324 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=42e05891-c2dd-4ff4-a147-9c6e747e86d2) - Closing cursor
[0m00:34:30.432324 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(13520, 15428), compute-name=) - Released connection
[0m00:34:30.433324 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_saleslt, idle-time=0.0009999275207519531s, acquire-count=0, language=None, thread-identifier=(13520, 15428), compute-name=) - Checking idleness
[0m00:34:30.436325 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:34:30.437324 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_snapshots, idle-time=0.00500035285949707s, acquire-count=0, language=None, thread-identifier=(13520, 15428), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m00:34:30.437324 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_snapshots, idle-time=0.00500035285949707s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Acquired connection on thread (13520, 15428), using default compute resource
[0m00:34:30.438325 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_snapshots, idle-time=0.0060007572174072266s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Checking idleness
[0m00:34:30.438325 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_snapshots, idle-time=0.0060007572174072266s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Retrieving connection
[0m00:34:30.439326 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:34:30.439326 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:34:30.440326 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=Unknown) - Created cursor
[0m00:34:30.552315 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:34:30.557317 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=17797b16-3398-4e4d-8e2a-cd89af35b5e1) - Closing cursor
[0m00:34:30.560316 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_snapshots, idle-time=0.1279921531677246s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Checking idleness
[0m00:34:30.560316 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_snapshots, idle-time=0.1279921531677246s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Retrieving connection
[0m00:34:30.561317 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:34:30.561317 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:34:30.562319 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=Unknown) - Created cursor
[0m00:34:30.663329 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m00:34:30.665329 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=889fc0b3-2a1b-4bba-9787-ab8d6edf42c5) - Closing cursor
[0m00:34:30.667329 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_snapshots, idle-time=0.23500490188598633s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Checking idleness
[0m00:34:30.668329 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_snapshots, idle-time=0.23600482940673828s, acquire-count=1, language=None, thread-identifier=(13520, 15428), compute-name=) - Retrieving connection
[0m00:34:30.668329 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:34:30.669329 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:34:30.669329 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=Unknown) - Created cursor
[0m00:34:30.809339 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:34:30.812340 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, command-id=e670d29d-2ce7-410c-bdba-906c792d1fd4) - Closing cursor
[0m00:34:30.813341 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2697144138544, session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(13520, 15428), compute-name=) - Released connection
[0m00:34:30.817342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273DD52EDE0>]}
[0m00:34:30.818342 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=1.6626312732696533s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Checking idleness
[0m00:34:30.819342 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=1.6636314392089844s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Retrieving connection
[0m00:34:30.820343 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=1.6646323204040527s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Checking idleness
[0m00:34:30.821342 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=1.6646323204040527s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Retrieving connection
[0m00:34:30.821342 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:34:30.822342 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:34:30.823342 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(13520, 17828), compute-name=) - Released connection
[0m00:34:30.824343 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:34:30.825343 [info ] [MainThread]: 
[0m00:34:30.829344 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m00:34:30.830345 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m00:34:30.831344 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13520, 1092), compute-name=) - Creating connection
[0m00:34:30.831344 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m00:34:30.832345 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Acquired connection on thread (13520, 1092), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m00:34:30.833345 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m00:34:30.841346 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m00:34:30.875354 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.042008161544799805s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:30.875354 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04300951957702637s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:30.876355 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.044010162353515625s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:30.877354 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04500937461853027s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:30.877354 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:34:30.878355 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:30.879354 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:34:30.879354 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:34:31.066369 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Connection created
[0m00:34:31.067369 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:31.507421 [debug] [Thread-1 (]: SQL status: OK in 0.6299999952316284 seconds
[0m00:34:31.511424 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=c689d0f4-113a-4e91-ac75-a4cc4887cff4) - Closing cursor
[0m00:34:31.548430 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=0.48206186294555664s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:31.549431 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=0.4830622673034668s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:31.550432 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:31.550432 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
select * from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:34:31.551431 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:31.743457 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:34:31.746458 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=fee35c4d-d952-4b35-a1af-111a3b8d51c2) - Closing cursor
[0m00:34:31.751459 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=0.6840898990631104s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:31.751459 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=0.6850903034210205s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:31.752459 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:31.752459 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:34:31.753459 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:31.915475 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:34:31.918475 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=64f3af70-098a-4c5c-882d-80b5359dc9eb) - Closing cursor
[0m00:34:31.926477 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=0.8601081371307373s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:31.926477 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=0.8601081371307373s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:31.927476 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:31.927476 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:34:31.928477 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:32.075727 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:32.078727 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=55b9dad1-fd3c-4db8-8314-5c68dd0db3f3) - Closing cursor
[0m00:34:32.103732 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=1.037363052368164s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:32.104732 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=1.0383636951446533s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:32.104732 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:32.105733 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`address_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:34:32.106733 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:32.908622 [debug] [Thread-1 (]: SQL status: OK in 0.800000011920929 seconds
[0m00:34:32.909622 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=ecb5b734-091b-423d-b721-6f4290ac6c47) - Closing cursor
[0m00:34:32.912623 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=1.846254587173462s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:32.912623 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=1.846254587173462s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:32.913623 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:32.913623 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:34:32.914624 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:33.054656 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:34:33.057657 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=c66a58e6-3cfe-4fdc-b71b-aa71515f4074) - Closing cursor
[0m00:34:33.060657 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9942889213562012s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:33.060657 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9942889213562012s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:33.061657 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:33.061657 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:34:33.062658 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:33.214692 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:33.217692 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=f7889937-046f-415e-b64f-e69ac51cc824) - Closing cursor
[0m00:34:33.220693 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=2.154324531555176s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:33.221693 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=2.1553244590759277s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:33.221693 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:33.222693 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:34:33.222693 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:33.352722 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:34:33.355724 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=c1b3ad50-5d30-4dd5-a769-47fbc07494fe) - Closing cursor
[0m00:34:33.357724 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=2.2913553714752197s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:33.358724 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=2.2923552989959717s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:33.358724 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:33.359724 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:34:33.359724 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:33.510758 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:33.512759 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=80b17197-73d9-464d-a00d-82abcd30678c) - Closing cursor
[0m00:34:33.519760 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=2.4533917903900146s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:33.520761 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=2.454392433166504s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:33.521762 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:33.521762 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:34:33.522762 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:33.656791 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:34:33.658793 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=5d3830f4-8414-4118-9f3a-39001b194e72) - Closing cursor
[0m00:34:33.667794 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.address_snapshot"
[0m00:34:33.668795 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=2.602426290512085s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:33.669794 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=2.6034257411956787s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:33.669794 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:33.670794 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`address_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:34:33.671796 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:37.215135 [debug] [Thread-1 (]: SQL status: OK in 3.5399999618530273 seconds
[0m00:34:37.216135 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=01066850-bc5a-4b49-82ba-9c6b9e58a2c9) - Closing cursor
[0m00:34:37.226137 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:34:37.234139 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=6.1677703857421875s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:37.235139 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=6.168770790100098s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:37.235139 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:34:37.236140 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:34:37.236140 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:37.659811 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m00:34:37.661812 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=7a9354c7-a1b1-4dab-8a80-9804af8324fb) - Closing cursor
[0m00:34:37.683815 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:34:37.685816 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:37.686817 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:37.687817 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273DF1A8650>]}
[0m00:34:37.688817 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 6.86s]
[0m00:34:37.689817 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m00:34:37.689817 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m00:34:37.690818 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m00:34:37.692820 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.address_snapshot, idle-time=0.007004261016845703s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:37.692820 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m00:34:37.693819 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.008003473281860352s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m00:34:37.694819 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.008003473281860352s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Acquired connection on thread (13520, 1092), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m00:34:37.694819 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m00:34:37.698819 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m00:34:37.702820 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.017003774642944336s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:37.703820 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.01800394058227539s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:37.704820 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:34:37.704820 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m00:34:37.705820 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:37.904071 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:34:37.908073 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=03a81c80-43b6-467d-be2c-be3ed492a290) - Closing cursor
[0m00:34:37.910073 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.2242574691772461s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:37.911074 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.22525811195373535s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:37.911074 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:34:37.912074 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:34:37.913074 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:38.017151 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Closing cursor
[0m00:34:38.018151 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Title` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `PostalCode`, `AddressID`, `CustomerId`]. SQLSTATE: 42703; line 10 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Title` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `PostalCode`, `AddressID`, `CustomerId`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Title` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `PostalCode`, `AddressID`, `CustomerId`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=42948f09-ebbe-493e-84ce-8c569cea4912
[0m00:34:38.019153 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:38.027153 [debug] [Thread-1 (]: Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Title` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `PostalCode`, `AddressID`, `CustomerId`]. SQLSTATE: 42703; line 10 pos 8
[0m00:34:38.028154 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:38.028154 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FAA5BBC0>]}
[0m00:34:38.029154 [error] [Thread-1 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 0.34s]
[0m00:34:38.030155 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m00:34:38.031155 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:34:38.032155 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m00:34:38.033155 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.005000114440917969s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:38.034156 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m00:34:38.034156 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.00600123405456543s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m00:34:38.035156 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.007001638412475586s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Acquired connection on thread (13520, 1092), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m00:34:38.035156 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m00:34:38.039156 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m00:34:38.044157 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.016002893447875977s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:38.045157 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.01700305938720703s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:38.045157 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:38.046157 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:34:38.046157 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:38.268253 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m00:34:38.270254 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=832f307e-189c-4aa8-96ce-b48198851c3a) - Closing cursor
[0m00:34:38.272255 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.24410009384155273s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:38.273255 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.2451004981994629s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:38.273255 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:38.274255 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:34:38.275256 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:38.414262 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:34:38.416261 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=296c71e4-a87a-41bf-bd7b-1c6d1dee4f0d) - Closing cursor
[0m00:34:38.419262 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.39110755920410156s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:38.420262 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.39110755920410156s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:38.420262 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:38.421264 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:34:38.421264 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:38.571277 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:38.574277 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=84c6964b-a3e2-47cf-90d5-544c942c591c) - Closing cursor
[0m00:34:38.578278 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.5501234531402588s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:38.579279 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.5511245727539062s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:38.580279 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:38.580279 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:34:38.581280 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:38.729289 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:38.732289 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=f98b91fb-950e-4072-a599-7741ea3aebe6) - Closing cursor
[0m00:34:38.735290 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.7071354389190674s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:38.736290 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.7081356048583984s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:38.736290 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:38.737290 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId||'-'||AddressId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customeraddress_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:34:38.738290 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:39.096356 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m00:34:39.097356 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=798924f9-8e01-43c7-8c27-4744db3d11a9) - Closing cursor
[0m00:34:39.099357 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.071202278137207s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:39.100357 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.071202278137207s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:39.100357 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:39.101357 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:34:39.102358 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:39.260374 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:34:39.263375 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=b5c225c5-a6bc-4cd4-bd3a-5b35c54712d7) - Closing cursor
[0m00:34:39.266374 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.2382192611694336s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:39.267375 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.2382192611694336s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:39.267375 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:39.268374 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:34:39.268374 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:39.434371 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:34:39.437371 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=bc774834-25c0-404d-9334-681fd874c251) - Closing cursor
[0m00:34:39.439372 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.4112179279327393s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:39.440372 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.4122178554534912s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:39.440372 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:39.441373 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:34:39.441373 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:39.611403 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:34:39.614402 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=3cdfc3b8-a803-4150-8b5a-d641aff47f86) - Closing cursor
[0m00:34:39.616402 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.5882480144500732s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:39.617402 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.589247703552246s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:39.617402 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:39.618403 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:34:39.618403 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:39.774468 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:34:39.777469 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=88ddc54d-3a94-4c61-89bb-9be661a30b49) - Closing cursor
[0m00:34:39.780470 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.7513148784637451s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:39.780470 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.7523155212402344s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:39.781470 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:39.781470 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:34:39.782470 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:39.928495 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:39.932497 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=b8321dd0-abdb-463f-a933-9049acfcdc3e) - Closing cursor
[0m00:34:39.933497 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:39.935496 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.906341314315796s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:39.935496 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.9073419570922852s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:39.936497 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:39.936497 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customeraddress_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:34:39.937497 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:43.406864 [debug] [Thread-1 (]: SQL status: OK in 3.4700000286102295 seconds
[0m00:34:43.408865 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=ecd3a865-ef02-4c76-aad4-74457c658d73) - Closing cursor
[0m00:34:43.412865 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:34:43.413865 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=5.385711193084717s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:43.413865 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=5.385711193084717s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:43.414865 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:34:43.414865 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:34:43.415866 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:43.812411 [debug] [Thread-1 (]: SQL status: OK in 0.4000000059604645 seconds
[0m00:34:43.813411 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=ca0768e9-ed08-4abc-b9a9-eb7220c4124e) - Closing cursor
[0m00:34:43.815413 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:34:43.816412 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:43.816412 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:43.817413 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FA9B9D90>]}
[0m00:34:43.818413 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 5.78s]
[0m00:34:43.819413 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:34:43.820414 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m00:34:43.820414 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m00:34:43.821413 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0050008296966552734s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:43.822415 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m00:34:43.822415 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.product_snapshot, idle-time=0.006002664566040039s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m00:34:43.823414 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.product_snapshot, idle-time=0.007002115249633789s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Acquired connection on thread (13520, 1092), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m00:34:43.823414 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m00:34:43.827414 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m00:34:43.832415 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.product_snapshot, idle-time=0.016002893447875977s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:43.833416 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.product_snapshot, idle-time=0.017003536224365234s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:43.833416 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:34:43.834417 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m00:34:43.835417 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:44.116866 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m00:34:44.119867 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=558612fa-f010-428e-a948-cc2cbc704242) - Closing cursor
[0m00:34:44.121867 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.product_snapshot, idle-time=0.30545544624328613s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:44.122867 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.product_snapshot, idle-time=0.3064553737640381s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:44.122867 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:34:44.123868 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:34:44.123868 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:44.228927 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Closing cursor
[0m00:34:44.229929 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`size`, `model`, `weight`, `listPrice`, `product_name`]. SQLSTATE: 42703; line 9 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`size`, `model`, `weight`, `listPrice`, `product_name`]. SQLSTATE: 42703; line 9 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`size`, `model`, `weight`, `listPrice`, `product_name`]. SQLSTATE: 42703; line 9 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=1aed91fb-2904-416d-857f-6e25ca670749
[0m00:34:44.232929 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:44.237930 [debug] [Thread-1 (]: Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`size`, `model`, `weight`, `listPrice`, `product_name`]. SQLSTATE: 42703; line 9 pos 8
[0m00:34:44.237930 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:44.238932 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FAF07CB0>]}
[0m00:34:44.239932 [error] [Thread-1 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.42s]
[0m00:34:44.240933 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m00:34:44.241932 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m00:34:44.241932 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m00:34:44.243932 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0060024261474609375s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:44.243932 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m00:34:44.244932 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.007002115249633789s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m00:34:44.245933 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.007002115249633789s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Acquired connection on thread (13520, 1092), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m00:34:44.245933 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m00:34:44.249934 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m00:34:44.253936 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0160062313079834s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:44.253936 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0160062313079834s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:44.254936 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:44.254936 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:34:44.255937 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:44.480636 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m00:34:44.483637 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=bdafacde-e5cb-4507-be8d-c5cde25b2437) - Closing cursor
[0m00:34:44.487638 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.24870777130126953s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:44.487638 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.2497084140777588s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:44.488639 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:44.489638 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:34:44.490639 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:44.640010 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:44.643009 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=95b2e5d7-d068-4379-a25f-d34e15105c7d) - Closing cursor
[0m00:34:44.645011 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.40708088874816895s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:44.646010 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.40708088874816895s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:44.646010 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:44.647012 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:34:44.647012 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:44.808402 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:34:44.811403 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=e4857617-200a-4776-9184-45f2f5adaedd) - Closing cursor
[0m00:34:44.814403 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.5764729976654053s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:44.815403 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.5774734020233154s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:44.816404 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:44.816404 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:34:44.817404 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:44.991251 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:34:44.994251 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=f3c20b9f-8976-4deb-b74b-55e4ebe14bb8) - Closing cursor
[0m00:34:44.997251 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7593209743499756s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:44.998255 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7603254318237305s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:44.998255 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:45.000252 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductModelID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productmodel_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:34:45.002252 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:45.333081 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m00:34:45.335081 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=d17f9712-bad5-445b-a512-3cf5d0d8015f) - Closing cursor
[0m00:34:45.337082 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.099151849746704s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:45.337082 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.099151849746704s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:45.338082 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:45.338082 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:34:45.339082 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:45.485094 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:45.487094 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=2074338e-ba87-4be9-9bc8-86f98ba56311) - Closing cursor
[0m00:34:45.490095 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.2521648406982422s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:45.491095 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.2521648406982422s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:45.491095 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:45.492095 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:34:45.492095 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:45.651131 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:34:45.655133 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=e808b2a2-d16b-4b5a-ad91-6b5d83c75aff) - Closing cursor
[0m00:34:45.660132 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.4212021827697754s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:45.661133 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.4232027530670166s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:45.662134 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:45.663133 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:34:45.663133 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:45.804146 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:34:45.807146 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=2150481f-5d65-422b-9211-bd7100574d44) - Closing cursor
[0m00:34:45.809146 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.5712165832519531s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:45.810147 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.5722167491912842s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:45.810147 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:45.811147 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:34:45.811147 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:45.989161 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:34:45.991161 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=c0bcb255-73b4-4871-99a5-aca738705f6f) - Closing cursor
[0m00:34:45.995162 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.7572321891784668s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:45.996162 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.758232593536377s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:45.997163 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:45.997163 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:34:45.998163 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:46.137190 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:34:46.140191 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=aa5cd5ad-d3e9-4d86-90e0-de87aefa952e) - Closing cursor
[0m00:34:46.142191 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:46.143191 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.9052612781524658s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:46.144191 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.9052612781524658s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:46.144191 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:46.145192 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productmodel_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:34:46.145192 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:49.506575 [debug] [Thread-1 (]: SQL status: OK in 3.359999895095825 seconds
[0m00:34:49.507575 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=c3a85521-df19-4c9e-b43d-f33b710ddfe2) - Closing cursor
[0m00:34:49.510576 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:34:49.511575 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.273645401000977s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:49.511575 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.273645401000977s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:49.512577 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:34:49.513576 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:34:49.514577 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:49.933636 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m00:34:49.935636 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=4370494b-b4bd-4375-95b0-7b50d74fc1b9) - Closing cursor
[0m00:34:49.937637 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:34:49.938637 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:49.939637 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:49.940637 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FAEC4E30>]}
[0m00:34:49.940637 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 5.70s]
[0m00:34:49.941637 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m00:34:49.942638 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:34:49.943638 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m00:34:49.944638 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.00400090217590332s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:49.944638 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m00:34:49.945638 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0050013065338134766s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m00:34:49.945638 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.006001472473144531s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Acquired connection on thread (13520, 1092), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m00:34:49.946638 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:34:49.950639 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:34:49.954640 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.014003515243530273s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:49.954640 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.015003442764282227s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:49.955640 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:49.955640 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:34:49.956641 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:50.244669 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:34:50.248669 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=5aa86057-e5a6-4e7f-98a6-f8f621794a8d) - Closing cursor
[0m00:34:50.251670 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.31203341484069824s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:50.252670 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.3130335807800293s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:50.253670 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:50.253670 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
select * from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:34:50.254670 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:50.397848 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:34:50.402850 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=75561259-906e-4949-bb47-68c6a14dce64) - Closing cursor
[0m00:34:50.405851 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.4662139415740967s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:50.406851 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.46721386909484863s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:50.406851 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:50.407851 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:34:50.407851 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:50.561907 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:50.564907 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=fd0cd55c-8a7f-40e2-a45c-9e775ded39a7) - Closing cursor
[0m00:34:50.567908 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.6282715797424316s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:50.567908 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.6282715797424316s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:50.568909 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:50.568909 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:34:50.569909 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:50.717735 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:50.720736 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=7a8f7708-0405-4453-9ef6-ba19c119d57b) - Closing cursor
[0m00:34:50.723736 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7840993404388428s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:50.724737 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.785099983215332s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:50.724737 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:50.725737 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderDetailID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:34:50.726737 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:51.102543 [debug] [Thread-1 (]: SQL status: OK in 0.3799999952316284 seconds
[0m00:34:51.103543 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=d2218da2-3ddf-45d9-8f90-dc31390cc355) - Closing cursor
[0m00:34:51.106544 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.165907621383667s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:51.106544 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.1669073104858398s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:51.107544 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:51.107544 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:34:51.108545 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:51.238600 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:34:51.241601 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=118f1a0d-3428-4388-b4d3-4632a2cca25b) - Closing cursor
[0m00:34:51.243601 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.3039638996124268s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:51.244601 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.3049638271331787s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:51.244601 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:51.245601 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:34:51.246601 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:51.397665 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:51.399666 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=54b91925-1ae1-48dd-8cc2-ea4312df7182) - Closing cursor
[0m00:34:51.403668 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.4640307426452637s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:51.404668 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.4650318622589111s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:51.404668 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:51.405668 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:34:51.405668 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:51.546727 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:34:51.549728 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=34f6b307-7d82-4050-8662-38077cac2448) - Closing cursor
[0m00:34:51.552729 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.6130919456481934s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:51.552729 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.6130919456481934s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:51.553728 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:51.553728 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:34:51.554729 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:51.712555 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:34:51.715555 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=c80fd818-98a9-466d-a10c-abd5819b1f36) - Closing cursor
[0m00:34:51.717555 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7779185771942139s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:51.718556 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7789192199707031s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:51.719557 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:51.719557 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:34:51.720557 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:51.850543 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:34:51.854544 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=a3042260-9951-4ac1-af40-ee4e7c4fdade) - Closing cursor
[0m00:34:51.856545 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:51.858545 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.918907880783081s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:51.858545 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.918907880783081s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:51.859547 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:51.860548 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:34:51.860548 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:55.043256 [debug] [Thread-1 (]: SQL status: OK in 3.180000066757202 seconds
[0m00:34:55.045257 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=b873be32-cdbb-4067-94c8-beb177ec16f8) - Closing cursor
[0m00:34:55.047257 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:34:55.048257 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.1086204051971436s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:55.049258 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.109620809555054s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:55.049258 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:34:55.050258 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:34:55.051259 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:55.480309 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m00:34:55.482310 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=715276b8-9c44-4530-9cda-0b2190f6f85e) - Closing cursor
[0m00:34:55.484310 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:34:55.485310 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:55.486311 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:34:55.486311 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FAF07F50>]}
[0m00:34:55.487311 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 5.54s]
[0m00:34:55.489311 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:34:55.489311 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:34:55.490312 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m00:34:55.491312 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0050008296966552734s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:55.491312 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m00:34:55.492312 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.00600123405456543s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:34:55.492312 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.00600123405456543s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Acquired connection on thread (13520, 1092), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m00:34:55.493313 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:34:55.497313 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:34:55.500314 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.01400303840637207s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:55.501314 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.015003204345703125s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:55.502315 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:55.502315 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:34:55.503315 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:55.715333 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:34:55.718334 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=a50efb32-33e4-44a4-9048-d01b2bf561a5) - Closing cursor
[0m00:34:55.720334 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.2340233325958252s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:55.721335 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.23502397537231445s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:55.721335 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:55.722335 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
select * from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:34:55.723335 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:55.871351 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:34:55.875352 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=ddc0e46c-835e-4b4e-95a7-302ee997b105) - Closing cursor
[0m00:34:55.878353 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.39104151725769043s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:55.878353 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.3920414447784424s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:55.879353 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:55.879353 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:34:55.880353 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:56.074392 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:34:56.077393 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=075e3c5f-2714-40f2-8f2b-a41d88e5bda1) - Closing cursor
[0m00:34:56.080393 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.5940818786621094s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:56.081393 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.5950820446014404s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:56.082393 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:56.082393 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:34:56.083393 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:56.246392 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:34:56.249393 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=9e7e5d02-a185-40e5-871f-06e5059fa2a8) - Closing cursor
[0m00:34:56.253394 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.7660820484161377s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:56.253394 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.767082691192627s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:56.254394 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:56.255394 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:34:56.257395 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:56.725334 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m00:34:56.726334 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=bc1816da-c179-4242-b136-59e9bec6f70e) - Closing cursor
[0m00:34:56.729335 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.2430241107940674s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:56.730335 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.2440240383148193s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:56.730335 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:56.731336 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:34:56.732336 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:56.868468 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:34:56.871470 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=8e603d86-6000-40e7-a2a2-28a633c41d1a) - Closing cursor
[0m00:34:56.873470 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.3871588706970215s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:56.874470 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.3881590366363525s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:56.874470 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:56.875471 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:34:56.876472 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:57.089348 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:34:57.092348 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=dd2e2f1a-7c8d-49bd-af1e-d7761759c752) - Closing cursor
[0m00:34:57.095347 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.6090362071990967s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:57.096347 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.6090362071990967s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:57.096347 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:57.097348 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:34:57.097348 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:57.265319 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:34:57.269320 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=8e95efaa-7012-4e85-a814-ca064287befa) - Closing cursor
[0m00:34:57.273321 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.7870094776153564s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:57.274321 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.7880098819732666s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:57.274321 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:57.275321 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:34:57.275321 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:57.478562 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:34:57.481563 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=2402e4bd-1110-43e3-b7a5-e102cf6ad053) - Closing cursor
[0m00:34:57.484564 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.9982528686523438s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:57.485564 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.9982528686523438s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:57.485564 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:57.486564 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:34:57.486564 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:34:57.653560 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:34:57.657561 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=1eb5021c-91f5-407b-979f-3631252f0b0e) - Closing cursor
[0m00:34:57.660561 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:57.662561 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.176250457763672s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:34:57.663564 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.177253007888794s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:34:57.663564 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:34:57.664562 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderheader_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:34:57.664562 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:35:01.397658 [debug] [Thread-1 (]: SQL status: OK in 3.7300000190734863 seconds
[0m00:35:01.399660 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=187c62e9-4239-4f0c-abad-113931193615) - Closing cursor
[0m00:35:01.402659 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:35:01.403659 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=5.917348384857178s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Checking idleness
[0m00:35:01.404660 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=5.917348384857178s, acquire-count=1, language=sql, thread-identifier=(13520, 1092), compute-name=) - Retrieving connection
[0m00:35:01.404660 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:35:01.405660 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:35:01.405660 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=Unknown) - Created cursor
[0m00:35:01.855061 [debug] [Thread-1 (]: SQL status: OK in 0.44999998807907104 seconds
[0m00:35:01.856061 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=219004a0-0bde-49b8-8c84-7244483e71cb, command-id=8c09dfd4-5371-43d4-ac5b-946a364dabd9) - Closing cursor
[0m00:35:01.857061 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:35:01.858062 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:35:01.859062 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2697148997248, session-id=219004a0-0bde-49b8-8c84-7244483e71cb, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(13520, 1092), compute-name=) - Released connection
[0m00:35:01.859062 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a1684a06-74be-4310-ad2c-cb02a4de0ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FAF04BC0>]}
[0m00:35:01.860062 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 6.37s]
[0m00:35:01.861062 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:35:01.863063 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=31.03872036933899s, acquire-count=0, language=None, thread-identifier=(13520, 17828), compute-name=) - Checking idleness
[0m00:35:01.863063 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=31.0397207736969s, acquire-count=0, language=None, thread-identifier=(13520, 17828), compute-name=) - Reusing connection previously named master
[0m00:35:01.864063 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=31.0397207736969s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Acquired connection on thread (13520, 17828), using default compute resource
[0m00:35:01.864063 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=31.040720462799072s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Checking idleness
[0m00:35:01.865063 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=None, name=master, idle-time=31.040720462799072s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Retrieving connection
[0m00:35:01.865063 [debug] [MainThread]: On master: ROLLBACK
[0m00:35:01.865063 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:35:02.062077 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=7efe3659-ccf5-4ab2-8f03-e1c7da26e4a8, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Connection created
[0m00:35:02.063078 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:35:02.063078 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=7efe3659-ccf5-4ab2-8f03-e1c7da26e4a8, name=master, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Checking idleness
[0m00:35:02.064079 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=7efe3659-ccf5-4ab2-8f03-e1c7da26e4a8, name=master, idle-time=0.0020012855529785156s, acquire-count=1, language=None, thread-identifier=(13520, 17828), compute-name=) - Retrieving connection
[0m00:35:02.064079 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:35:02.065078 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:35:02.065078 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2697144879008, session-id=7efe3659-ccf5-4ab2-8f03-e1c7da26e4a8, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(13520, 17828), compute-name=) - Released connection
[0m00:35:02.066078 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:35:02.066078 [debug] [MainThread]: On master: ROLLBACK
[0m00:35:02.067079 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:35:02.067079 [debug] [MainThread]: On master: Close
[0m00:35:02.068079 [debug] [MainThread]: Databricks adapter: Connection(session-id=7efe3659-ccf5-4ab2-8f03-e1c7da26e4a8) - Closing connection
[0m00:35:02.130089 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:35:02.131090 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:35:02.131090 [debug] [MainThread]: Databricks adapter: Connection(session-id=86e9f837-01c9-4ef1-a374-e6291ab80749) - Closing connection
[0m00:35:02.189096 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:35:02.190099 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:35:02.190099 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:35:02.191099 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:35:02.191099 [debug] [MainThread]: Databricks adapter: Connection(session-id=f32f26d1-b23d-4c70-b5fd-ffabbb652ad0) - Closing connection
[0m00:35:02.264115 [debug] [MainThread]: Connection 'snapshot.medallion_spark.salesorderheader_snapshot' was properly closed.
[0m00:35:02.265115 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: ROLLBACK
[0m00:35:02.265115 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:35:02.266116 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: Close
[0m00:35:02.266116 [debug] [MainThread]: Databricks adapter: Connection(session-id=219004a0-0bde-49b8-8c84-7244483e71cb) - Closing connection
[0m00:35:02.335116 [info ] [MainThread]: 
[0m00:35:02.336116 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 33.18 seconds (33.18s).
[0m00:35:02.338116 [debug] [MainThread]: Command end result
[0m00:35:02.384127 [info ] [MainThread]: 
[0m00:35:02.384127 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m00:35:02.385127 [info ] [MainThread]: 
[0m00:35:02.386127 [error] [MainThread]:   Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Title` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `PostalCode`, `AddressID`, `CustomerId`]. SQLSTATE: 42703; line 10 pos 8
[0m00:35:02.387128 [info ] [MainThread]: 
[0m00:35:02.388127 [error] [MainThread]:   Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`size`, `model`, `weight`, `listPrice`, `product_name`]. SQLSTATE: 42703; line 9 pos 8
[0m00:35:02.389128 [info ] [MainThread]: 
[0m00:35:02.390128 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=0 TOTAL=7
[0m00:35:02.391128 [debug] [MainThread]: Command `dbt snapshot` failed at 00:35:02.391128 after 36.00 seconds
[0m00:35:02.392129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FAEF7B60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273FAF67320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273DF02B980>]}
[0m00:35:02.392129 [debug] [MainThread]: Flushing usage events
[0m00:37:07.589370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A27D7740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A5167C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A5167CE0>]}


============================== 00:37:07.594370 | 9dc095c4-0a38-457f-9c44-2e9cdf76c5fe ==============================
[0m00:37:07.594370 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:37:07.595369 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt snapshot', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m00:37:07.765409 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:37:07.765409 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:37:07.766409 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:37:09.295754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A4330560>]}
[0m00:37:09.354768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A2F4D130>]}
[0m00:37:09.355768 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:37:09.370772 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:37:09.623829 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:37:09.624829 [debug] [MainThread]: Partial parsing: updated file: medallion_spark://snapshots\customer.sql
[0m00:37:10.023920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269BFD7A2D0>]}
[0m00:37:10.175953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269BFE2A810>]}
[0m00:37:10.176954 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:37:10.177953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269BFE703B0>]}
[0m00:37:10.180955 [info ] [MainThread]: 
[0m00:37:10.181954 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15072, 8020), compute-name=) - Creating connection
[0m00:37:10.181954 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:37:10.182955 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Acquired connection on thread (15072, 8020), using default compute resource
[0m00:37:10.188956 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2652727659936, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15072, 16676), compute-name=) - Creating connection
[0m00:37:10.189955 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:37:10.189955 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2652727659936, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15072, 16676), compute-name=) - Acquired connection on thread (15072, 16676), using default compute resource
[0m00:37:10.190956 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2652727659936, session-id=None, name=list_hive_metastore, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(15072, 16676), compute-name=) - Checking idleness
[0m00:37:10.190956 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2652727659936, session-id=None, name=list_hive_metastore, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(15072, 16676), compute-name=) - Retrieving connection
[0m00:37:10.191957 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:37:10.191957 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:37:10.192957 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:37:10.444814 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2652727659936, session-id=0aebc4b4-a0bf-4ccc-86e9-43a0b13b4f36, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15072, 16676), compute-name=) - Connection created
[0m00:37:10.444814 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0aebc4b4-a0bf-4ccc-86e9-43a0b13b4f36, command-id=Unknown) - Created cursor
[0m00:37:10.572456 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m00:37:10.576457 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0aebc4b4-a0bf-4ccc-86e9-43a0b13b4f36, command-id=70867dc5-8a6e-4d0c-8101-c2d8220f5a0d) - Closing cursor
[0m00:37:10.577456 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2652727659936, session-id=0aebc4b4-a0bf-4ccc-86e9-43a0b13b4f36, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15072, 16676), compute-name=) - Released connection
[0m00:37:10.579457 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15072, 18236), compute-name=) - Creating connection
[0m00:37:10.580457 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:37:10.580457 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Acquired connection on thread (15072, 18236), using default compute resource
[0m00:37:10.581457 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Checking idleness
[0m00:37:10.581457 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Retrieving connection
[0m00:37:10.582458 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:37:10.582458 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:37:10.583458 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:37:10.773488 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Connection created
[0m00:37:10.774486 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=Unknown) - Created cursor
[0m00:37:10.921523 [debug] [ThreadPool]: SQL status: OK in 0.3400000035762787 seconds
[0m00:37:10.926524 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=38acc9f6-b879-46b6-9ea2-3ad0f22c3151) - Closing cursor
[0m00:37:10.937527 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_saleslt, idle-time=0.16403889656066895s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Checking idleness
[0m00:37:10.938527 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_saleslt, idle-time=0.1650393009185791s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Retrieving connection
[0m00:37:10.938527 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_saleslt, idle-time=0.1650393009185791s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Checking idleness
[0m00:37:10.939527 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_saleslt, idle-time=0.16603899002075195s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Retrieving connection
[0m00:37:10.939527 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:10.940527 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:37:10.940527 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:37:10.941527 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=Unknown) - Created cursor
[0m00:37:11.139755 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m00:37:11.142756 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=d9d598b3-2741-4125-bde3-d2ae90a5a57e) - Closing cursor
[0m00:37:11.147757 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_saleslt, idle-time=0.3742687702178955s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Checking idleness
[0m00:37:11.148757 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_saleslt, idle-time=0.37526965141296387s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Retrieving connection
[0m00:37:11.148757 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:37:11.149757 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:37:11.149757 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=Unknown) - Created cursor
[0m00:37:11.321575 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:37:11.323575 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=1d750fe2-b432-42c2-a217-b8454626ea4d) - Closing cursor
[0m00:37:11.324575 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15072, 18236), compute-name=) - Released connection
[0m00:37:11.325576 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_saleslt, idle-time=0.0010008811950683594s, acquire-count=0, language=None, thread-identifier=(15072, 18236), compute-name=) - Checking idleness
[0m00:37:11.327577 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:37:11.328577 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_snapshots, idle-time=0.0040018558502197266s, acquire-count=0, language=None, thread-identifier=(15072, 18236), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m00:37:11.328577 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_snapshots, idle-time=0.0040018558502197266s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Acquired connection on thread (15072, 18236), using default compute resource
[0m00:37:11.329577 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_snapshots, idle-time=0.00500178337097168s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Checking idleness
[0m00:37:11.329577 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_snapshots, idle-time=0.00500178337097168s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Retrieving connection
[0m00:37:11.330577 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:37:11.330577 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:37:11.330577 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=Unknown) - Created cursor
[0m00:37:11.493506 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:37:11.497507 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=e27734b0-0b77-4712-8efd-a5a43038c4d7) - Closing cursor
[0m00:37:11.500508 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_snapshots, idle-time=0.17493200302124023s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Checking idleness
[0m00:37:11.500508 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_snapshots, idle-time=0.1759324073791504s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Retrieving connection
[0m00:37:11.501508 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:37:11.501508 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:37:11.502508 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=Unknown) - Created cursor
[0m00:37:11.613526 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:37:11.615526 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=6dad0424-dc67-42f5-a5ff-e9aee661353b) - Closing cursor
[0m00:37:11.618527 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_snapshots, idle-time=0.29395103454589844s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Checking idleness
[0m00:37:11.618527 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_snapshots, idle-time=0.29395103454589844s, acquire-count=1, language=None, thread-identifier=(15072, 18236), compute-name=) - Retrieving connection
[0m00:37:11.619528 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:37:11.619528 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:37:11.620528 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=Unknown) - Created cursor
[0m00:37:11.768572 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:11.771574 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, command-id=4e6441e1-7009-4fda-b64c-74e04ee2204a) - Closing cursor
[0m00:37:11.771574 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2653215268048, session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15072, 18236), compute-name=) - Released connection
[0m00:37:11.774574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A2286E10>]}
[0m00:37:11.775575 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=1.5926203727722168s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Checking idleness
[0m00:37:11.776575 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=1.5926203727722168s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Retrieving connection
[0m00:37:11.776575 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=1.5936198234558105s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Checking idleness
[0m00:37:11.777575 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=1.5946202278137207s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Retrieving connection
[0m00:37:11.778576 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:11.778576 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:37:11.778576 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15072, 8020), compute-name=) - Released connection
[0m00:37:11.779576 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:37:11.780575 [info ] [MainThread]: 
[0m00:37:11.783576 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m00:37:11.785577 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m00:37:11.786577 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15072, 9032), compute-name=) - Creating connection
[0m00:37:11.787576 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m00:37:11.788577 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Acquired connection on thread (15072, 9032), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m00:37:11.790578 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m00:37:11.800580 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m00:37:11.832587 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04400992393493652s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:11.833587 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04500985145568848s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:11.834588 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04601097106933594s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:11.834588 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04601097106933594s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:11.835588 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:11.836589 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:11.836589 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:37:11.837589 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:37:12.021583 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Connection created
[0m00:37:12.022582 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:12.343667 [debug] [Thread-1 (]: SQL status: OK in 0.5099999904632568 seconds
[0m00:37:12.348668 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=73c51f6d-233b-4dc0-9ec9-cce4b9b9b2e9) - Closing cursor
[0m00:37:12.392678 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.37009572982788086s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:12.392678 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.3710951805114746s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:12.393679 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:12.394679 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
select * from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:37:12.395680 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:12.553699 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:37:12.556699 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=38ba63ea-0b94-495e-ac2b-2fa50398185e) - Closing cursor
[0m00:37:12.561701 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5391178131103516s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:12.561701 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5401179790496826s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:12.562701 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:12.562701 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:37:12.563701 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:12.730927 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:37:12.733928 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=c17d678c-c522-4d72-aec6-95cf49a2f08e) - Closing cursor
[0m00:37:12.738929 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.7173464298248291s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:12.739930 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.7183470726013184s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:12.739930 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:12.740930 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:37:12.740930 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:12.896732 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:37:12.899733 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=b70afa5e-8130-4668-98fd-47eece667de3) - Closing cursor
[0m00:37:12.924739 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.9031562805175781s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:12.925738 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.9041557312011719s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:12.926739 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:12.927739 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`address_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:37:12.928738 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:13.692051 [debug] [Thread-1 (]: SQL status: OK in 0.7599999904632568 seconds
[0m00:37:13.693051 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=27131f1d-a972-419d-b891-7255f29487ee) - Closing cursor
[0m00:37:13.695051 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=1.6734685897827148s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:13.696051 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=1.6744685173034668s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:13.696051 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:13.697052 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:37:13.697052 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:13.828081 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:37:13.831083 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=14d4c355-2b57-4721-8c06-fe7e9d24e3a3) - Closing cursor
[0m00:37:13.834083 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=1.811500072479248s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:13.835083 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=1.8135006427764893s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:13.835083 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:13.836084 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:37:13.837083 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:13.995142 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:37:13.998143 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=14ab9909-413b-4633-8f67-6476e5adc1c3) - Closing cursor
[0m00:37:14.001144 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9795613288879395s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:14.001144 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9795613288879395s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:14.002144 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:14.002144 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:37:14.003144 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:14.143981 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:14.147982 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=61c70dee-c81b-4a40-8864-70c18af37308) - Closing cursor
[0m00:37:14.151983 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=2.1304001808166504s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:14.152983 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=2.1314008235931396s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:14.153983 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:14.153983 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:37:14.154983 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:14.321037 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:37:14.324038 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=5fbc695d-016f-4d81-803f-13ffe68bd4a3) - Closing cursor
[0m00:37:14.331040 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=2.3094570636749268s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:14.332040 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=2.3094570636749268s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:14.332040 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:14.333040 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:37:14.333040 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:14.467916 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:37:14.470918 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=16165874-ab57-4053-86c8-170fde0985fc) - Closing cursor
[0m00:37:14.478920 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.address_snapshot"
[0m00:37:14.480920 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=2.4583373069763184s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:14.480920 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=2.459336996078491s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:14.481921 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:14.481921 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`address_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:37:14.482920 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:18.081347 [debug] [Thread-1 (]: SQL status: OK in 3.5999999046325684 seconds
[0m00:37:18.083347 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=6c32f7b6-6d25-4595-97cb-7adfadc79aaf) - Closing cursor
[0m00:37:18.093350 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:37:18.098351 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=6.07676887512207s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:18.099351 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=6.077768564224243s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:18.099351 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:37:18.100351 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:37:18.101351 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:18.502401 [debug] [Thread-1 (]: SQL status: OK in 0.4000000059604645 seconds
[0m00:37:18.503401 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=069e0563-3c34-4bca-8158-d5552a8a3707) - Closing cursor
[0m00:37:18.534409 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:37:18.536409 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:18.537410 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:18.538410 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A49F81A0>]}
[0m00:37:18.539410 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 6.75s]
[0m00:37:18.540410 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m00:37:18.541410 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m00:37:18.541410 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m00:37:18.542411 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.address_snapshot, idle-time=0.005001544952392578s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:18.543410 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m00:37:18.543410 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0060007572174072266s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m00:37:18.544410 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.007000923156738281s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Acquired connection on thread (15072, 9032), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m00:37:18.544410 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m00:37:18.550413 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m00:37:18.556414 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.01800370216369629s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:18.557414 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.019004106521606445s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:18.557414 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:37:18.558415 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m00:37:18.559415 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:18.855116 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:37:18.858116 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=cd0fb6ad-066f-422c-af98-0f799c5bd908) - Closing cursor
[0m00:37:18.861117 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.32370710372924805s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:18.861117 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.32370710372924805s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:18.862117 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:37:18.862117 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:37:18.863117 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:18.970157 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Closing cursor
[0m00:37:18.971158 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `FirstName` cannot be resolved. Did you mean one of the following? [`fullname`, `City`, `CustomerId`, `PostalCode`, `AddressID`]. SQLSTATE: 42703; line 10 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `FirstName` cannot be resolved. Did you mean one of the following? [`fullname`, `City`, `CustomerId`, `PostalCode`, `AddressID`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `FirstName` cannot be resolved. Did you mean one of the following? [`fullname`, `City`, `CustomerId`, `PostalCode`, `AddressID`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=04d6a7ba-9f6d-45ab-a309-3620836f90a2
[0m00:37:18.972157 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:18.980159 [debug] [Thread-1 (]: Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `FirstName` cannot be resolved. Did you mean one of the following? [`fullname`, `City`, `CustomerId`, `PostalCode`, `AddressID`]. SQLSTATE: 42703; line 10 pos 8
[0m00:37:18.981159 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:18.982160 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269C018E360>]}
[0m00:37:18.982160 [error] [Thread-1 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 0.44s]
[0m00:37:18.983160 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m00:37:18.984160 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:37:18.985161 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m00:37:18.986161 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.005001544952392578s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:18.987161 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m00:37:18.987161 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m00:37:18.988161 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.007001638412475586s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Acquired connection on thread (15072, 9032), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m00:37:18.988161 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m00:37:18.992162 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m00:37:18.996163 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.015003442764282227s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:18.997163 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.01600360870361328s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:18.998163 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:18.998163 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:37:18.999164 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:19.196643 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:37:19.199644 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=cda743e8-e646-4c54-a6c1-84fc17719505) - Closing cursor
[0m00:37:19.201645 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.22048544883728027s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:19.202645 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.22148609161376953s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:19.202645 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:19.203645 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:37:19.203645 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:19.344476 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:19.346477 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=c2cf7b0f-d50c-44ed-875e-296ebbb9886d) - Closing cursor
[0m00:37:19.349478 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.3683187961578369s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:19.350478 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.3683187961578369s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:19.350478 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:19.351478 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:37:19.351478 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:19.511516 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:37:19.514516 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=75492d5b-285c-42b7-8c9f-6d7823339d9f) - Closing cursor
[0m00:37:19.517516 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.5363571643829346s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:19.518517 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.5363571643829346s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:19.518517 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:19.519517 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:37:19.519517 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:19.664352 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:19.667352 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=11a23bd7-45d1-4ea5-99ef-4fa8bbfc48fe) - Closing cursor
[0m00:37:19.671353 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.6901941299438477s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:19.671353 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.6901941299438477s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:19.672354 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:19.673354 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId||'-'||AddressId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customeraddress_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:37:19.674354 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:20.025588 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m00:37:20.027588 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=08a0b317-de1d-41a4-bcdd-5edb0d60766c) - Closing cursor
[0m00:37:20.032589 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.0504302978515625s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:20.033590 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.0524306297302246s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:20.034590 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:20.035591 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:37:20.036590 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:20.172604 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:20.176603 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=c03ecfc0-80db-4c04-bb19-8570278e8fe4) - Closing cursor
[0m00:37:20.180605 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.199446201324463s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:20.182605 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.2004475593566895s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:20.182605 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:20.183606 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:37:20.184606 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:20.329620 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:20.331621 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=071796a7-60df-44c6-be1f-1a878f547e8c) - Closing cursor
[0m00:37:20.334622 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.3534624576568604s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:20.335622 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.3534624576568604s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:20.335622 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:20.336622 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:37:20.336622 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:20.470633 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:37:20.474634 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=19e52487-c1d2-4257-9cfc-399c41e478b2) - Closing cursor
[0m00:37:20.478635 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.4964752197265625s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:20.478635 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.4974758625030518s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:20.479634 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:20.479634 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:37:20.480635 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:20.622665 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:20.625667 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=b24fb3d4-ec9c-4bd3-99eb-7b5aed558780) - Closing cursor
[0m00:37:20.628667 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.647507667541504s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:20.629666 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.6485071182250977s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:20.629666 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:20.630667 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:37:20.630667 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:20.763679 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:37:20.766680 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=a3b11e53-ed85-4056-be79-4ff2fad8fd1b) - Closing cursor
[0m00:37:20.768682 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:20.769681 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.7885217666625977s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:20.769681 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.7885217666625977s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:20.770681 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:20.770681 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customeraddress_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:37:20.771681 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:23.670021 [debug] [Thread-1 (]: SQL status: OK in 2.9000000953674316 seconds
[0m00:37:23.671021 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=9f5b908c-f374-49d5-9da9-cec7092378e6) - Closing cursor
[0m00:37:23.674021 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:37:23.676023 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=4.6938629150390625s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:23.676023 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=4.694863796234131s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:23.677021 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:37:23.677021 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:37:23.678021 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:24.087070 [debug] [Thread-1 (]: SQL status: OK in 0.4099999964237213 seconds
[0m00:37:24.088070 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=a2e751b7-ed4a-4958-a907-b7e2e2d05f5e) - Closing cursor
[0m00:37:24.090071 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:37:24.091071 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:24.091071 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:24.092071 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269C018ECF0>]}
[0m00:37:24.093071 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 5.11s]
[0m00:37:24.094073 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:37:24.095073 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m00:37:24.096074 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m00:37:24.097073 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:24.097073 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m00:37:24.098073 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.product_snapshot, idle-time=0.007001399993896484s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m00:37:24.098073 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.product_snapshot, idle-time=0.007001399993896484s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Acquired connection on thread (15072, 9032), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m00:37:24.099073 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m00:37:24.103074 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m00:37:24.109075 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.product_snapshot, idle-time=0.018004417419433594s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:24.110075 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.product_snapshot, idle-time=0.019003868103027344s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:24.111076 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:37:24.111076 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m00:37:24.112076 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:24.302115 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:37:24.305116 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=563c06d7-f80e-40b4-8681-0752e30db04e) - Closing cursor
[0m00:37:24.307116 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.product_snapshot, idle-time=0.21604466438293457s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:24.308116 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.product_snapshot, idle-time=0.21704483032226562s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:24.308116 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:37:24.309117 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:37:24.310117 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:24.411102 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Closing cursor
[0m00:37:24.412102 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`size`, `model`, `weight`, `listPrice`, `product_name`]. SQLSTATE: 42703; line 9 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`size`, `model`, `weight`, `listPrice`, `product_name`]. SQLSTATE: 42703; line 9 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`size`, `model`, `weight`, `listPrice`, `product_name`]. SQLSTATE: 42703; line 9 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=8b0f1faf-e039-417f-b8bf-30b42fed050d
[0m00:37:24.413102 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:24.418103 [debug] [Thread-1 (]: Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`size`, `model`, `weight`, `listPrice`, `product_name`]. SQLSTATE: 42703; line 9 pos 8
[0m00:37:24.418103 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:24.419104 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269C02B5010>]}
[0m00:37:24.420104 [error] [Thread-1 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.32s]
[0m00:37:24.421103 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m00:37:24.421103 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m00:37:24.422103 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m00:37:24.423104 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.product_snapshot, idle-time=0.005000591278076172s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:24.423104 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m00:37:24.424104 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.006000995635986328s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m00:37:24.424104 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.006000995635986328s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Acquired connection on thread (15072, 9032), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m00:37:24.425104 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m00:37:24.429105 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m00:37:24.434106 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.016003131866455078s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:24.435107 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.017003536224365234s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:24.435107 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:24.436107 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:37:24.437107 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:24.642745 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:37:24.645745 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=a12bc1bd-554a-4c94-8a55-857c9b437bf0) - Closing cursor
[0m00:37:24.647746 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.22964262962341309s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:24.648746 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.23064303398132324s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:24.648746 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:24.649746 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:37:24.650747 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:25.002383 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m00:37:25.005384 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=67864492-96da-4eb8-835e-c9b420afcb57) - Closing cursor
[0m00:37:25.008385 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.5902819633483887s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:25.009385 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.5912821292877197s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:25.009385 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:25.010385 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:37:25.010385 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:25.163190 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:25.166191 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=009e3a2a-cf60-409b-af91-761e2dde8d09) - Closing cursor
[0m00:37:25.169191 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7510883808135986s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:25.170192 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7520883083343506s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:25.170192 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:25.171192 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:37:25.172192 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:25.320702 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:25.324703 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=bb59bfd9-a1ac-461c-85c6-0eaf4c850436) - Closing cursor
[0m00:37:25.327704 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.9096009731292725s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:25.328704 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.9106011390686035s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:25.329704 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:25.329704 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductModelID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productmodel_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:37:25.330705 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:25.755276 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m00:37:25.756276 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=2506e7df-ee52-4647-a93b-392adda618aa) - Closing cursor
[0m00:37:25.759277 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.3401734828948975s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:25.759277 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.3411741256713867s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:25.760277 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:25.760277 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:37:25.761277 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:25.921288 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:37:25.924290 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=799de662-4b0d-4055-aa33-f64a358f6d8f) - Closing cursor
[0m00:37:25.927290 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.5081861019134521s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:25.927290 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.5091869831085205s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:25.928290 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:25.928290 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:37:25.929290 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:26.121321 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:37:26.124323 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=de55843d-ea75-4031-a807-8568a75b0dd2) - Closing cursor
[0m00:37:26.127323 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.7082197666168213s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:26.127323 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.7092199325561523s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:26.128323 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:26.128323 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:37:26.129323 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:26.275337 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:26.279338 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=5562de2b-9d74-46f1-8183-1dd7965b1cb2) - Closing cursor
[0m00:37:26.283339 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.8652362823486328s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:26.284340 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.866236686706543s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:26.284340 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:26.285339 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:37:26.285339 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:26.462351 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:37:26.465352 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=bd9fdfa0-aa67-4186-a0d1-00beb20a1b56) - Closing cursor
[0m00:37:26.469353 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.0512502193450928s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:26.470354 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.0512502193450928s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:26.470354 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:26.471354 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:37:26.472354 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:26.625288 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:26.628289 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=04cf4bee-e05f-4931-be5f-69412ce24083) - Closing cursor
[0m00:37:26.630290 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:26.631290 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.213186740875244s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:26.631290 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=2.213186740875244s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:26.632290 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:26.632290 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productmodel_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:37:26.633290 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:30.265805 [debug] [Thread-1 (]: SQL status: OK in 3.630000114440918 seconds
[0m00:37:30.266805 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=ede9354b-240f-4cf6-8a5b-04612c1cae83) - Closing cursor
[0m00:37:30.271807 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:37:30.272808 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.8547046184539795s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:30.273808 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.8547046184539795s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:30.273808 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:37:30.274808 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:37:30.274808 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:30.691878 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m00:37:30.692877 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=e79e2a13-2da0-48bb-a93c-c78af04c6093) - Closing cursor
[0m00:37:30.694878 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:37:30.695877 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:30.695877 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:30.696879 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269C072B500>]}
[0m00:37:30.697879 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 6.27s]
[0m00:37:30.698880 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m00:37:30.699883 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:37:30.699883 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m00:37:30.700878 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.005001544952392578s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:30.701880 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m00:37:30.701880 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.006002902984619141s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m00:37:30.702881 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0070040225982666016s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Acquired connection on thread (15072, 9032), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m00:37:30.702881 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:37:30.706880 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:37:30.710881 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.015003681182861328s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:30.711881 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.016004323959350586s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:30.711881 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:30.712882 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:37:30.712882 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:30.946083 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:37:30.949084 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=ffad4bc6-53de-46b1-a1cb-4459e546305e) - Closing cursor
[0m00:37:30.951085 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.25520825386047363s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:30.952085 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.2562079429626465s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:30.953085 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:30.953085 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
select * from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:37:30.954086 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:31.093306 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:31.100307 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=e3b42b28-91c6-4e4e-9f74-04586e286f54) - Closing cursor
[0m00:37:31.102308 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.4064309597015381s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:31.103308 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.40743088722229004s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:31.103308 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:31.104309 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:37:31.104309 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:31.246918 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:31.249918 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=9aa1e862-8a98-4ecd-a742-43f202972cef) - Closing cursor
[0m00:37:31.252919 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.5560421943664551s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:31.252919 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.5570425987243652s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:31.253920 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:31.253920 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:37:31.254920 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:31.407270 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:31.410271 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=12c31e09-4a84-451f-a187-eef08835cc92) - Closing cursor
[0m00:37:31.413271 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7173945903778076s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:31.414271 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7183942794799805s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:31.414271 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:31.415272 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderDetailID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:37:31.416271 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:31.763988 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m00:37:31.764987 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=07c51a4f-681b-425f-80fb-90fcd8de7404) - Closing cursor
[0m00:37:31.766988 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.0711109638214111s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:31.767988 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.072110891342163s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:31.767988 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:31.768988 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:37:31.769988 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:31.907001 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:31.910003 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=fce3b09d-16b9-485f-82b6-1a1cd5c31117) - Closing cursor
[0m00:37:31.913003 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.2161259651184082s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:31.913003 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.2171258926391602s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:31.914003 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:31.914003 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:37:31.915003 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:32.074187 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:37:32.077187 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=cf1a37a7-f58e-4833-b423-cac216611cfd) - Closing cursor
[0m00:37:32.080188 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.3843114376068115s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:32.080188 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.3843114376068115s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:32.081188 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:32.081188 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:37:32.082189 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:32.222236 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:32.225237 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=4aa50111-c307-4cf5-a410-b7cc580c8099) - Closing cursor
[0m00:37:32.230236 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.5333611965179443s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:32.231237 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.5353600978851318s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:32.232238 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:32.233238 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:37:32.234239 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:32.391288 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:37:32.395289 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=0538970c-4729-4b8d-a5ff-e0ba377f674a) - Closing cursor
[0m00:37:32.400291 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7034132480621338s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:32.401291 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.705413579940796s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:32.402291 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:32.403291 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:37:32.403291 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:32.540072 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:32.546074 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=0a0000b6-d2c8-461f-84a1-1de948c14da9) - Closing cursor
[0m00:37:32.548074 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:32.550075 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.8531975746154785s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:32.550075 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.8541979789733887s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:32.551075 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:32.551075 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:37:32.552075 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:36.178337 [debug] [Thread-1 (]: SQL status: OK in 3.630000114440918 seconds
[0m00:37:36.179338 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=15eb6da4-834a-4621-9e74-cc95a91eab91) - Closing cursor
[0m00:37:36.182338 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:37:36.183339 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.487462282180786s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:36.184341 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.488464117050171s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:36.184341 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:37:36.185341 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:37:36.185341 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:36.592408 [debug] [Thread-1 (]: SQL status: OK in 0.4099999964237213 seconds
[0m00:37:36.593408 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=43bfb69a-a91d-4d1f-9aca-b6b1151e50c0) - Closing cursor
[0m00:37:36.595409 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:37:36.596409 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:36.596409 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:36.597409 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269C0733F20>]}
[0m00:37:36.598409 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 5.90s]
[0m00:37:36.599409 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:37:36.600410 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:37:36.600410 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m00:37:36.601410 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0050013065338134766s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:36.602410 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m00:37:36.602410 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:37:36.603411 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.007001638412475586s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Acquired connection on thread (15072, 9032), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m00:37:36.603411 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:37:36.609413 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:37:36.614413 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.018004417419433594s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:36.615414 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.018004417419433594s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:36.615414 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:36.616415 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:37:36.617414 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:36.886427 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m00:37:36.889428 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=203153b5-ecbb-4d1e-94f5-074d346a700c) - Closing cursor
[0m00:37:36.891428 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.29501938819885254s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:36.892430 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.2960212230682373s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:36.892430 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:36.893429 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
select * from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:37:36.894430 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:37.046629 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:37.051631 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=616cface-992e-4a8f-873a-c387c463b9c5) - Closing cursor
[0m00:37:37.054631 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.4582216739654541s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:37.055631 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.45922207832336426s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:37.055631 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:37.056631 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:37:37.056631 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:37.201635 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:37.204636 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=39a1d174-1ae3-4fe9-a46b-0d8326bbc477) - Closing cursor
[0m00:37:37.210639 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.6132304668426514s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:37.211639 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.6152305603027344s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:37.212640 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:37.213640 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:37:37.214640 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:37.366668 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:37.369668 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=909cdc5e-8ea9-479f-8473-0522bac9351f) - Closing cursor
[0m00:37:37.373669 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.7762601375579834s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:37.373669 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.7772603034973145s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:37.374669 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:37.375668 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:37:37.376669 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:37.865717 [debug] [Thread-1 (]: SQL status: OK in 0.49000000953674316 seconds
[0m00:37:37.868718 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=0fdd3bb8-e8c4-4b47-a4d7-f8e3de49947c) - Closing cursor
[0m00:37:37.872719 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.2763097286224365s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:37.873719 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.2773101329803467s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:37.874719 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:37.875719 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:37:37.876720 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:38.013497 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:37:38.017499 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=fd9ffe61-2524-4054-bda9-abb5f9fad8e7) - Closing cursor
[0m00:37:38.022500 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.426091194152832s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:38.023500 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.427091121673584s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:38.024501 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:38.024501 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:37:38.025501 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:38.176181 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:38.179182 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=761258df-9687-4568-9d27-d2151221e218) - Closing cursor
[0m00:37:38.181182 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.5847728252410889s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:38.182182 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.5857727527618408s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:38.182182 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:38.183183 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:37:38.183183 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:38.318220 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:37:38.321221 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=9324f882-face-4550-a0e2-016c4fb4c210) - Closing cursor
[0m00:37:38.324222 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.7278132438659668s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:38.325222 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.7288131713867188s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:38.325222 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:38.326223 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:37:38.327223 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:38.478782 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:37:38.481784 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=16de9e41-a3f6-4855-8954-2fb2d44ddc69) - Closing cursor
[0m00:37:38.484784 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.8883752822875977s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:38.484784 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.8883752822875977s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:38.485784 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:38.485784 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:37:38.486785 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:38.619631 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:37:38.623631 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=ac55e3e6-53a4-40f4-a0f1-14181bc5f63d) - Closing cursor
[0m00:37:38.626633 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:38.627633 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.031224250793457s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:38.628633 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.032223701477051s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:38.629634 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:38.629634 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderheader_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:37:38.630635 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:42.373088 [debug] [Thread-1 (]: SQL status: OK in 3.740000009536743 seconds
[0m00:37:42.374088 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=81a68bd6-62ca-4588-9fcc-33a0ef1dd1e5) - Closing cursor
[0m00:37:42.377089 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:37:42.378089 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=5.781680107116699s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Checking idleness
[0m00:37:42.379089 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=5.781680107116699s, acquire-count=1, language=sql, thread-identifier=(15072, 9032), compute-name=) - Retrieving connection
[0m00:37:42.379089 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:37:42.379089 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:37:42.380090 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=Unknown) - Created cursor
[0m00:37:42.823302 [debug] [Thread-1 (]: SQL status: OK in 0.4399999976158142 seconds
[0m00:37:42.825303 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c35917c-2333-4a52-9dea-237a8b004128, command-id=9aaf9750-7bbc-4869-9482-f511068f45ee) - Closing cursor
[0m00:37:42.826303 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:37:42.827303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:42.828304 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2653218268144, session-id=0c35917c-2333-4a52-9dea-237a8b004128, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15072, 9032), compute-name=) - Released connection
[0m00:37:42.828304 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dc095c4-0a38-457f-9c44-2e9cdf76c5fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269C01F5F40>]}
[0m00:37:42.829304 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 6.23s]
[0m00:37:42.830304 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:37:42.832306 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=31.05372953414917s, acquire-count=0, language=None, thread-identifier=(15072, 8020), compute-name=) - Checking idleness
[0m00:37:42.833305 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=31.05372953414917s, acquire-count=0, language=None, thread-identifier=(15072, 8020), compute-name=) - Reusing connection previously named master
[0m00:37:42.833305 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=31.054729223251343s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Acquired connection on thread (15072, 8020), using default compute resource
[0m00:37:42.834305 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=31.055729150772095s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Checking idleness
[0m00:37:42.835306 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=None, name=master, idle-time=31.055729150772095s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Retrieving connection
[0m00:37:42.835306 [debug] [MainThread]: On master: ROLLBACK
[0m00:37:42.835306 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:37:43.043817 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=47cc57a5-dc6c-4ac6-946f-ca47a20df76a, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Connection created
[0m00:37:43.044817 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:37:43.044817 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=47cc57a5-dc6c-4ac6-946f-ca47a20df76a, name=master, idle-time=0.0009996891021728516s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Checking idleness
[0m00:37:43.045818 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=47cc57a5-dc6c-4ac6-946f-ca47a20df76a, name=master, idle-time=0.0020008087158203125s, acquire-count=1, language=None, thread-identifier=(15072, 8020), compute-name=) - Retrieving connection
[0m00:37:43.045818 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:43.046818 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:37:43.046818 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2653217071824, session-id=47cc57a5-dc6c-4ac6-946f-ca47a20df76a, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15072, 8020), compute-name=) - Released connection
[0m00:37:43.047818 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:37:43.047818 [debug] [MainThread]: On master: ROLLBACK
[0m00:37:43.048818 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:37:43.048818 [debug] [MainThread]: On master: Close
[0m00:37:43.049819 [debug] [MainThread]: Databricks adapter: Connection(session-id=47cc57a5-dc6c-4ac6-946f-ca47a20df76a) - Closing connection
[0m00:37:43.114830 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:37:43.115832 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:37:43.115832 [debug] [MainThread]: Databricks adapter: Connection(session-id=0aebc4b4-a0bf-4ccc-86e9-43a0b13b4f36) - Closing connection
[0m00:37:43.175855 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:37:43.175855 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:37:43.176856 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:37:43.177855 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:37:43.177855 [debug] [MainThread]: Databricks adapter: Connection(session-id=0e59e761-b15d-4e79-a5df-6eef55a9daea) - Closing connection
[0m00:37:43.245874 [debug] [MainThread]: Connection 'snapshot.medallion_spark.salesorderheader_snapshot' was properly closed.
[0m00:37:43.247877 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: ROLLBACK
[0m00:37:43.247877 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:37:43.248878 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: Close
[0m00:37:43.248878 [debug] [MainThread]: Databricks adapter: Connection(session-id=0c35917c-2333-4a52-9dea-237a8b004128) - Closing connection
[0m00:37:43.318896 [info ] [MainThread]: 
[0m00:37:43.319898 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 33.14 seconds (33.14s).
[0m00:37:43.321899 [debug] [MainThread]: Command end result
[0m00:37:43.373911 [info ] [MainThread]: 
[0m00:37:43.374911 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m00:37:43.374911 [info ] [MainThread]: 
[0m00:37:43.375911 [error] [MainThread]:   Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `FirstName` cannot be resolved. Did you mean one of the following? [`fullname`, `City`, `CustomerId`, `PostalCode`, `AddressID`]. SQLSTATE: 42703; line 10 pos 8
[0m00:37:43.376911 [info ] [MainThread]: 
[0m00:37:43.377912 [error] [MainThread]:   Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`size`, `model`, `weight`, `listPrice`, `product_name`]. SQLSTATE: 42703; line 9 pos 8
[0m00:37:43.378913 [info ] [MainThread]: 
[0m00:37:43.378913 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=0 TOTAL=7
[0m00:37:43.380913 [debug] [MainThread]: Command `dbt snapshot` failed at 00:37:43.380913 after 35.94 seconds
[0m00:37:43.380913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A5307AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269BFC86D20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A5065AC0>]}
[0m00:37:43.381913 [debug] [MainThread]: Flushing usage events
[0m00:40:31.656101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE4539B680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE4758F3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE4758F470>]}


============================== 00:40:31.661103 | 317d27a1-b426-4e6f-b156-7c496dc348bd ==============================
[0m00:40:31.661103 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:40:31.662104 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:40:31.683109 [info ] [MainThread]: dbt version: 1.8.2
[0m00:40:31.683109 [info ] [MainThread]: python version: 3.12.4
[0m00:40:31.685108 [info ] [MainThread]: python path: C:\Program Files\Python312\python.exe
[0m00:40:31.685108 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m00:40:31.855146 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:40:31.856146 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:40:31.857147 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:40:33.236459 [info ] [MainThread]: Using profiles dir at C:\Users\shuru\.dbt
[0m00:40:33.237459 [info ] [MainThread]: Using profiles.yml file at C:\Users\shuru\.dbt\profiles.yml
[0m00:40:33.238460 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\shuru\OneDrive\Desktop\database\medallion_spark\dbt_project.yml
[0m00:40:33.239460 [info ] [MainThread]: adapter type: databricks
[0m00:40:33.239460 [info ] [MainThread]: adapter version: 1.8.1
[0m00:40:33.330479 [info ] [MainThread]: Configuration:
[0m00:40:33.331480 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m00:40:33.331480 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m00:40:33.332481 [info ] [MainThread]: Required dependencies:
[0m00:40:33.333481 [debug] [MainThread]: Executing "git --help"
[0m00:40:33.354485 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m00:40:33.354485 [debug] [MainThread]: STDERR: "b''"
[0m00:40:33.355486 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m00:40:33.356486 [info ] [MainThread]: Connection:
[0m00:40:33.357487 [info ] [MainThread]:   host: adb-1655488425849601.1.azuredatabricks.net
[0m00:40:33.358487 [info ] [MainThread]:   http_path: sql/protocolv1/o/1655488425849601/0614-014958-bfyxstku
[0m00:40:33.359487 [info ] [MainThread]:   catalog: hive_metastore
[0m00:40:33.359487 [info ] [MainThread]:   schema: saleslt
[0m00:40:33.360487 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:40:33.361487 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1916757855792, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(17860, 9192), compute-name=) - Creating connection
[0m00:40:33.361487 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m00:40:33.362488 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1916757855792, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17860, 9192), compute-name=) - Acquired connection on thread (17860, 9192), using default compute resource
[0m00:40:33.362488 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1916757855792, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17860, 9192), compute-name=) - Checking idleness
[0m00:40:33.363488 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1916757855792, session-id=None, name=debug, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(17860, 9192), compute-name=) - Retrieving connection
[0m00:40:33.363488 [debug] [MainThread]: Using databricks connection "debug"
[0m00:40:33.364488 [debug] [MainThread]: On debug: select 1 as id
[0m00:40:33.364488 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:40:33.675729 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1916757855792, session-id=7e6d4615-25bc-457b-8776-fba1f3f2e572, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17860, 9192), compute-name=) - Connection created
[0m00:40:33.676728 [debug] [MainThread]: Databricks adapter: Cursor(session-id=7e6d4615-25bc-457b-8776-fba1f3f2e572, command-id=Unknown) - Created cursor
[0m00:40:33.927778 [debug] [MainThread]: SQL status: OK in 0.5600000023841858 seconds
[0m00:40:33.928778 [debug] [MainThread]: Databricks adapter: Cursor(session-id=7e6d4615-25bc-457b-8776-fba1f3f2e572, command-id=6605855b-14dc-4273-b0c5-a19e5aeae7b3) - Closing cursor
[0m00:40:33.929778 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1916757855792, session-id=7e6d4615-25bc-457b-8776-fba1f3f2e572, name=debug, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17860, 9192), compute-name=) - Released connection
[0m00:40:33.929778 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m00:40:33.930778 [info ] [MainThread]: [32mAll checks passed![0m
[0m00:40:33.931779 [debug] [MainThread]: Command `dbt debug` succeeded at 00:40:33.931779 after 2.43 seconds
[0m00:40:33.932779 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m00:40:33.932779 [debug] [MainThread]: On debug: Close
[0m00:40:33.933782 [debug] [MainThread]: Databricks adapter: Connection(session-id=7e6d4615-25bc-457b-8776-fba1f3f2e572) - Closing connection
[0m00:40:34.011769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE4539B680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE47ABCE30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE47CFE600>]}
[0m00:40:34.011769 [debug] [MainThread]: Flushing usage events
[0m00:46:17.429535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAC89DC140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAC89DC110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAC89DC380>]}


============================== 00:46:17.436537 | 0f885401-5567-45d0-8848-ec88c11a1a07 ==============================
[0m00:46:17.436537 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:46:17.438538 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt snapshot', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:46:17.624580 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:46:17.624580 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:46:17.625580 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:46:19.178930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAC6439430>]}
[0m00:46:19.236944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAE3335B50>]}
[0m00:46:19.236944 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:46:19.250947 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:46:19.504005 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m00:46:19.505004 [debug] [MainThread]: Partial parsing: updated file: medallion_spark://snapshots\product.sql
[0m00:46:19.506004 [debug] [MainThread]: Partial parsing: updated file: medallion_spark://snapshots\customer.sql
[0m00:46:19.900094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAC8B58D10>]}
[0m00:46:20.046126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAE38040E0>]}
[0m00:46:20.047127 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:46:20.048128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAE3720290>]}
[0m00:46:20.050128 [info ] [MainThread]: 
[0m00:46:20.052128 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16868, 14340), compute-name=) - Creating connection
[0m00:46:20.052128 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:46:20.053129 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Acquired connection on thread (16868, 14340), using default compute resource
[0m00:46:20.060130 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932983291520, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16868, 10420), compute-name=) - Creating connection
[0m00:46:20.061131 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:46:20.062131 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932983291520, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16868, 10420), compute-name=) - Acquired connection on thread (16868, 10420), using default compute resource
[0m00:46:20.062131 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932983291520, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16868, 10420), compute-name=) - Checking idleness
[0m00:46:20.063131 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932983291520, session-id=None, name=list_hive_metastore, idle-time=0.00099945068359375s, acquire-count=1, language=None, thread-identifier=(16868, 10420), compute-name=) - Retrieving connection
[0m00:46:20.063131 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:46:20.064130 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:46:20.064130 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:46:20.391184 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932983291520, session-id=7ad68764-93bd-44b4-8d7d-d9d63867892e, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16868, 10420), compute-name=) - Connection created
[0m00:46:20.392184 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7ad68764-93bd-44b4-8d7d-d9d63867892e, command-id=Unknown) - Created cursor
[0m00:46:20.966252 [debug] [ThreadPool]: SQL status: OK in 0.8999999761581421 seconds
[0m00:46:20.969253 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7ad68764-93bd-44b4-8d7d-d9d63867892e, command-id=506e8835-e4c1-43d3-9916-6db6e9bc53b9) - Closing cursor
[0m00:46:20.970253 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932983291520, session-id=7ad68764-93bd-44b4-8d7d-d9d63867892e, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16868, 10420), compute-name=) - Released connection
[0m00:46:20.972254 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16868, 12928), compute-name=) - Creating connection
[0m00:46:20.973255 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:46:20.973255 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Acquired connection on thread (16868, 12928), using default compute resource
[0m00:46:20.974254 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0009992122650146484s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Checking idleness
[0m00:46:20.974254 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0009992122650146484s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Retrieving connection
[0m00:46:20.975254 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:46:20.975254 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:46:20.976255 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:46:21.199341 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Connection created
[0m00:46:21.200341 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=Unknown) - Created cursor
[0m00:46:21.299291 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m00:46:21.304291 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=8caae547-409d-4608-8828-698e04d96f43) - Closing cursor
[0m00:46:21.314293 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_saleslt, idle-time=0.11495256423950195s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Checking idleness
[0m00:46:21.315294 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_saleslt, idle-time=0.11595296859741211s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Retrieving connection
[0m00:46:21.316295 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_saleslt, idle-time=0.11695361137390137s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Checking idleness
[0m00:46:21.316295 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_saleslt, idle-time=0.11695361137390137s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Retrieving connection
[0m00:46:21.317294 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:21.317294 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:46:21.318294 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:46:21.318294 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=Unknown) - Created cursor
[0m00:46:21.447323 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m00:46:21.450323 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=435273bc-b0fb-4bb7-adf2-2dae6e36c2fb) - Closing cursor
[0m00:46:21.455324 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_saleslt, idle-time=0.2559828758239746s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Checking idleness
[0m00:46:21.456324 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_saleslt, idle-time=0.25698280334472656s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Retrieving connection
[0m00:46:21.456324 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:46:21.457325 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:46:21.457325 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=Unknown) - Created cursor
[0m00:46:21.599324 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:46:21.602327 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=eccc0732-7a62-4512-95b2-91dd3be3c667) - Closing cursor
[0m00:46:21.603327 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16868, 12928), compute-name=) - Released connection
[0m00:46:21.604325 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_saleslt, idle-time=0.0009982585906982422s, acquire-count=0, language=None, thread-identifier=(16868, 12928), compute-name=) - Checking idleness
[0m00:46:21.607325 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:46:21.608325 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_snapshots, idle-time=0.004997968673706055s, acquire-count=0, language=None, thread-identifier=(16868, 12928), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m00:46:21.608325 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_snapshots, idle-time=0.004997968673706055s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Acquired connection on thread (16868, 12928), using default compute resource
[0m00:46:21.609326 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_snapshots, idle-time=0.005999088287353516s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Checking idleness
[0m00:46:21.609326 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_snapshots, idle-time=0.005999088287353516s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Retrieving connection
[0m00:46:21.610325 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:46:21.610325 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:46:21.611326 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=Unknown) - Created cursor
[0m00:46:21.768353 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:46:21.772354 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=03fa1330-25bf-405f-985c-578dcea53110) - Closing cursor
[0m00:46:21.774354 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_snapshots, idle-time=0.17102694511413574s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Checking idleness
[0m00:46:21.775356 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_snapshots, idle-time=0.1720283031463623s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Retrieving connection
[0m00:46:21.775356 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:46:21.776355 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:46:21.776355 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=Unknown) - Created cursor
[0m00:46:21.950116 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:46:21.952117 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=6faa143d-943d-4ff9-a95f-a0eb64952cc5) - Closing cursor
[0m00:46:21.955117 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_snapshots, idle-time=0.3517899513244629s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Checking idleness
[0m00:46:21.956117 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_snapshots, idle-time=0.35278987884521484s, acquire-count=1, language=None, thread-identifier=(16868, 12928), compute-name=) - Retrieving connection
[0m00:46:21.956117 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:46:21.957117 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:46:21.957117 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=Unknown) - Created cursor
[0m00:46:22.141187 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m00:46:22.144188 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, command-id=53fdbf2b-044d-4d61-a7d9-ba408da8008c) - Closing cursor
[0m00:46:22.145186 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2932984317328, session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16868, 12928), compute-name=) - Released connection
[0m00:46:22.148187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAC5FF99D0>]}
[0m00:46:22.149187 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=2.096057891845703s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Checking idleness
[0m00:46:22.149187 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=2.096057891845703s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Retrieving connection
[0m00:46:22.150187 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=2.097058057785034s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Checking idleness
[0m00:46:22.150187 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=2.097058057785034s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Retrieving connection
[0m00:46:22.151187 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:22.151187 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:46:22.152188 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16868, 14340), compute-name=) - Released connection
[0m00:46:22.152188 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:46:22.153187 [info ] [MainThread]: 
[0m00:46:22.156189 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m00:46:22.157189 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m00:46:22.158189 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16868, 17876), compute-name=) - Creating connection
[0m00:46:22.158189 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m00:46:22.159189 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Acquired connection on thread (16868, 17876), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m00:46:22.160190 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m00:46:22.168192 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m00:46:22.202200 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.043010711669921875s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:22.203200 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04401087760925293s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:22.203200 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04401087760925293s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:22.204200 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04501080513000488s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:22.205200 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:22.207201 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:22.207201 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:46:22.208202 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:46:22.369202 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Connection created
[0m00:46:22.370202 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:22.692447 [debug] [Thread-1 (]: SQL status: OK in 0.47999998927116394 seconds
[0m00:46:22.695449 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=5c2f4c95-8d65-4ef1-8e22-1869a0fcebb9) - Closing cursor
[0m00:46:22.733457 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.3642549514770508s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:22.734457 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.36525511741638184s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:22.734457 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:22.735458 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
select * from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:46:22.735458 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:22.884473 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:46:22.888474 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=bc86be8e-fae5-4659-9a46-01bed36ef8cb) - Closing cursor
[0m00:46:22.894476 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5242729187011719s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:22.894476 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5252730846405029s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:22.895477 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:22.895477 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:46:22.896477 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:23.067482 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:46:23.071483 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=a54cc56f-2683-4095-8e82-94eafc020a8d) - Closing cursor
[0m00:46:23.078484 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.7092812061309814s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:23.079484 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.7092812061309814s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:23.079484 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:23.080484 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:46:23.080484 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:23.249515 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:46:23.252515 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=c45989a1-1891-47d9-9d4e-c72ed9c7a4d2) - Closing cursor
[0m00:46:23.279521 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.9093179702758789s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:23.279521 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.9103183746337891s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:23.280521 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:23.281521 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`address_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:46:23.282521 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:24.075612 [debug] [Thread-1 (]: SQL status: OK in 0.7900000214576721 seconds
[0m00:46:24.077613 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=d65e3613-60ff-4b57-b454-499eb8cd6e8d) - Closing cursor
[0m00:46:24.080613 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=1.7114107608795166s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:24.081614 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=1.7114107608795166s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:24.081614 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:24.082613 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:46:24.082613 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:24.221632 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:46:24.225633 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=f0c26d0d-200d-417b-924b-be97079bb6ef) - Closing cursor
[0m00:46:24.228633 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=1.8594305515289307s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:24.228633 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=1.8594305515289307s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:24.229633 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:24.229633 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:46:24.230633 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:24.385660 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:46:24.390661 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=61741e66-a409-4d97-9eec-125d6a29e24f) - Closing cursor
[0m00:46:24.394662 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=2.02445912361145s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:24.394662 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=2.025459051132202s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:24.395662 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:24.395662 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:46:24.396663 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:24.529671 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:46:24.532672 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=b488dfef-0757-4ddf-865f-6bd71a0889d6) - Closing cursor
[0m00:46:24.534672 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=2.1654698848724365s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:24.535673 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=2.166470527648926s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:24.535673 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:24.536673 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:46:24.536673 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:24.697673 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:46:24.700673 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=72650de4-949a-490c-aeea-70559c7ac80b) - Closing cursor
[0m00:46:24.710676 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=2.340473175048828s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:24.710676 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=2.34147310256958s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:24.711676 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:24.711676 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:46:24.712676 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:24.842717 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:46:24.845718 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=a92556e2-ebbf-4777-9600-b9f61661c859) - Closing cursor
[0m00:46:24.853720 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.address_snapshot"
[0m00:46:24.854720 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=2.485517740249634s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:24.855722 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=2.4865193367004395s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:24.856721 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:24.856721 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`address_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:46:24.857721 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:28.920229 [debug] [Thread-1 (]: SQL status: OK in 4.059999942779541 seconds
[0m00:46:28.921230 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=2675a4dd-c69e-4995-ab96-eac302c35faf) - Closing cursor
[0m00:46:28.930232 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:46:28.936233 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=6.567030668258667s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:28.937233 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=6.568030118942261s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:28.937233 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:46:28.938233 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:46:28.938233 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:29.375248 [debug] [Thread-1 (]: SQL status: OK in 0.4399999976158142 seconds
[0m00:46:29.376249 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=79b24834-9910-4b39-82d9-0272edd9cd7c) - Closing cursor
[0m00:46:29.398254 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:46:29.400254 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:29.401255 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:29.402255 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAE3975880>]}
[0m00:46:29.403255 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 7.24s]
[0m00:46:29.404256 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m00:46:29.405256 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m00:46:29.405256 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m00:46:29.406256 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.address_snapshot, idle-time=0.005001068115234375s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:29.407256 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m00:46:29.407256 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.00600123405456543s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m00:46:29.408256 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.007001399993896484s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Acquired connection on thread (16868, 17876), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m00:46:29.408256 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m00:46:29.412257 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m00:46:29.417258 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.015003204345703125s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:29.417258 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.016002893447875977s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:29.418258 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:46:29.418258 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m00:46:29.419259 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:29.634284 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m00:46:29.639287 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=919b223b-11d3-4324-8b46-8fd7ff96b4a5) - Closing cursor
[0m00:46:29.643287 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.24203157424926758s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:29.644287 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.24303245544433594s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:29.645288 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:46:29.645288 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:46:29.646288 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:29.782295 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Closing cursor
[0m00:46:29.783295 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=a6da6cb2-7db9-40e2-be21-a510f3deaf08
[0m00:46:29.784295 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:29.792297 [debug] [Thread-1 (]: Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
[0m00:46:29.793297 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:29.794298 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAE3699730>]}
[0m00:46:29.795299 [error] [Thread-1 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 0.39s]
[0m00:46:29.796298 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m00:46:29.797299 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:46:29.799298 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m00:46:29.800299 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.007002592086791992s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:29.801300 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m00:46:29.802300 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.009003162384033203s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m00:46:29.803301 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.010004043579101562s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Acquired connection on thread (16868, 17876), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m00:46:29.804300 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m00:46:29.809301 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m00:46:29.814302 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.021004915237426758s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:29.815302 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.021004915237426758s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:29.815302 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:29.816303 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:46:29.816303 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:30.022340 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:46:30.027341 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=e598bc9a-a509-4f25-b492-49e92bc913af) - Closing cursor
[0m00:46:30.031342 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.23804473876953125s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:30.032342 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.2390453815460205s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:30.033341 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:30.033341 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:46:30.034341 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:30.173347 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:46:30.177349 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=5d213358-b4fb-4f45-9b02-4e7ce8b75120) - Closing cursor
[0m00:46:30.180349 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.3870522975921631s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:30.182350 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.38805270195007324s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:30.182350 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:30.183349 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:46:30.183349 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:30.348614 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:46:30.352614 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=6db7d094-0083-446c-b684-06e70dc4efd4) - Closing cursor
[0m00:46:30.357616 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.5643188953399658s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:30.358616 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.5653188228607178s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:30.359616 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:30.360617 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:46:30.361617 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:30.506638 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:46:30.509639 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=362a497b-e064-4be9-8b29-8357b7f64151) - Closing cursor
[0m00:46:30.512639 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.7193422317504883s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:30.513639 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.7203423976898193s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:30.513639 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:30.514640 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId||'-'||AddressId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customeraddress_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:46:30.515640 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:30.983336 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m00:46:30.984336 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=dae83a16-fc55-41fa-a2b5-7bde2a510e09) - Closing cursor
[0m00:46:30.987336 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.1930396556854248s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:30.987336 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.1940388679504395s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:30.988336 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:30.988336 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:46:30.989337 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:31.165409 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:46:31.168409 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=ca12e4a0-8eda-4297-9a48-875fcedc0217) - Closing cursor
[0m00:46:31.172410 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.378113031387329s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:31.172410 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.3791136741638184s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:31.173411 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:31.173411 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:46:31.174410 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:31.363476 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:46:31.365477 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=72567671-7eb2-4a3a-ae44-7282acec0947) - Closing cursor
[0m00:46:31.368478 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.575181484222412s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:31.368478 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.575181484222412s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:31.369478 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:31.369478 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:46:31.370479 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:31.500512 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:46:31.503512 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=8d39b675-bda1-4d59-b66a-433ab9561792) - Closing cursor
[0m00:46:31.506513 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.7132160663604736s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:31.507514 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.714217185974121s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:31.507514 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:31.508514 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:46:31.508514 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:31.704931 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:46:31.707933 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=9a8e9704-c61e-4fac-aeec-04246fff159e) - Closing cursor
[0m00:46:31.710933 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.9176361560821533s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:31.710933 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.9176361560821533s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:31.711933 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:31.711933 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:46:31.712934 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:31.847975 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:46:31.849976 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=d3273f02-95fc-484d-95a7-cc6176fb447d) - Closing cursor
[0m00:46:31.851977 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:31.852977 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.0596795082092285s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:31.853976 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=2.0606789588928223s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:31.853976 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:31.854977 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customeraddress_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:46:31.855978 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:35.822235 [debug] [Thread-1 (]: SQL status: OK in 3.9700000286102295 seconds
[0m00:46:35.823234 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=b6101743-4411-421b-8b02-58b3a0d6ec34) - Closing cursor
[0m00:46:35.826235 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:46:35.827235 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=6.033937692642212s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:35.828235 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=6.03493857383728s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:35.828235 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:46:35.829235 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:46:35.829235 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:36.263108 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m00:46:36.264108 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=7038fa14-be54-4d00-863b-cef86bfa8ba9) - Closing cursor
[0m00:46:36.266108 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:46:36.267109 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:36.268108 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:36.269110 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAE3D55F70>]}
[0m00:46:36.269110 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 6.47s]
[0m00:46:36.271109 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:46:36.271109 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m00:46:36.272111 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m00:46:36.273110 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.005002021789550781s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:36.274110 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m00:46:36.274110 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.product_snapshot, idle-time=0.006001949310302734s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m00:46:36.275111 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.product_snapshot, idle-time=0.007002592086791992s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Acquired connection on thread (16868, 17876), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m00:46:36.276110 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m00:46:36.280111 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m00:46:36.285114 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.product_snapshot, idle-time=0.017005205154418945s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:36.286113 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.product_snapshot, idle-time=0.018004894256591797s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:36.287116 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:46:36.287116 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m00:46:36.288114 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:36.503101 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:46:36.506102 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=243a3e56-477f-4bd1-ae54-26cf30c88e45) - Closing cursor
[0m00:46:36.509103 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.product_snapshot, idle-time=0.24099421501159668s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:36.509103 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.product_snapshot, idle-time=0.24099421501159668s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:36.510103 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:46:36.510103 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:46:36.511103 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:36.636114 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Closing cursor
[0m00:46:36.637115 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=b94bf32a-7a01-44ca-85c8-77061f7e3038
[0m00:46:36.639115 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:36.644116 [debug] [Thread-1 (]: Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
[0m00:46:36.645117 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:36.646119 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAE3E67BF0>]}
[0m00:46:36.647118 [error] [Thread-1 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.37s]
[0m00:46:36.648118 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m00:46:36.648118 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m00:46:36.649118 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m00:46:36.650117 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.product_snapshot, idle-time=0.005000114440917969s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:36.651119 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m00:46:36.651119 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0060024261474609375s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m00:46:36.652117 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.007000446319580078s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Acquired connection on thread (16868, 17876), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m00:46:36.652117 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m00:46:36.656121 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m00:46:36.660120 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.015003681182861328s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:36.661122 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.016005277633666992s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:36.662122 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:36.662122 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:46:36.663122 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:36.881164 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m00:46:36.883164 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=cae75b4f-c834-4e11-bb9e-4ccd081cf74f) - Closing cursor
[0m00:46:36.886165 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.24104785919189453s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:36.886165 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.24104785919189453s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:36.887165 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:36.887165 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:46:36.888166 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:37.026168 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:46:37.029169 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=d8fc2efd-f868-4dbe-9157-2dd9e586fc75) - Closing cursor
[0m00:46:37.033170 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.38805365562438965s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:37.034171 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.3890540599822998s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:37.034171 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:37.035171 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:46:37.036171 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:37.196196 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:46:37.199197 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=0ce399ce-c51b-4052-94fe-0b9064dca76d) - Closing cursor
[0m00:46:37.202197 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.5570805072784424s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:37.203198 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.5580811500549316s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:37.204198 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:37.204198 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:46:37.205197 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:37.358244 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:46:37.361245 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=9b3705d6-ff09-4004-8dde-a5455951e0d3) - Closing cursor
[0m00:46:37.365245 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7201290130615234s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:37.365245 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7201290130615234s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:37.366246 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:37.367246 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductModelID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productmodel_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:46:37.367246 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:37.707898 [debug] [Thread-1 (]: SQL status: OK in 0.3400000035762787 seconds
[0m00:46:37.709899 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=02d895e4-8953-4adc-a696-201ebe8b8f5c) - Closing cursor
[0m00:46:37.712900 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.0677831172943115s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:37.713900 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.0677831172943115s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:37.713900 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:37.714900 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:46:37.714900 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:37.852268 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:46:37.856269 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=193b2ccd-05d8-43b8-96ea-cd0a9f38ba78) - Closing cursor
[0m00:46:37.860269 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.2151527404785156s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:37.861270 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.2161533832550049s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:37.862270 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:37.862270 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:46:37.863270 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:38.030251 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:46:38.033251 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=66e9277e-85f5-4a1c-a655-0af71a685764) - Closing cursor
[0m00:46:38.035252 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.3901355266571045s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:38.036252 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.3911356925964355s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:38.036252 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:38.037253 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:46:38.037253 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:38.190074 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:46:38.193075 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=787f66cd-5539-43d6-875e-5b659d5a0e5e) - Closing cursor
[0m00:46:38.196075 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.5499587059020996s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:38.196075 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.5509586334228516s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:38.197076 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:38.197076 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:46:38.198077 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:38.359124 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:46:38.361125 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=e0702a14-70c3-4cb3-be09-0ce402f4c021) - Closing cursor
[0m00:46:38.365126 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.7200088500976562s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:38.366126 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.7210090160369873s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:38.366126 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:38.367126 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:46:38.367126 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:38.520604 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:46:38.523605 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=ae631d74-281d-4483-a5c0-50c8ce65e06d) - Closing cursor
[0m00:46:38.525605 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:38.526606 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.8814895153045654s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:38.527606 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.8824896812438965s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:38.527606 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:38.528606 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productmodel_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:46:38.528606 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:42.401681 [debug] [Thread-1 (]: SQL status: OK in 3.869999885559082 seconds
[0m00:46:42.403681 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=01d721af-fbca-467b-ab8c-d745198e98f6) - Closing cursor
[0m00:46:42.408683 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:46:42.409683 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.76456618309021s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:42.410684 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.76456618309021s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:42.410684 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:46:42.411683 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:46:42.411683 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:42.846668 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m00:46:42.847668 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=750001f5-3666-44cb-b4cc-7b285b3c00a1) - Closing cursor
[0m00:46:42.849670 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:46:42.850670 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:42.851669 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:42.851669 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAE3E888C0>]}
[0m00:46:42.852670 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 6.20s]
[0m00:46:42.853670 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m00:46:42.854670 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:46:42.854670 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m00:46:42.855670 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.005000114440917969s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:42.856672 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m00:46:42.857671 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.007001161575317383s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m00:46:42.858671 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.008001565933227539s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Acquired connection on thread (16868, 17876), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m00:46:42.858671 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:46:42.862671 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:46:42.866672 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.016002655029296875s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:42.867672 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.017002344131469727s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:42.867672 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:42.868673 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:46:42.868673 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:43.134708 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m00:46:43.138707 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=b740e848-16ef-400a-87bc-3eb145224735) - Closing cursor
[0m00:46:43.141708 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.29103827476501465s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:43.142708 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.29103827476501465s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:43.142708 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:43.143709 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
select * from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:46:43.144709 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:43.285761 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:46:43.291762 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=acfb7812-bb44-4c5c-994e-24e76784a01e) - Closing cursor
[0m00:46:43.293763 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.44309282302856445s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:43.294763 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.4440927505493164s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:43.295763 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:43.295763 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:46:43.296763 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:43.436589 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:46:43.440590 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=8e5ed54c-8cb4-4886-a0f4-a26f8dc4c99f) - Closing cursor
[0m00:46:43.443591 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.5929207801818848s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:43.444591 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.5939207077026367s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:43.445594 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:43.445594 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:46:43.446594 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:43.599735 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:46:43.603736 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=c5f81dd3-fe49-497d-ad43-f5f98b3eff7a) - Closing cursor
[0m00:46:43.607737 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7560667991638184s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:43.607737 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7570672035217285s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:43.608737 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:43.609738 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderDetailID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:46:43.611738 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:43.978008 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m00:46:43.979009 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=2c775429-61df-4138-bc7a-66aace49d7d5) - Closing cursor
[0m00:46:43.982009 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.1303393840789795s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:43.982009 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.1313395500183105s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:43.983010 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:43.983010 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:46:43.984010 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:44.149061 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:46:44.151061 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=19431189-613a-4485-9bff-428f0adec850) - Closing cursor
[0m00:46:44.154063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.3033931255340576s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:44.154063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.3033931255340576s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:44.155063 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:44.156063 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:46:44.156063 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:44.304329 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:46:44.307330 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=ad5cd2c3-fd9a-46ec-88d3-50036b1010ea) - Closing cursor
[0m00:46:44.311331 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.459660291671753s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:44.311331 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.4606611728668213s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:44.312331 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:44.312331 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:46:44.313331 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:44.443051 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:46:44.446051 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=3c536462-cd81-48ec-84fb-9de0203c24fe) - Closing cursor
[0m00:46:44.448051 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.597381353378296s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:44.449052 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.5983819961547852s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:44.450052 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:44.450052 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:46:44.451053 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:44.599078 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:46:44.602079 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=96038480-a7be-4eb8-99c6-557b0dafbe52) - Closing cursor
[0m00:46:44.605079 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7544090747833252s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:44.606080 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7544090747833252s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:44.606080 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:44.607080 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:46:44.607080 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:44.737106 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:46:44.740108 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=c04c5e5f-3ba5-449f-8b11-8f928fddef53) - Closing cursor
[0m00:46:44.742108 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:44.743109 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.8924391269683838s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:44.744109 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.8934390544891357s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:44.744109 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:44.745109 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:46:44.746110 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:48.712326 [debug] [Thread-1 (]: SQL status: OK in 3.9700000286102295 seconds
[0m00:46:48.714325 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=2657c9f1-3584-4379-936c-e8c543d7e5ce) - Closing cursor
[0m00:46:48.718326 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:46:48.720326 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.868656873703003s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:48.720326 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.869656562805176s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:48.721327 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:46:48.722327 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:46:48.723327 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:49.174275 [debug] [Thread-1 (]: SQL status: OK in 0.44999998807907104 seconds
[0m00:46:49.175275 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=1d3d969d-a7e8-4de4-aa6a-ec100ad4de04) - Closing cursor
[0m00:46:49.177276 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:46:49.177276 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:49.178276 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:49.179278 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAE39A1820>]}
[0m00:46:49.180277 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 6.32s]
[0m00:46:49.181279 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:46:49.182278 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:46:49.182278 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m00:46:49.183278 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0050013065338134766s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:49.184278 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m00:46:49.184278 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.006001949310302734s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:46:49.185279 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.007002115249633789s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Acquired connection on thread (16868, 17876), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m00:46:49.185279 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:46:49.189279 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:46:49.193280 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.015003681182861328s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:49.194280 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.016004085540771484s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:49.194280 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:49.195280 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:46:49.195280 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:49.476651 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m00:46:49.479652 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=5fdcd06d-61aa-4c3f-b003-a5e6129d1cbe) - Closing cursor
[0m00:46:49.481652 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.3033757209777832s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:49.482653 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.30437612533569336s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:49.483653 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:49.484653 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
select * from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:46:49.485653 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:49.633671 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:46:49.638672 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=24d7c0df-ea37-4d4a-973f-0b9d3743e341) - Closing cursor
[0m00:46:49.640673 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.46239614486694336s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:49.641673 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.4633970260620117s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:49.642673 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:49.642673 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:46:49.643674 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:49.789688 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:46:49.792688 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=bedae589-5aac-4dca-a2f1-7d9906d06c1a) - Closing cursor
[0m00:46:49.795688 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.6174118518829346s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:49.796689 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.6174118518829346s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:49.796689 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:49.796689 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:46:49.797689 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:49.955719 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:46:49.958720 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=1dfec4a8-2f76-462a-a508-e48ba4eb448d) - Closing cursor
[0m00:46:49.961721 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.7834444046020508s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:49.962721 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.7844445705413818s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:49.962721 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:49.964722 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:46:49.965723 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:50.425419 [debug] [Thread-1 (]: SQL status: OK in 0.46000000834465027 seconds
[0m00:46:50.426419 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=6e79dc26-9d7d-48c8-854e-e8889eb1bf5f) - Closing cursor
[0m00:46:50.431420 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.253143548965454s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:50.432420 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.2541437149047852s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:50.433421 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:50.433421 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:46:50.434420 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:50.572793 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:46:50.575794 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=203e0fdd-988e-4692-88bb-2b776a06a727) - Closing cursor
[0m00:46:50.577794 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.3995177745819092s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:50.578795 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.4005181789398193s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:50.578795 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:50.579796 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:46:50.580796 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:50.748819 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:46:50.751820 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=32f11870-ef04-4eed-b5b5-28b95520f545) - Closing cursor
[0m00:46:50.754821 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.5765442848205566s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:50.755821 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.577544927597046s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:50.755821 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:50.756821 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:46:50.756821 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:50.887822 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:46:50.890823 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=a6261c38-47ad-43ae-a46b-8e92eddaa0cf) - Closing cursor
[0m00:46:50.892823 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.7145466804504395s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:50.893823 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.7155466079711914s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:50.894825 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:50.894825 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:46:50.895824 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:51.099847 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:46:51.104847 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=521cd871-7c10-4864-be09-d2e38aebcca2) - Closing cursor
[0m00:46:51.108848 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.9305720329284668s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:51.109849 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.9315721988677979s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:51.109849 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:51.110849 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:46:51.110849 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:51.240860 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:46:51.242860 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=059a730a-6699-449f-bee5-3eeb7d338c24) - Closing cursor
[0m00:46:51.244861 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:51.246861 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.068584680557251s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:51.246861 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.068584680557251s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:51.247861 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:51.247861 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderheader_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:46:51.248861 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:55.247368 [debug] [Thread-1 (]: SQL status: OK in 4.0 seconds
[0m00:46:55.249368 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=cfa06fe2-3bf4-4321-959b-9dad44578771) - Closing cursor
[0m00:46:55.251368 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:46:55.252368 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=6.074091911315918s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Checking idleness
[0m00:46:55.253368 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=6.075092077255249s, acquire-count=1, language=sql, thread-identifier=(16868, 17876), compute-name=) - Retrieving connection
[0m00:46:55.253368 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:46:55.254369 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:46:55.255369 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=Unknown) - Created cursor
[0m00:46:55.743421 [debug] [Thread-1 (]: SQL status: OK in 0.49000000953674316 seconds
[0m00:46:55.744421 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c269b87c-6467-4121-9550-75feeb4f938f, command-id=daffedfb-ad29-42a4-b7f6-fca5d8c2b150) - Closing cursor
[0m00:46:55.746422 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:46:55.747422 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:55.747422 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2932985569280, session-id=c269b87c-6467-4121-9550-75feeb4f938f, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16868, 17876), compute-name=) - Released connection
[0m00:46:55.748422 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f885401-5567-45d0-8848-ec88c11a1a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAE393D4F0>]}
[0m00:46:55.749423 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 6.57s]
[0m00:46:55.750423 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:46:55.751424 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=33.60023641586304s, acquire-count=0, language=None, thread-identifier=(16868, 14340), compute-name=) - Checking idleness
[0m00:46:55.752423 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=33.60023641586304s, acquire-count=0, language=None, thread-identifier=(16868, 14340), compute-name=) - Reusing connection previously named master
[0m00:46:55.752423 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=33.60123610496521s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Acquired connection on thread (16868, 14340), using default compute resource
[0m00:46:55.753424 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=33.60223722457886s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Checking idleness
[0m00:46:55.754425 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=None, name=master, idle-time=33.60223722457886s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Retrieving connection
[0m00:46:55.754425 [debug] [MainThread]: On master: ROLLBACK
[0m00:46:55.754425 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:46:55.919666 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=8c01d33f-a82d-487c-b5e3-0a2f14449a53, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Connection created
[0m00:46:55.919666 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:46:55.920666 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=8c01d33f-a82d-487c-b5e3-0a2f14449a53, name=master, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Checking idleness
[0m00:46:55.920666 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=8c01d33f-a82d-487c-b5e3-0a2f14449a53, name=master, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(16868, 14340), compute-name=) - Retrieving connection
[0m00:46:55.921667 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:55.921667 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:46:55.922666 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2932980807248, session-id=8c01d33f-a82d-487c-b5e3-0a2f14449a53, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16868, 14340), compute-name=) - Released connection
[0m00:46:55.922666 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:46:55.923667 [debug] [MainThread]: On master: ROLLBACK
[0m00:46:55.923667 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:46:55.924667 [debug] [MainThread]: On master: Close
[0m00:46:55.924667 [debug] [MainThread]: Databricks adapter: Connection(session-id=8c01d33f-a82d-487c-b5e3-0a2f14449a53) - Closing connection
[0m00:46:55.992658 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:46:55.992658 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:46:55.993658 [debug] [MainThread]: Databricks adapter: Connection(session-id=7ad68764-93bd-44b4-8d7d-d9d63867892e) - Closing connection
[0m00:46:56.051448 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:46:56.052448 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:46:56.053448 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:46:56.053448 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:46:56.053448 [debug] [MainThread]: Databricks adapter: Connection(session-id=b6091c3e-cd40-436a-8b58-c0ff25c7affb) - Closing connection
[0m00:46:56.107443 [debug] [MainThread]: Connection 'snapshot.medallion_spark.salesorderheader_snapshot' was properly closed.
[0m00:46:56.107443 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: ROLLBACK
[0m00:46:56.108443 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:46:56.109443 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: Close
[0m00:46:56.109443 [debug] [MainThread]: Databricks adapter: Connection(session-id=c269b87c-6467-4121-9550-75feeb4f938f) - Closing connection
[0m00:46:56.170468 [info ] [MainThread]: 
[0m00:46:56.171470 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 36.12 seconds (36.12s).
[0m00:46:56.175470 [debug] [MainThread]: Command end result
[0m00:46:56.224481 [info ] [MainThread]: 
[0m00:46:56.225481 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m00:46:56.225481 [info ] [MainThread]: 
[0m00:46:56.226480 [error] [MainThread]:   Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
[0m00:46:56.227482 [info ] [MainThread]: 
[0m00:46:56.228482 [error] [MainThread]:   Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
[0m00:46:56.229482 [info ] [MainThread]: 
[0m00:46:56.229482 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=0 TOTAL=7
[0m00:46:56.231483 [debug] [MainThread]: Command `dbt snapshot` failed at 00:46:56.231483 after 38.97 seconds
[0m00:46:56.231483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAC871A0F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAC502C410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AAC502CAD0>]}
[0m00:46:56.232482 [debug] [MainThread]: Flushing usage events
[0m00:47:14.399576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F36035880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F3814FE00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F3814F350>]}


============================== 00:47:14.404579 | f459982d-760d-46e0-a475-6dd74be767f8 ==============================
[0m00:47:14.404579 [info ] [MainThread]: Running with dbt=1.8.2
[0m00:47:14.405579 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:47:14.574615 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:47:14.575616 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:47:14.576616 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:47:16.099961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F5312A480>]}
[0m00:47:16.156973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F38653CB0>]}
[0m00:47:16.157973 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m00:47:16.171979 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m00:47:16.416031 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:47:16.416031 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:47:16.464042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F533F6CF0>]}
[0m00:47:16.699097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F532E1BB0>]}
[0m00:47:16.700097 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m00:47:16.701098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F53258DA0>]}
[0m00:47:16.703096 [info ] [MainThread]: 
[0m00:47:16.704098 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(7096, 19236), compute-name=) - Creating connection
[0m00:47:16.705099 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:47:16.705099 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Acquired connection on thread (7096, 19236), using default compute resource
[0m00:47:16.713099 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487893504, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(7096, 5964), compute-name=) - Creating connection
[0m00:47:16.714099 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:47:16.714099 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487893504, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7096, 5964), compute-name=) - Acquired connection on thread (7096, 5964), using default compute resource
[0m00:47:16.715101 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487893504, session-id=None, name=list_hive_metastore, idle-time=0.0010020732879638672s, acquire-count=1, language=None, thread-identifier=(7096, 5964), compute-name=) - Checking idleness
[0m00:47:16.716100 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487893504, session-id=None, name=list_hive_metastore, idle-time=0.0010020732879638672s, acquire-count=1, language=None, thread-identifier=(7096, 5964), compute-name=) - Retrieving connection
[0m00:47:16.716100 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:47:16.717100 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:47:16.717100 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:47:16.887044 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487893504, session-id=171a2329-788f-4e8d-a243-a51b0e0ea7b8, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7096, 5964), compute-name=) - Connection created
[0m00:47:16.888045 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=171a2329-788f-4e8d-a243-a51b0e0ea7b8, command-id=Unknown) - Created cursor
[0m00:47:16.978966 [debug] [ThreadPool]: SQL status: OK in 0.25999999046325684 seconds
[0m00:47:16.983966 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=171a2329-788f-4e8d-a243-a51b0e0ea7b8, command-id=6d5605a8-41cf-438e-9021-416f818f9dd3) - Closing cursor
[0m00:47:16.984966 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487893504, session-id=171a2329-788f-4e8d-a243-a51b0e0ea7b8, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(7096, 5964), compute-name=) - Released connection
[0m00:47:16.986967 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, acquire-count=0, language=None, thread-identifier=(7096, 6732), compute-name=) - Creating connection
[0m00:47:16.987968 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:47:16.987968 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Acquired connection on thread (7096, 6732), using default compute resource
[0m00:47:16.988969 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0010013580322265625s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Checking idleness
[0m00:47:16.989969 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.002001047134399414s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Retrieving connection
[0m00:47:16.989969 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:47:16.990968 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:47:16.991968 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:47:17.181981 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Connection created
[0m00:47:17.182982 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=Unknown) - Created cursor
[0m00:47:17.298998 [debug] [ThreadPool]: SQL status: OK in 0.3100000023841858 seconds
[0m00:47:17.302999 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=d51d01f9-179a-46b1-815f-79b35eb56b7e) - Closing cursor
[0m00:47:17.315002 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_snapshots, idle-time=0.13302063941955566s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Checking idleness
[0m00:47:17.316001 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_snapshots, idle-time=0.13402009010314941s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Retrieving connection
[0m00:47:17.317002 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_snapshots, idle-time=0.13402009010314941s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Checking idleness
[0m00:47:17.317002 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_snapshots, idle-time=0.13502073287963867s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Retrieving connection
[0m00:47:17.318004 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:17.319004 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:47:17.319004 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:47:17.320004 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=Unknown) - Created cursor
[0m00:47:17.479010 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:47:17.482012 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=0e225f11-0124-4365-b882-d698be28f59f) - Closing cursor
[0m00:47:17.487013 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_snapshots, idle-time=0.30503129959106445s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Checking idleness
[0m00:47:17.488013 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_snapshots, idle-time=0.3060312271118164s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Retrieving connection
[0m00:47:17.488013 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:47:17.489012 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:47:17.489012 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=Unknown) - Created cursor
[0m00:47:17.656030 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:47:17.660033 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=35163136-c2d8-41a0-9a8a-72c36e7afc67) - Closing cursor
[0m00:47:17.662034 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(7096, 6732), compute-name=) - Released connection
[0m00:47:17.663035 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_snapshots, idle-time=0.0010006427764892578s, acquire-count=0, language=None, thread-identifier=(7096, 6732), compute-name=) - Checking idleness
[0m00:47:17.666033 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:47:17.667035 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_saleslt, idle-time=0.005001068115234375s, acquire-count=0, language=None, thread-identifier=(7096, 6732), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m00:47:17.668036 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_saleslt, idle-time=0.006001472473144531s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Acquired connection on thread (7096, 6732), using default compute resource
[0m00:47:17.669034 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_saleslt, idle-time=0.0069997310638427734s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Checking idleness
[0m00:47:17.670036 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_saleslt, idle-time=0.00800180435180664s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Retrieving connection
[0m00:47:17.671036 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:47:17.672036 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:47:17.673035 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=Unknown) - Created cursor
[0m00:47:17.827045 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:17.831045 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=03d9ddf9-683f-4f98-9386-388684522cb8) - Closing cursor
[0m00:47:17.833046 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_saleslt, idle-time=0.17101168632507324s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Checking idleness
[0m00:47:17.834046 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_saleslt, idle-time=0.1720116138458252s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Retrieving connection
[0m00:47:17.834046 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:47:17.835047 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:47:17.835047 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=Unknown) - Created cursor
[0m00:47:17.958085 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m00:47:17.961085 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=ba554c0d-f5be-4d9f-875c-c547e16b1f53) - Closing cursor
[0m00:47:17.963086 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_saleslt, idle-time=0.3010518550872803s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Checking idleness
[0m00:47:17.964086 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_saleslt, idle-time=0.30205225944519043s, acquire-count=1, language=None, thread-identifier=(7096, 6732), compute-name=) - Retrieving connection
[0m00:47:17.965086 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:47:17.965086 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:47:17.966087 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=Unknown) - Created cursor
[0m00:47:18.144804 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m00:47:18.147805 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, command-id=469274bc-eac7-457d-9a9e-2bcd2998f0e8) - Closing cursor
[0m00:47:18.148805 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1371487889328, session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(7096, 6732), compute-name=) - Released connection
[0m00:47:18.151805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F52E9B410>]}
[0m00:47:18.152806 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=1.447707176208496s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Checking idleness
[0m00:47:18.153807 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=1.447707176208496s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Retrieving connection
[0m00:47:18.153807 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=1.4487080574035645s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Checking idleness
[0m00:47:18.154806 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=1.449707269668579s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Retrieving connection
[0m00:47:18.155807 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:18.155807 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:47:18.156807 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(7096, 19236), compute-name=) - Released connection
[0m00:47:18.157807 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:47:18.158808 [info ] [MainThread]: 
[0m00:47:18.161808 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m00:47:18.163808 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m00:47:18.164810 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(7096, 12484), compute-name=) - Creating connection
[0m00:47:18.165809 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m00:47:18.165809 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Acquired connection on thread (7096, 12484), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m00:47:18.166809 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m00:47:18.175811 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m00:47:18.210818 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04500937461853027s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:18.211818 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04600954055786133s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:18.212819 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04701066017150879s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:18.213820 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.048011064529418945s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:18.214820 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:18.214820 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:18.215820 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:47:18.216820 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:47:18.372879 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Connection created
[0m00:47:18.373880 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:18.728148 [debug] [Thread-1 (]: SQL status: OK in 0.5099999904632568 seconds
[0m00:47:18.731149 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=f6ce5b5f-3aad-428d-92bb-8743e9addb64) - Closing cursor
[0m00:47:18.768157 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.395277738571167s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:18.769157 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.39627742767333984s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:18.769157 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:18.770157 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
select * from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:47:18.771158 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:18.923428 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:18.926429 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=859c8375-01da-4fb6-8a69-e883333e7698) - Closing cursor
[0m00:47:18.930429 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5575499534606934s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:18.931430 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5585498809814453s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:18.931430 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:18.932430 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:47:18.933430 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:19.093199 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:47:19.098203 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=3a8ac3b0-fb93-4058-af36-befc7816bef6) - Closing cursor
[0m00:47:19.104202 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.7303223609924316s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:19.104202 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.7313230037689209s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:19.105202 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:19.105202 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:47:19.106203 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:19.249625 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:47:19.252625 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=d6b6a5e8-56ff-4f41-bf7d-a72cdeecb977) - Closing cursor
[0m00:47:19.280632 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.907752513885498s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:19.281633 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.9087533950805664s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:19.281633 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:19.282632 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`address_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:47:19.283633 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:19.650268 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m00:47:19.653268 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=a18d2ded-b85c-4d37-a64e-ea1921dfe4f1) - Closing cursor
[0m00:47:19.655269 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=1.2823894023895264s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:19.656269 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=1.2823894023895264s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:19.656269 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:19.657268 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:47:19.657268 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:19.793280 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:47:19.798282 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=6ba31d6b-64ea-43a8-b226-77dacb288270) - Closing cursor
[0m00:47:19.802283 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=1.429403305053711s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:19.803283 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=1.430403232574463s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:19.804282 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:19.804282 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:47:19.805283 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:19.945310 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:47:19.948310 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=95e6173a-e0cd-407e-81cc-ba8f8ebf9e1b) - Closing cursor
[0m00:47:19.950311 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=1.5774314403533936s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:19.951311 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=1.5784316062927246s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:19.952312 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:19.952312 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:47:19.953311 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:20.083554 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:47:20.087555 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=2efd98e3-b148-45da-a3bd-cbb2b55a92af) - Closing cursor
[0m00:47:20.090556 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=1.7176764011383057s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:20.090556 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=1.7176764011383057s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:20.091556 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:20.091556 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:47:20.092556 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:20.265367 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:47:20.268368 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=31b197dc-3fdd-43c6-9a43-c5d11e33ca7a) - Closing cursor
[0m00:47:20.277370 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9044907093048096s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:20.278370 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9054908752441406s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:20.279371 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:20.279371 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m00:47:20.280371 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:20.426353 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:47:20.431355 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=6a58c1ae-ffa9-4d43-a70e-790162a3f214) - Closing cursor
[0m00:47:20.440357 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.address_snapshot"
[0m00:47:20.441357 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=2.068477153778076s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:20.441357 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=2.068477153778076s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:20.442357 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:20.443358 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`address_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:47:20.443358 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:24.161803 [debug] [Thread-1 (]: SQL status: OK in 3.7200000286102295 seconds
[0m00:47:24.162804 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=9e0a1be6-70d0-4cdb-8336-df23202b60ad) - Closing cursor
[0m00:47:24.172806 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:47:24.178807 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=5.805927515029907s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:24.179807 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=5.806927442550659s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:24.179807 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m00:47:24.180808 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m00:47:24.181808 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:24.583859 [debug] [Thread-1 (]: SQL status: OK in 0.4000000059604645 seconds
[0m00:47:24.584860 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=960c0eb7-dbfc-4d55-8eea-518ca6b99a7f) - Closing cursor
[0m00:47:24.606865 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:47:24.608865 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:24.609865 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:24.610867 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F5352EB70>]}
[0m00:47:24.611867 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 6.45s]
[0m00:47:24.613866 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m00:47:24.614867 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m00:47:24.614867 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m00:47:24.615867 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.address_snapshot, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:24.616868 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m00:47:24.616868 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.007002592086791992s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m00:47:24.617867 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.00800180435180664s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Acquired connection on thread (7096, 12484), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m00:47:24.618868 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m00:47:24.622868 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m00:47:24.627869 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.01800370216369629s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:24.628870 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.01800370216369629s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:24.628870 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:47:24.629869 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m00:47:24.629869 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:24.836668 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:47:24.839670 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=614c1069-3ea7-4da7-813b-b22a823b5cff) - Closing cursor
[0m00:47:24.843669 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.23380374908447266s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:24.844669 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.2348036766052246s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:24.844669 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m00:47:24.845670 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:47:24.846670 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:24.965725 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Closing cursor
[0m00:47:24.966726 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=be4ae426-687c-48c2-82c3-2f196dd8621d
[0m00:47:24.967726 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:24.975727 [debug] [Thread-1 (]: Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
[0m00:47:24.976727 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:24.977728 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F5301EC90>]}
[0m00:47:24.978729 [error] [Thread-1 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 0.36s]
[0m00:47:24.979729 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m00:47:24.980729 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:47:24.982730 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m00:47:24.983729 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.006003379821777344s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:24.983729 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m00:47:24.984730 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0070018768310546875s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m00:47:24.984730 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.008002758026123047s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Acquired connection on thread (7096, 12484), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m00:47:24.985730 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m00:47:24.989731 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m00:47:24.993732 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.01700448989868164s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:24.993732 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.01700448989868164s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:24.994732 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:24.994732 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:47:24.995733 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:25.218931 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m00:47:25.221931 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=c6176cd1-1152-4963-8dec-f8b0afabe508) - Closing cursor
[0m00:47:25.223932 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.24720478057861328s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:25.224932 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.24720478057861328s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:25.224932 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:25.225932 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:47:25.227933 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:25.360231 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:47:25.363232 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=621e68d4-0cbf-47ea-8a07-5d920aa71f2e) - Closing cursor
[0m00:47:25.366233 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.38950562477111816s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:25.367232 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.3905050754547119s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:25.367232 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:25.368233 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:47:25.368233 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:25.515798 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:25.518799 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=c48f5441-e033-4777-a434-008497697164) - Closing cursor
[0m00:47:25.521800 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.5450727939605713s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:25.522799 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.546072244644165s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:25.522799 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:25.523800 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:47:25.523800 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:25.692996 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m00:47:25.695997 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=15ec3007-16b1-41d9-9037-2560a3767dbb) - Closing cursor
[0m00:47:25.698999 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.7222716808319092s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:25.699999 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.723271369934082s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:25.699999 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:25.700999 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId||'-'||AddressId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customeraddress_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:47:25.701999 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:26.037028 [debug] [Thread-1 (]: SQL status: OK in 0.3400000035762787 seconds
[0m00:47:26.038028 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=2cc3db0f-0024-43d2-b44c-e66bdf8d93c2) - Closing cursor
[0m00:47:26.041029 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.0643017292022705s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:26.041029 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.0643017292022705s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:26.042029 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:26.042029 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:47:26.043029 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:26.186051 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:47:26.189052 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=d329aaf2-daac-4cb4-99cd-bd79098dc009) - Closing cursor
[0m00:47:26.192053 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.2153260707855225s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:26.193053 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.2163262367248535s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:26.193053 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:26.194052 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:47:26.194052 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:26.354423 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:47:26.356423 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=ad2b2484-bc06-46f0-98fa-1f296990ec42) - Closing cursor
[0m00:47:26.359424 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.3826968669891357s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:26.360424 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.3836970329284668s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:26.360424 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:26.361425 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:47:26.362425 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:26.494107 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:47:26.497108 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=56588b6a-32f6-48d3-8d1a-f6caed87360b) - Closing cursor
[0m00:47:26.499109 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.52238130569458s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:26.500109 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.52238130569458s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:26.500109 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:26.501109 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:47:26.501109 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:26.652107 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:26.655109 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=982794fd-2bab-4d85-9cab-a580d7ef6e1b) - Closing cursor
[0m00:47:26.658108 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.6813809871673584s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:26.658108 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.6813809871673584s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:26.659109 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:26.660109 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m00:47:26.660109 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:26.793127 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:47:26.796127 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=1d9854f0-b3f2-4b39-b37c-2979bbb7c122) - Closing cursor
[0m00:47:26.798128 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:26.799128 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.8224008083343506s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:26.800128 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.8224008083343506s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:26.800128 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:26.801129 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customeraddress_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:47:26.801129 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:30.276602 [debug] [Thread-1 (]: SQL status: OK in 3.4800000190734863 seconds
[0m00:47:30.278602 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=1e728e05-4b11-4008-9fc0-747483ec942d) - Closing cursor
[0m00:47:30.281603 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:47:30.283604 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=5.3068766593933105s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:30.284604 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=5.3068766593933105s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:30.284604 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m00:47:30.285605 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m00:47:30.285605 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:30.682699 [debug] [Thread-1 (]: SQL status: OK in 0.4000000059604645 seconds
[0m00:47:30.683699 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=44a7b8df-6017-4b79-b248-b80869476f1f) - Closing cursor
[0m00:47:30.685700 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:47:30.686701 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:30.687701 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:30.687701 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F53980A40>]}
[0m00:47:30.688701 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 5.70s]
[0m00:47:30.689701 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m00:47:30.690701 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m00:47:30.690701 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m00:47:30.691702 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0050008296966552734s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:30.692702 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m00:47:30.692702 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0060007572174072266s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m00:47:30.693701 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.product_snapshot, idle-time=0.00700068473815918s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Acquired connection on thread (7096, 12484), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m00:47:30.693701 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m00:47:30.697703 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m00:47:30.703704 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.product_snapshot, idle-time=0.017003536224365234s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:30.704703 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.product_snapshot, idle-time=0.017003536224365234s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:30.704703 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:47:30.705704 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m00:47:30.705704 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:31.002619 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:47:31.006619 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=c31664f9-477f-475c-9b0f-6fcf00daf74f) - Closing cursor
[0m00:47:31.009621 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.product_snapshot, idle-time=0.3229196071624756s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:31.010620 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.product_snapshot, idle-time=0.32391953468322754s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:31.010620 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m00:47:31.011622 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:47:31.011622 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:31.116634 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Closing cursor
[0m00:47:31.117634 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:715)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:559)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:67)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:685)
	... 35 more
, operation-id=2057813c-97d5-4f80-ae98-cb646c31dc25
[0m00:47:31.118634 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:31.123636 [debug] [Thread-1 (]: Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
[0m00:47:31.124635 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:31.125637 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F5396DE20>]}
[0m00:47:31.126636 [error] [Thread-1 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.43s]
[0m00:47:31.127638 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m00:47:31.127638 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m00:47:31.128637 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m00:47:31.129636 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0050008296966552734s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:31.129636 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m00:47:31.130637 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.006001710891723633s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m00:47:31.131638 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.006001710891723633s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Acquired connection on thread (7096, 12484), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m00:47:31.131638 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m00:47:31.135639 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m00:47:31.140639 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.015004158020019531s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:31.140639 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.016004085540771484s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:31.141640 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:31.141640 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:47:31.142640 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:31.361633 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m00:47:31.364633 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=c6fa164a-990d-4b8a-8969-cce328184b29) - Closing cursor
[0m00:47:31.367634 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.2429978847503662s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:31.368634 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.24399828910827637s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:31.369635 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:31.369635 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:47:31.370635 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:31.521677 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:31.523677 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=fa431983-0a6e-4920-9fa6-7e7c042c9a72) - Closing cursor
[0m00:47:31.525678 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.40104246139526367s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:31.526678 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.4020421504974365s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:31.527678 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:31.527678 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:47:31.528678 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:31.670697 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:47:31.673698 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=7bf76f7f-d9e3-4a8b-a328-e2b56278aa9a) - Closing cursor
[0m00:47:31.675699 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.5510637760162354s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:31.676699 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.5520634651184082s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:31.676699 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:31.677700 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:47:31.677700 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:31.828750 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:31.831751 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=65fc39b0-a828-448b-b61a-5c630293fe0a) - Closing cursor
[0m00:47:31.835752 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7111165523529053s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:31.836753 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7121169567108154s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:31.836753 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:31.837752 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductModelID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productmodel_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:47:31.838753 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:32.162991 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m00:47:32.163992 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=a30acee2-4d28-4c0f-a317-92982942eac9) - Closing cursor
[0m00:47:32.166993 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.0423574447631836s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:32.166993 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.0423574447631836s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:32.167993 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:32.167993 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:47:32.168992 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:32.305006 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:47:32.308008 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=71ac2b49-3d56-4ca0-9160-821f566ee035) - Closing cursor
[0m00:47:32.311008 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.1863722801208496s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:32.311008 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.1863722801208496s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:32.312009 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:32.312009 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:47:32.313008 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:32.458795 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:32.462796 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=a618897f-fce5-480f-acc1-0de7faacbf65) - Closing cursor
[0m00:47:32.465797 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.3411612510681152s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:32.466798 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.3421621322631836s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:32.467798 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:32.468798 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:47:32.468798 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:32.609832 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:47:32.614834 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=66efff98-9d73-43d7-86a2-74106fe7e193) - Closing cursor
[0m00:47:32.617834 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.4931983947753906s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:32.618834 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.4941987991333008s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:32.619834 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:32.620836 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:47:32.621839 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:32.797876 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m00:47:32.802876 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=7f49b0c8-0aba-42ca-a698-79c7ab62dde2) - Closing cursor
[0m00:47:32.807878 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.682241439819336s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:32.808878 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.6842427253723145s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:32.809879 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:32.810878 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m00:47:32.811880 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:32.996935 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:47:32.999935 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=450f4027-98c1-40e5-8161-3cc60393362c) - Closing cursor
[0m00:47:33.000935 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:33.002936 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.8783001899719238s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:33.003935 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.8792996406555176s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:33.003935 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:33.004935 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productmodel_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:47:33.005936 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:36.659302 [debug] [Thread-1 (]: SQL status: OK in 3.6500000953674316 seconds
[0m00:47:36.661302 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=fefe6b2a-84d2-4665-bc95-110909fe333b) - Closing cursor
[0m00:47:36.664303 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:47:36.665303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.540667533874512s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:36.666304 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.540667533874512s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:36.666304 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m00:47:36.667304 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m00:47:36.667304 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:37.061303 [debug] [Thread-1 (]: SQL status: OK in 0.38999998569488525 seconds
[0m00:47:37.062302 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=3379c744-b65f-4b8e-8c63-d4b8b6febfd2) - Closing cursor
[0m00:47:37.063303 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:47:37.064304 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:37.065303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:37.066305 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F53495910>]}
[0m00:47:37.067304 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 5.94s]
[0m00:47:37.068304 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m00:47:37.068304 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:47:37.069304 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m00:47:37.070305 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.005002021789550781s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:37.071306 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m00:47:37.071306 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.006003379821777344s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m00:47:37.072306 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.007003307342529297s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Acquired connection on thread (7096, 12484), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m00:47:37.072306 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:47:37.076307 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:47:37.080308 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.015005111694335938s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:37.081308 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.01600480079650879s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:37.082309 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:37.082309 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:47:37.083309 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:37.369321 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:47:37.372322 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=76b51311-790c-4679-8d8d-d88fb2771cab) - Closing cursor
[0m00:47:37.375323 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.3090202808380127s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:37.375323 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.31001949310302734s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:37.376323 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:37.376323 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
select * from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:47:37.377323 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:37.524351 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:37.530352 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=afd64711-23b9-4f5a-93aa-9ba11fe89d11) - Closing cursor
[0m00:47:37.533352 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.4680488109588623s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:37.533352 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.4680488109588623s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:37.534352 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:37.534352 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:47:37.535353 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:37.681388 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:37.684388 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=0e56a811-3287-4ae8-909d-2922b8109831) - Closing cursor
[0m00:47:37.687389 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.6220858097076416s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:37.688390 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.6220858097076416s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:37.688390 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:37.689390 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:47:37.689390 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:37.847392 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:47:37.850394 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=6b480463-abb4-495e-97e2-8d14c0fa3046) - Closing cursor
[0m00:47:37.854394 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7890908718109131s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:37.854394 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.7890908718109131s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:37.855394 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:37.856395 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderDetailID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:47:37.857395 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:38.225990 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m00:47:38.227991 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=6f92c048-4507-4be0-8100-5e2fe2c496fe) - Closing cursor
[0m00:47:38.229991 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.1646883487701416s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:38.230992 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.1656887531280518s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:38.230992 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:38.231992 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:47:38.232992 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:38.382027 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:38.385028 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=299e2b5e-8e09-41c2-91e1-4f99a6f08f8e) - Closing cursor
[0m00:47:38.388028 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.3227248191833496s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:38.389028 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.3237247467041016s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:38.390028 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:38.390028 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:47:38.391028 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:38.581071 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m00:47:38.585073 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=d47d9238-ffb9-42c2-83ad-74c7e65ccd05) - Closing cursor
[0m00:47:38.587073 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.5217697620391846s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:38.588076 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.522773027420044s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:38.589074 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:38.589074 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:47:38.590075 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:38.721909 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:47:38.724911 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=660ef172-84b2-4aa5-8d1b-2d01f1c68f22) - Closing cursor
[0m00:47:38.727910 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.6626064777374268s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:38.728909 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.6626064777374268s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:38.728909 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:38.729910 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:47:38.729910 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:38.888564 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m00:47:38.891565 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=ec2c2c6b-491c-4568-82c5-a75fda7bd854) - Closing cursor
[0m00:47:38.894565 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.8292622566223145s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:38.894565 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.8292622566223145s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:38.895566 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:38.895566 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m00:47:38.896566 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:39.021586 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:47:39.024588 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=2a88c356-e28c-462b-bbc5-ee4c319cc5b8) - Closing cursor
[0m00:47:39.026588 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:39.027587 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.9622845649719238s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:39.028588 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.9622845649719238s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:39.028588 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:39.029588 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:47:39.029588 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:42.874746 [debug] [Thread-1 (]: SQL status: OK in 3.8499999046325684 seconds
[0m00:47:42.875746 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=4c236e0c-ea5b-4ec3-a686-72d5facfe9b3) - Closing cursor
[0m00:47:42.879747 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:47:42.880747 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.815444469451904s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:42.881748 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.8164448738098145s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:42.882748 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m00:47:42.883748 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m00:47:42.884750 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:43.269997 [debug] [Thread-1 (]: SQL status: OK in 0.38999998569488525 seconds
[0m00:47:43.271998 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=0f8189dc-2841-44e0-8699-9f8662a33b10) - Closing cursor
[0m00:47:43.273999 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:47:43.275998 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:43.275998 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:43.276999 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F539E87D0>]}
[0m00:47:43.278000 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 6.21s]
[0m00:47:43.280000 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:47:43.281001 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:47:43.282001 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m00:47:43.284001 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.007003307342529297s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:43.284001 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m00:47:43.285001 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.009002685546875s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m00:47:43.286000 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.010002613067626953s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Acquired connection on thread (7096, 12484), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m00:47:43.286000 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:47:43.292002 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:47:43.300004 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.024006366729736328s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:43.302005 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.025007247924804688s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:43.303005 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:43.304007 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:47:43.304007 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:43.593765 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:47:43.597760 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=d7104230-26c8-427d-8394-12bdd213c630) - Closing cursor
[0m00:47:43.600761 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.3237619400024414s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:43.601763 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.32476329803466797s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:43.601763 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:43.602774 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
select * from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m00:47:43.602774 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:43.752803 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:43.757803 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=8f25649e-589a-4799-ba6c-8354bee38f17) - Closing cursor
[0m00:47:43.761804 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.48480677604675293s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:43.761804 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.4858059883117676s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:43.762804 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:43.763806 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:47:43.763806 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:43.909837 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:43.914840 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=59f37d55-e311-4c25-a523-ca09cbbc6fe7) - Closing cursor
[0m00:47:43.920841 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.6448428630828857s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:43.921841 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.6458430290222168s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:43.922841 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:43.923841 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:47:43.923841 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:44.070941 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:44.073942 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=4b3e19bc-3412-4d4b-a029-9b52229f6898) - Closing cursor
[0m00:47:44.077943 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.8019452095031738s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:44.078944 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.8029460906982422s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:44.078944 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:44.081944 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m00:47:44.083946 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:44.528146 [debug] [Thread-1 (]: SQL status: OK in 0.44999998807907104 seconds
[0m00:47:44.531148 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=7b526566-4fd8-4b5b-9d53-240279148995) - Closing cursor
[0m00:47:44.534148 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.2571494579315186s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:44.534148 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.258150339126587s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:44.535148 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:44.536149 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:47:44.537149 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:44.664177 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:47:44.668179 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=4c79a403-8f65-4c10-8cc6-217c989ba492) - Closing cursor
[0m00:47:44.671180 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.3941807746887207s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:44.672179 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.3951823711395264s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:44.672179 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:44.673180 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:47:44.673180 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:44.822214 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:44.826214 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=e01272b2-bceb-4d9f-99dc-d18ac4b2af55) - Closing cursor
[0m00:47:44.829214 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.5532164573669434s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:44.830215 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.5542171001434326s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:44.831215 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:44.831215 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:47:44.832215 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:44.963245 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m00:47:44.966246 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=72bd87bb-b82d-4e16-aec5-11d276ee7ac8) - Closing cursor
[0m00:47:44.969247 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.693248987197876s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:44.970247 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.694249153137207s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:44.971247 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:44.971247 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:47:44.972247 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:45.124285 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m00:47:45.127286 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=6c1d2920-c72d-4e1a-b731-461490a2cb91) - Closing cursor
[0m00:47:45.130286 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.8542885780334473s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:45.131287 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.8552894592285156s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:45.132287 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:45.132287 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m00:47:45.133287 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:45.271525 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m00:47:45.275526 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=9970a03c-fa90-4dd5-a31b-bcb7a725ba41) - Closing cursor
[0m00:47:45.277526 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:45.279527 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.0035290718078613s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:45.280528 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.0045299530029297s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:45.280528 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:45.281529 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderheader_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m00:47:45.282530 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:49.149341 [debug] [Thread-1 (]: SQL status: OK in 3.869999885559082 seconds
[0m00:47:49.151340 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=f1c13341-766e-407e-aed5-ba567e657628) - Closing cursor
[0m00:47:49.154342 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:47:49.155341 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=5.879343509674072s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Checking idleness
[0m00:47:49.156342 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=5.880343914031982s, acquire-count=1, language=sql, thread-identifier=(7096, 12484), compute-name=) - Retrieving connection
[0m00:47:49.156342 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m00:47:49.157341 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m00:47:49.158342 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=Unknown) - Created cursor
[0m00:47:49.591905 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m00:47:49.593904 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, command-id=e40ccf06-56d8-462c-9242-8fdffee3bb76) - Closing cursor
[0m00:47:49.594906 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:47:49.595907 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:49.596904 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1371487892352, session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(7096, 12484), compute-name=) - Released connection
[0m00:47:49.597905 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f459982d-760d-46e0-a475-6dd74be767f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F38B282C0>]}
[0m00:47:49.597905 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 6.31s]
[0m00:47:49.598905 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m00:47:49.600905 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=31.444097757339478s, acquire-count=0, language=None, thread-identifier=(7096, 19236), compute-name=) - Checking idleness
[0m00:47:49.601905 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=31.445098400115967s, acquire-count=0, language=None, thread-identifier=(7096, 19236), compute-name=) - Reusing connection previously named master
[0m00:47:49.602905 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=31.44609808921814s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Acquired connection on thread (7096, 19236), using default compute resource
[0m00:47:49.603905 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=31.44709825515747s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Checking idleness
[0m00:47:49.603905 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=None, name=master, idle-time=31.44709825515747s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Retrieving connection
[0m00:47:49.604906 [debug] [MainThread]: On master: ROLLBACK
[0m00:47:49.605907 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:47:49.793551 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=c9873594-23ac-472d-836d-7d2f8e5fe8aa, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Connection created
[0m00:47:49.794551 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:47:49.796553 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=c9873594-23ac-472d-836d-7d2f8e5fe8aa, name=master, idle-time=0.002004861831665039s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Checking idleness
[0m00:47:49.796553 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=c9873594-23ac-472d-836d-7d2f8e5fe8aa, name=master, idle-time=0.0030019283294677734s, acquire-count=1, language=None, thread-identifier=(7096, 19236), compute-name=) - Retrieving connection
[0m00:47:49.797554 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:49.798552 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:47:49.799552 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1371488687248, session-id=c9873594-23ac-472d-836d-7d2f8e5fe8aa, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(7096, 19236), compute-name=) - Released connection
[0m00:47:49.800553 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:47:49.800553 [debug] [MainThread]: On master: ROLLBACK
[0m00:47:49.801552 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:47:49.802553 [debug] [MainThread]: On master: Close
[0m00:47:49.803553 [debug] [MainThread]: Databricks adapter: Connection(session-id=c9873594-23ac-472d-836d-7d2f8e5fe8aa) - Closing connection
[0m00:47:49.871568 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:47:49.872570 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:47:49.874570 [debug] [MainThread]: Databricks adapter: Connection(session-id=171a2329-788f-4e8d-a243-a51b0e0ea7b8) - Closing connection
[0m00:47:49.955588 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:47:49.956588 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:47:49.956588 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:47:49.957588 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:47:49.958589 [debug] [MainThread]: Databricks adapter: Connection(session-id=31f85135-baee-4c92-a3f2-a50c6212f2a9) - Closing connection
[0m00:47:50.012419 [debug] [MainThread]: Connection 'snapshot.medallion_spark.salesorderheader_snapshot' was properly closed.
[0m00:47:50.012419 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: ROLLBACK
[0m00:47:50.013420 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:47:50.013420 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: Close
[0m00:47:50.014420 [debug] [MainThread]: Databricks adapter: Connection(session-id=8b331b5d-e65b-4502-8d3d-9057bce5e51b) - Closing connection
[0m00:47:50.070434 [info ] [MainThread]: 
[0m00:47:50.071434 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 33.37 seconds (33.37s).
[0m00:47:50.075434 [debug] [MainThread]: Command end result
[0m00:47:50.144450 [info ] [MainThread]: 
[0m00:47:50.145450 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m00:47:50.146450 [info ] [MainThread]: 
[0m00:47:50.148453 [error] [MainThread]:   Runtime Error in snapshot customer_snapshot (snapshots\customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `NameStyle` cannot be resolved. Did you mean one of the following? [`City`, `fullname`, `AddressID`, `AddressType`, `PostalCode`]. SQLSTATE: 42703; line 10 pos 8
[0m00:47:50.150451 [info ] [MainThread]: 
[0m00:47:50.151452 [error] [MainThread]:   Runtime Error in snapshot product_snapshot (snapshots\product.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ProductID` cannot be resolved. Did you mean one of the following? [`product_sk`, `model`, `product_name`, `weight`, `size`]. SQLSTATE: 42703; line 9 pos 8
[0m00:47:50.153453 [info ] [MainThread]: 
[0m00:47:50.154452 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=0 TOTAL=7
[0m00:47:50.156452 [debug] [MainThread]: Command `dbt snapshot` failed at 00:47:50.156452 after 35.86 seconds
[0m00:47:50.158456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F361B3230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F535234A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013F35B775F0>]}
[0m00:47:50.159453 [debug] [MainThread]: Flushing usage events
[0m01:20:59.102997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002385D8CC2F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002385B1A7920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002385D77F020>]}


============================== 01:20:59.107997 | 565b60b5-3dda-441c-a58c-cfdbbc815be8 ==============================
[0m01:20:59.107997 [info ] [MainThread]: Running with dbt=1.8.2
[0m01:20:59.108997 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt snapshot', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:20:59.283036 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:20:59.284037 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:20:59.284037 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:21:00.859393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002385DCAEF90>]}
[0m01:21:00.915405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238773AB410>]}
[0m01:21:00.916405 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m01:21:00.930408 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m01:21:01.158460 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:21:01.159460 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:21:01.205471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023878AE6B70>]}
[0m01:21:01.442525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023878906150>]}
[0m01:21:01.443526 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m01:21:01.444526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238788BC0E0>]}
[0m01:21:01.446525 [info ] [MainThread]: 
[0m01:21:01.447526 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(19220, 17992), compute-name=) - Creating connection
[0m01:21:01.448525 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:21:01.448525 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Acquired connection on thread (19220, 17992), using default compute resource
[0m01:21:01.455527 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441564438400, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(19220, 16620), compute-name=) - Creating connection
[0m01:21:01.455527 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m01:21:01.456528 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441564438400, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(19220, 16620), compute-name=) - Acquired connection on thread (19220, 16620), using default compute resource
[0m01:21:01.456528 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441564438400, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(19220, 16620), compute-name=) - Checking idleness
[0m01:21:01.457528 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441564438400, session-id=None, name=list_hive_metastore, idle-time=0.00099945068359375s, acquire-count=1, language=None, thread-identifier=(19220, 16620), compute-name=) - Retrieving connection
[0m01:21:01.457528 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m01:21:01.458528 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m01:21:01.458528 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:21:01.690601 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441564438400, session-id=4bb7e805-52de-4519-aab2-952176039c7e, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(19220, 16620), compute-name=) - Connection created
[0m01:21:01.691603 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4bb7e805-52de-4519-aab2-952176039c7e, command-id=Unknown) - Created cursor
[0m01:21:01.854050 [debug] [ThreadPool]: SQL status: OK in 0.4000000059604645 seconds
[0m01:21:01.858051 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4bb7e805-52de-4519-aab2-952176039c7e, command-id=d990272c-305f-4750-b3a2-039fbf15310b) - Closing cursor
[0m01:21:01.858051 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441564438400, session-id=4bb7e805-52de-4519-aab2-952176039c7e, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(19220, 16620), compute-name=) - Released connection
[0m01:21:01.860052 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(19220, 19108), compute-name=) - Creating connection
[0m01:21:01.861052 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m01:21:01.861052 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Acquired connection on thread (19220, 19108), using default compute resource
[0m01:21:01.862052 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Checking idleness
[0m01:21:01.863053 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0020008087158203125s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Retrieving connection
[0m01:21:01.864053 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:21:01.865053 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:21:01.865053 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:21:02.029070 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Connection created
[0m01:21:02.030070 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=Unknown) - Created cursor
[0m01:21:02.132087 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m01:21:02.137089 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=0b4f34c4-c427-4c76-a68d-d8ada5e70838) - Closing cursor
[0m01:21:02.149093 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_saleslt, idle-time=0.12002229690551758s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Checking idleness
[0m01:21:02.150092 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_saleslt, idle-time=0.12102150917053223s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Retrieving connection
[0m01:21:02.150092 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_saleslt, idle-time=0.12102150917053223s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Checking idleness
[0m01:21:02.151093 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_saleslt, idle-time=0.12202286720275879s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Retrieving connection
[0m01:21:02.151093 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:21:02.152093 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:21:02.152093 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m01:21:02.153092 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=Unknown) - Created cursor
[0m01:21:02.351108 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m01:21:02.354109 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=83bb1e2a-7228-49e0-affd-1f9266462c4e) - Closing cursor
[0m01:21:02.361111 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_saleslt, idle-time=0.33204078674316406s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Checking idleness
[0m01:21:02.361111 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_saleslt, idle-time=0.33204078674316406s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Retrieving connection
[0m01:21:02.362111 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:21:02.362111 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m01:21:02.363111 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=Unknown) - Created cursor
[0m01:21:02.501992 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:02.504993 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=907b77ec-6dff-4495-a2e5-fd2d112f5297) - Closing cursor
[0m01:21:02.505993 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(19220, 19108), compute-name=) - Released connection
[0m01:21:02.506993 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(19220, 19108), compute-name=) - Checking idleness
[0m01:21:02.508994 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m01:21:02.508994 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_snapshots, idle-time=0.003000974655151367s, acquire-count=0, language=None, thread-identifier=(19220, 19108), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m01:21:02.509994 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_snapshots, idle-time=0.004001617431640625s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Acquired connection on thread (19220, 19108), using default compute resource
[0m01:21:02.509994 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_snapshots, idle-time=0.004001617431640625s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Checking idleness
[0m01:21:02.510994 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_snapshots, idle-time=0.004001617431640625s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Retrieving connection
[0m01:21:02.510994 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:21:02.510994 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m01:21:02.511995 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=Unknown) - Created cursor
[0m01:21:02.615155 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m01:21:02.620156 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=eb90909e-8bcf-4218-a9c1-ce83f54dbd8a) - Closing cursor
[0m01:21:02.625158 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_snapshots, idle-time=0.11916518211364746s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Checking idleness
[0m01:21:02.626158 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_snapshots, idle-time=0.12016487121582031s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Retrieving connection
[0m01:21:02.627158 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:21:02.628158 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m01:21:02.628158 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=Unknown) - Created cursor
[0m01:21:02.718162 [debug] [ThreadPool]: SQL status: OK in 0.09000000357627869 seconds
[0m01:21:02.720174 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=a95df4f8-c63b-4459-910e-5f1199fa089a) - Closing cursor
[0m01:21:02.723159 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_snapshots, idle-time=0.21716666221618652s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Checking idleness
[0m01:21:02.724160 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_snapshots, idle-time=0.21816730499267578s, acquire-count=1, language=None, thread-identifier=(19220, 19108), compute-name=) - Retrieving connection
[0m01:21:02.724160 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:21:02.725160 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m01:21:02.725160 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=Unknown) - Created cursor
[0m01:21:02.863088 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:02.867089 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, command-id=07621d23-8d50-4b9b-b9c6-ca3e2cc3ead8) - Closing cursor
[0m01:21:02.868089 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2441562764304, session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(19220, 19108), compute-name=) - Released connection
[0m01:21:02.871089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002385AC76F90>]}
[0m01:21:02.872090 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=1.4225635528564453s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Checking idleness
[0m01:21:02.872090 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=1.4235641956329346s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Retrieving connection
[0m01:21:02.873090 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=1.4245641231536865s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Checking idleness
[0m01:21:02.874089 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=1.4255640506744385s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Retrieving connection
[0m01:21:02.874089 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:21:02.875091 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:21:02.876091 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(19220, 17992), compute-name=) - Released connection
[0m01:21:02.877090 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:21:02.878091 [info ] [MainThread]: 
[0m01:21:02.882092 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m01:21:02.882092 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m01:21:02.883092 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(19220, 2464), compute-name=) - Creating connection
[0m01:21:02.884091 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m01:21:02.885092 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Acquired connection on thread (19220, 2464), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m01:21:02.885092 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m01:21:02.894094 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m01:21:02.929102 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04401063919067383s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:02.929102 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04501056671142578s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:02.930102 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04601097106933594s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:02.930102 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.04601097106933594s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:02.931103 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:21:02.931103 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:02.932103 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m01:21:02.933102 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:21:03.089589 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Connection created
[0m01:21:03.090588 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:03.413237 [debug] [Thread-1 (]: SQL status: OK in 0.47999998927116394 seconds
[0m01:21:03.416238 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=87861949-98de-4e61-987c-b1810e6a9e32) - Closing cursor
[0m01:21:03.452245 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.362656831741333s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:03.453245 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.36365675926208496s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:03.454246 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:03.454246 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
select * from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m01:21:03.455246 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:03.622259 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m01:21:03.626260 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=5ebb4dc9-bb00-4dfa-a26e-2403390de527) - Closing cursor
[0m01:21:03.632262 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5426733493804932s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:03.633262 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.5436732769012451s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:03.633262 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:03.634262 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m01:21:03.634262 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:03.807283 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m01:21:03.811284 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=0808a39c-e979-4cb8-9f36-b218a3a75cee) - Closing cursor
[0m01:21:03.817288 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.7276995182037354s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:03.818287 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.7276995182037354s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:03.818287 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:03.819287 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m01:21:03.819287 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:03.975686 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:03.979688 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=862ffab0-c1ec-4fe2-bdeb-f4f70a4216a4) - Closing cursor
[0m01:21:04.005694 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.9151046276092529s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:04.005694 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.9161052703857422s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:04.006694 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:04.007694 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`address_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m01:21:04.009694 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:04.865873 [debug] [Thread-1 (]: SQL status: OK in 0.8600000143051147 seconds
[0m01:21:04.867874 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=edbef4ae-2a83-4d94-ad65-fe5f29fdf791) - Closing cursor
[0m01:21:04.869873 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=1.780285120010376s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:04.869873 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=1.780285120010376s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:04.870874 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:04.870874 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m01:21:04.871874 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:05.006940 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:05.009941 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=0038d7eb-bc8a-439e-904f-bb4d42f7db56) - Closing cursor
[0m01:21:05.011941 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9223527908325195s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:05.012941 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=1.9233529567718506s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:05.013941 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:05.013941 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m01:21:05.014942 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:05.175989 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:05.178990 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=4754ae7e-4ce0-473e-af9b-4f382b02a9bc) - Closing cursor
[0m01:21:05.181990 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=2.0924019813537598s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:05.181990 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=2.0924019813537598s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:05.182991 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:05.182991 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m01:21:05.183992 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:05.319462 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:05.322463 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=04137659-1af6-4374-b609-901987b17988) - Closing cursor
[0m01:21:05.325464 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=2.2358756065368652s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:05.326464 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=2.2358756065368652s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:05.326464 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:05.327464 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m01:21:05.327464 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:05.486485 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:05.488486 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=f2652bb4-c71b-4282-9bdf-b5e5964f7c67) - Closing cursor
[0m01:21:05.496488 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=2.4068989753723145s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:05.497487 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=2.4078989028930664s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:05.498487 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:05.498487 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m01:21:05.499487 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:05.650793 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m01:21:05.654794 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=d9ec96d9-15c3-4db0-beee-b577981a221d) - Closing cursor
[0m01:21:05.663797 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.address_snapshot"
[0m01:21:05.664798 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=2.575209617614746s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:05.665797 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=2.5762085914611816s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:05.665797 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:05.666798 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`address_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m01:21:05.667799 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:09.443993 [debug] [Thread-1 (]: SQL status: OK in 3.7799999713897705 seconds
[0m01:21:09.444994 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=da960f3d-3c7c-4f62-a8b9-704defbdb204) - Closing cursor
[0m01:21:09.452996 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m01:21:09.460000 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=6.369409084320068s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:09.460000 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=6.370411157608032s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:09.460998 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.address_snapshot"
[0m01:21:09.461998 [debug] [Thread-1 (]: On snapshot.medallion_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.address_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m01:21:09.461998 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:09.967026 [debug] [Thread-1 (]: SQL status: OK in 0.5099999904632568 seconds
[0m01:21:09.969027 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=e2992ca5-4ce4-44db-a05b-a6994b52a83a) - Closing cursor
[0m01:21:09.990031 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m01:21:09.992032 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:09.993032 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:09.995032 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023878C1CC50>]}
[0m01:21:09.996032 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 7.11s]
[0m01:21:09.997034 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m01:21:09.998033 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m01:21:09.998033 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m01:21:09.999034 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.address_snapshot, idle-time=0.00600123405456543s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:10.000033 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m01:21:10.000033 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.00700068473815918s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m01:21:10.001034 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.008001565933227539s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Acquired connection on thread (19220, 2464), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m01:21:10.001034 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m01:21:10.005035 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m01:21:10.009036 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.01600360870361328s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:10.010036 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.01700305938720703s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:10.010036 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:10.011035 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m01:21:10.011035 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:10.275160 [debug] [Thread-1 (]: SQL status: OK in 0.25999999046325684 seconds
[0m01:21:10.279161 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=1cf23aaf-ba9c-4bbe-b3f0-2cfc07a2db9a) - Closing cursor
[0m01:21:10.281162 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.2881295680999756s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:10.282161 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.2881295680999756s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:10.282161 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:10.283162 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m01:21:10.283162 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:10.492911 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m01:21:10.495912 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=3ec10383-7f34-4e48-aa60-df39440f9136) - Closing cursor
[0m01:21:10.497912 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.5048797130584717s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:10.498912 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.5058794021606445s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:10.498912 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:10.499911 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m01:21:10.500913 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:10.643110 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:10.646111 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=e4eadaaf-fa1e-4318-9fa7-506c971e3a51) - Closing cursor
[0m01:21:10.649113 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.6560802459716797s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:10.650112 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.6570799350738525s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:10.651114 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:10.651114 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m01:21:10.652114 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:10.793133 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:10.796134 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=0cb6f8e5-8aed-4d40-b2df-46fcf5ad579b) - Closing cursor
[0m01:21:10.799135 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.8061022758483887s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:10.800134 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.8061022758483887s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:10.800134 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:10.801135 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customer_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`NameStyle` != source_data.`NameStyle`
        or
        (
            ((snapshotted_data.`NameStyle` is null) and not (source_data.`NameStyle` is null))
            or
            ((not snapshotted_data.`NameStyle` is null) and (source_data.`NameStyle` is null))
        ) or snapshotted_data.`Title` != source_data.`Title`
        or
        (
            ((snapshotted_data.`Title` is null) and not (source_data.`Title` is null))
            or
            ((not snapshotted_data.`Title` is null) and (source_data.`Title` is null))
        ) or snapshotted_data.`FirstName` != source_data.`FirstName`
        or
        (
            ((snapshotted_data.`FirstName` is null) and not (source_data.`FirstName` is null))
            or
            ((not snapshotted_data.`FirstName` is null) and (source_data.`FirstName` is null))
        ) or snapshotted_data.`MiddleName` != source_data.`MiddleName`
        or
        (
            ((snapshotted_data.`MiddleName` is null) and not (source_data.`MiddleName` is null))
            or
            ((not snapshotted_data.`MiddleName` is null) and (source_data.`MiddleName` is null))
        ) or snapshotted_data.`LastName` != source_data.`LastName`
        or
        (
            ((snapshotted_data.`LastName` is null) and not (source_data.`LastName` is null))
            or
            ((not snapshotted_data.`LastName` is null) and (source_data.`LastName` is null))
        ) or snapshotted_data.`Suffix` != source_data.`Suffix`
        or
        (
            ((snapshotted_data.`Suffix` is null) and not (source_data.`Suffix` is null))
            or
            ((not snapshotted_data.`Suffix` is null) and (source_data.`Suffix` is null))
        ) or snapshotted_data.`CompanyName` != source_data.`CompanyName`
        or
        (
            ((snapshotted_data.`CompanyName` is null) and not (source_data.`CompanyName` is null))
            or
            ((not snapshotted_data.`CompanyName` is null) and (source_data.`CompanyName` is null))
        ) or snapshotted_data.`SalesPerson` != source_data.`SalesPerson`
        or
        (
            ((snapshotted_data.`SalesPerson` is null) and not (source_data.`SalesPerson` is null))
            or
            ((not snapshotted_data.`SalesPerson` is null) and (source_data.`SalesPerson` is null))
        ) or snapshotted_data.`EmailAddress` != source_data.`EmailAddress`
        or
        (
            ((snapshotted_data.`EmailAddress` is null) and not (source_data.`EmailAddress` is null))
            or
            ((not snapshotted_data.`EmailAddress` is null) and (source_data.`EmailAddress` is null))
        ) or snapshotted_data.`Phone` != source_data.`Phone`
        or
        (
            ((snapshotted_data.`Phone` is null) and not (source_data.`Phone` is null))
            or
            ((not snapshotted_data.`Phone` is null) and (source_data.`Phone` is null))
        ) or snapshotted_data.`PasswordHash` != source_data.`PasswordHash`
        or
        (
            ((snapshotted_data.`PasswordHash` is null) and not (source_data.`PasswordHash` is null))
            or
            ((not snapshotted_data.`PasswordHash` is null) and (source_data.`PasswordHash` is null))
        ) or snapshotted_data.`PasswordSalt` != source_data.`PasswordSalt`
        or
        (
            ((snapshotted_data.`PasswordSalt` is null) and not (source_data.`PasswordSalt` is null))
            or
            ((not snapshotted_data.`PasswordSalt` is null) and (source_data.`PasswordSalt` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`NameStyle` != source_data.`NameStyle`
        or
        (
            ((snapshotted_data.`NameStyle` is null) and not (source_data.`NameStyle` is null))
            or
            ((not snapshotted_data.`NameStyle` is null) and (source_data.`NameStyle` is null))
        ) or snapshotted_data.`Title` != source_data.`Title`
        or
        (
            ((snapshotted_data.`Title` is null) and not (source_data.`Title` is null))
            or
            ((not snapshotted_data.`Title` is null) and (source_data.`Title` is null))
        ) or snapshotted_data.`FirstName` != source_data.`FirstName`
        or
        (
            ((snapshotted_data.`FirstName` is null) and not (source_data.`FirstName` is null))
            or
            ((not snapshotted_data.`FirstName` is null) and (source_data.`FirstName` is null))
        ) or snapshotted_data.`MiddleName` != source_data.`MiddleName`
        or
        (
            ((snapshotted_data.`MiddleName` is null) and not (source_data.`MiddleName` is null))
            or
            ((not snapshotted_data.`MiddleName` is null) and (source_data.`MiddleName` is null))
        ) or snapshotted_data.`LastName` != source_data.`LastName`
        or
        (
            ((snapshotted_data.`LastName` is null) and not (source_data.`LastName` is null))
            or
            ((not snapshotted_data.`LastName` is null) and (source_data.`LastName` is null))
        ) or snapshotted_data.`Suffix` != source_data.`Suffix`
        or
        (
            ((snapshotted_data.`Suffix` is null) and not (source_data.`Suffix` is null))
            or
            ((not snapshotted_data.`Suffix` is null) and (source_data.`Suffix` is null))
        ) or snapshotted_data.`CompanyName` != source_data.`CompanyName`
        or
        (
            ((snapshotted_data.`CompanyName` is null) and not (source_data.`CompanyName` is null))
            or
            ((not snapshotted_data.`CompanyName` is null) and (source_data.`CompanyName` is null))
        ) or snapshotted_data.`SalesPerson` != source_data.`SalesPerson`
        or
        (
            ((snapshotted_data.`SalesPerson` is null) and not (source_data.`SalesPerson` is null))
            or
            ((not snapshotted_data.`SalesPerson` is null) and (source_data.`SalesPerson` is null))
        ) or snapshotted_data.`EmailAddress` != source_data.`EmailAddress`
        or
        (
            ((snapshotted_data.`EmailAddress` is null) and not (source_data.`EmailAddress` is null))
            or
            ((not snapshotted_data.`EmailAddress` is null) and (source_data.`EmailAddress` is null))
        ) or snapshotted_data.`Phone` != source_data.`Phone`
        or
        (
            ((snapshotted_data.`Phone` is null) and not (source_data.`Phone` is null))
            or
            ((not snapshotted_data.`Phone` is null) and (source_data.`Phone` is null))
        ) or snapshotted_data.`PasswordHash` != source_data.`PasswordHash`
        or
        (
            ((snapshotted_data.`PasswordHash` is null) and not (source_data.`PasswordHash` is null))
            or
            ((not snapshotted_data.`PasswordHash` is null) and (source_data.`PasswordHash` is null))
        ) or snapshotted_data.`PasswordSalt` != source_data.`PasswordSalt`
        or
        (
            ((snapshotted_data.`PasswordSalt` is null) and not (source_data.`PasswordSalt` is null))
            or
            ((not snapshotted_data.`PasswordSalt` is null) and (source_data.`PasswordSalt` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m01:21:10.803135 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:11.272104 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m01:21:11.273105 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=df9efe01-6b3c-47d6-8ff3-5d7a1d4a5aa1) - Closing cursor
[0m01:21:11.275105 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=1.2820727825164795s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:11.276105 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=1.2830724716186523s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:11.276105 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:11.277106 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
[0m01:21:11.277106 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:11.451205 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m01:21:11.454206 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=1041a7ba-4b38-4548-9cdb-68d92acc1b31) - Closing cursor
[0m01:21:11.456205 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=1.4631729125976562s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:11.457207 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=1.4641742706298828s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:11.457207 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:11.458207 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m01:21:11.459207 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:11.620230 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:11.623231 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=93f4a01a-786a-4abb-a488-4f499f115d9b) - Closing cursor
[0m01:21:11.628232 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=1.635200023651123s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:11.628232 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=1.635200023651123s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:11.629233 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:11.629233 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
[0m01:21:11.630234 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:11.771171 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:11.775172 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=60ad0113-503f-4a4e-82ef-f04a580936fb) - Closing cursor
[0m01:21:11.779173 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=1.7861402034759521s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:11.780173 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=1.7861402034759521s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:11.780173 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:11.781173 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m01:21:11.781173 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:11.937238 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:11.940238 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=74193299-f470-428a-9d08-b16aa791571e) - Closing cursor
[0m01:21:11.943240 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=1.9502074718475342s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:11.944240 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=1.9512073993682861s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:11.945240 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:11.945240 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
[0m01:21:11.946240 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:12.086568 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:12.090570 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=9391c44c-47b2-4e9d-b97e-cea27af09e80) - Closing cursor
[0m01:21:12.091570 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.customer_snapshot"
[0m01:21:12.093570 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=2.1005375385284424s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:12.093570 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=2.1005375385284424s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:12.094571 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:12.094571 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customer_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m01:21:12.095571 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:15.387554 [debug] [Thread-1 (]: SQL status: OK in 3.2899999618530273 seconds
[0m01:21:15.388555 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=a82349db-0d63-4683-a467-8d4416af891c) - Closing cursor
[0m01:21:15.391555 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
[0m01:21:15.392555 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=5.399522304534912s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:15.393556 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=5.400523662567139s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:15.393556 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customer_snapshot"
[0m01:21:15.394556 [debug] [Thread-1 (]: On snapshot.medallion_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customer_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
[0m01:21:15.394556 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:15.954548 [debug] [Thread-1 (]: SQL status: OK in 0.5600000023841858 seconds
[0m01:21:15.955549 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=dc6f5a6d-3973-4388-a02d-b112435b203a) - Closing cursor
[0m01:21:15.958549 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m01:21:15.959549 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:15.960550 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:15.961550 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238786DE6C0>]}
[0m01:21:15.962550 [info ] [Thread-1 (]: 2 of 7 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 5.96s]
[0m01:21:15.963551 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m01:21:15.964550 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m01:21:15.964550 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m01:21:15.965551 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.005001068115234375s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:15.966553 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m01:21:15.966553 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.006002902984619141s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m01:21:15.967553 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.007003307342529297s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Acquired connection on thread (19220, 2464), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m01:21:15.968553 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m01:21:15.972554 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m01:21:15.978555 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.01800537109375s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:15.979555 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.019005298614501953s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:15.979555 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:15.980556 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m01:21:15.980556 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:16.262779 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m01:21:16.264779 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=0eadb50e-24b1-4f19-9e80-0343637837da) - Closing cursor
[0m01:21:16.267779 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.3062293529510498s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:16.267779 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.30722951889038086s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:16.268780 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:16.268780 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m01:21:16.269780 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:16.417804 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m01:21:16.420804 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=89f44dfa-067a-4094-8aa9-de710ba6a42f) - Closing cursor
[0m01:21:16.424805 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.4632554054260254s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:16.424805 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.46425557136535645s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:16.425806 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:16.426806 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m01:21:16.427806 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:16.581854 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:16.583855 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=d117334b-7305-400b-b451-a7ec9c3536a1) - Closing cursor
[0m01:21:16.588854 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.6273050308227539s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:16.588854 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.6283044815063477s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:16.589856 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:16.590855 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m01:21:16.591856 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:16.763339 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m01:21:16.768340 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=925e8861-cba5-4865-bbc7-e1b30318cfe3) - Closing cursor
[0m01:21:16.774341 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.8127915859222412s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:16.774341 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.8137912750244141s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:16.775342 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:16.776342 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId||'-'||AddressId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customeraddress_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m01:21:16.776342 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:17.141882 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m01:21:17.142882 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=d86778dd-fe18-4836-aba5-0798e80edf0c) - Closing cursor
[0m01:21:17.144881 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.1843314170837402s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:17.145883 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.185333251953125s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:17.145883 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:17.146883 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m01:21:17.146883 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:17.307897 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:17.310897 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=fe99dc43-5a3e-439a-8582-47efbcff5600) - Closing cursor
[0m01:21:17.312897 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.3523471355438232s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:17.313898 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.353348731994629s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:17.313898 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:17.314899 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m01:21:17.314899 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:17.483090 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m01:21:17.486091 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=81752892-a466-4c93-86ce-0c524a44d263) - Closing cursor
[0m01:21:17.489093 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.5285429954528809s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:17.489093 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.5285429954528809s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:17.490094 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:17.490094 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m01:21:17.491093 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:17.627095 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:17.630095 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=24484c7b-873b-4e8f-b0dd-c45cf7ba7ac0) - Closing cursor
[0m01:21:17.632097 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.6715469360351562s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:17.633096 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.672546148300171s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:17.634097 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:17.634097 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m01:21:17.635097 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:17.799964 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:17.801965 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=99a52b10-c147-4273-a526-f15996bebdc6) - Closing cursor
[0m01:21:17.804965 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.8444156646728516s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:17.805966 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.8444156646728516s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:17.805966 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:17.806966 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m01:21:17.806966 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:17.936071 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m01:21:17.939072 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=7304a281-ce6c-4406-bb38-d10291cf3acf) - Closing cursor
[0m01:21:17.941072 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:17.942073 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.981523036956787s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:17.942073 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=1.981523036956787s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:17.943073 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:17.943073 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customeraddress_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m01:21:17.944073 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:21.409581 [debug] [Thread-1 (]: SQL status: OK in 3.4700000286102295 seconds
[0m01:21:21.410581 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=8a866c01-8582-40d9-ae02-5eb19e3fc96e) - Closing cursor
[0m01:21:21.413582 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m01:21:21.414582 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=5.454031944274902s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:21.415583 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=5.455032825469971s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:21.415583 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.customeraddress_snapshot"
[0m01:21:21.416583 [debug] [Thread-1 (]: On snapshot.medallion_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.customeraddress_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m01:21:21.416583 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:21.855706 [debug] [Thread-1 (]: SQL status: OK in 0.4399999976158142 seconds
[0m01:21:21.856706 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=bb014dd4-1a20-4e6c-9aa7-f2d2550d1f50) - Closing cursor
[0m01:21:21.858707 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m01:21:21.859707 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:21.860707 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:21.860707 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023878C04DD0>]}
[0m01:21:21.861707 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 5.90s]
[0m01:21:21.862709 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m01:21:21.863709 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m01:21:21.863709 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m01:21:21.864708 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0050013065338134766s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:21.865709 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m01:21:21.865709 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.006001949310302734s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m01:21:21.866709 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.007001638412475586s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Acquired connection on thread (19220, 2464), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m01:21:21.866709 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m01:21:21.870709 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m01:21:21.874711 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.015004158020019531s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:21.875710 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.01600337028503418s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:21.876712 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:21.876712 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m01:21:21.877712 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:22.113767 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m01:21:22.117768 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=040036ae-8582-4e4d-b813-b47c3d999438) - Closing cursor
[0m01:21:22.120769 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.2610621452331543s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:22.121769 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.2610621452331543s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:22.121769 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:22.122770 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m01:21:22.122770 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:22.506425 [debug] [Thread-1 (]: SQL status: OK in 0.3799999952316284 seconds
[0m01:21:22.514426 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=6c59cef1-49e4-47b1-b80d-d69dcdaf9072) - Closing cursor
[0m01:21:22.517427 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.6567199230194092s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:22.517427 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.6577198505401611s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:22.518428 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:22.518428 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
describe table `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m01:21:22.519427 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:22.668794 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m01:21:22.672795 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=c6c042b1-cafd-44c0-be43-904c2576edd9) - Closing cursor
[0m01:21:22.676796 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.8170888423919678s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:22.677796 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.8180892467498779s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:22.678796 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:22.679797 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
describe table `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m01:21:22.680797 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:22.842860 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:22.846861 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=d9a25dff-e611-4807-acc1-04e261b0ba14) - Closing cursor
[0m01:21:22.849862 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.9901554584503174s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:22.850862 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.991154670715332s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:22.850862 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:22.852862 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`product_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`ProductNumber` != source_data.`ProductNumber`
        or
        (
            ((snapshotted_data.`ProductNumber` is null) and not (source_data.`ProductNumber` is null))
            or
            ((not snapshotted_data.`ProductNumber` is null) and (source_data.`ProductNumber` is null))
        ) or snapshotted_data.`Color` != source_data.`Color`
        or
        (
            ((snapshotted_data.`Color` is null) and not (source_data.`Color` is null))
            or
            ((not snapshotted_data.`Color` is null) and (source_data.`Color` is null))
        ) or snapshotted_data.`StandardCost` != source_data.`StandardCost`
        or
        (
            ((snapshotted_data.`StandardCost` is null) and not (source_data.`StandardCost` is null))
            or
            ((not snapshotted_data.`StandardCost` is null) and (source_data.`StandardCost` is null))
        ) or snapshotted_data.`ListPrice` != source_data.`ListPrice`
        or
        (
            ((snapshotted_data.`ListPrice` is null) and not (source_data.`ListPrice` is null))
            or
            ((not snapshotted_data.`ListPrice` is null) and (source_data.`ListPrice` is null))
        ) or snapshotted_data.`Size` != source_data.`Size`
        or
        (
            ((snapshotted_data.`Size` is null) and not (source_data.`Size` is null))
            or
            ((not snapshotted_data.`Size` is null) and (source_data.`Size` is null))
        ) or snapshotted_data.`Weight` != source_data.`Weight`
        or
        (
            ((snapshotted_data.`Weight` is null) and not (source_data.`Weight` is null))
            or
            ((not snapshotted_data.`Weight` is null) and (source_data.`Weight` is null))
        ) or snapshotted_data.`ProductCategoryID` != source_data.`ProductCategoryID`
        or
        (
            ((snapshotted_data.`ProductCategoryID` is null) and not (source_data.`ProductCategoryID` is null))
            or
            ((not snapshotted_data.`ProductCategoryID` is null) and (source_data.`ProductCategoryID` is null))
        ) or snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`SellStartDate` != source_data.`SellStartDate`
        or
        (
            ((snapshotted_data.`SellStartDate` is null) and not (source_data.`SellStartDate` is null))
            or
            ((not snapshotted_data.`SellStartDate` is null) and (source_data.`SellStartDate` is null))
        ) or snapshotted_data.`SellEndDate` != source_data.`SellEndDate`
        or
        (
            ((snapshotted_data.`SellEndDate` is null) and not (source_data.`SellEndDate` is null))
            or
            ((not snapshotted_data.`SellEndDate` is null) and (source_data.`SellEndDate` is null))
        ) or snapshotted_data.`DiscontinuedDate` != source_data.`DiscontinuedDate`
        or
        (
            ((snapshotted_data.`DiscontinuedDate` is null) and not (source_data.`DiscontinuedDate` is null))
            or
            ((not snapshotted_data.`DiscontinuedDate` is null) and (source_data.`DiscontinuedDate` is null))
        ) or snapshotted_data.`ThumbNailPhoto` != source_data.`ThumbNailPhoto`
        or
        (
            ((snapshotted_data.`ThumbNailPhoto` is null) and not (source_data.`ThumbNailPhoto` is null))
            or
            ((not snapshotted_data.`ThumbNailPhoto` is null) and (source_data.`ThumbNailPhoto` is null))
        ) or snapshotted_data.`ThumbnailPhotoFileName` != source_data.`ThumbnailPhotoFileName`
        or
        (
            ((snapshotted_data.`ThumbnailPhotoFileName` is null) and not (source_data.`ThumbnailPhotoFileName` is null))
            or
            ((not snapshotted_data.`ThumbnailPhotoFileName` is null) and (source_data.`ThumbnailPhotoFileName` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`ProductNumber` != source_data.`ProductNumber`
        or
        (
            ((snapshotted_data.`ProductNumber` is null) and not (source_data.`ProductNumber` is null))
            or
            ((not snapshotted_data.`ProductNumber` is null) and (source_data.`ProductNumber` is null))
        ) or snapshotted_data.`Color` != source_data.`Color`
        or
        (
            ((snapshotted_data.`Color` is null) and not (source_data.`Color` is null))
            or
            ((not snapshotted_data.`Color` is null) and (source_data.`Color` is null))
        ) or snapshotted_data.`StandardCost` != source_data.`StandardCost`
        or
        (
            ((snapshotted_data.`StandardCost` is null) and not (source_data.`StandardCost` is null))
            or
            ((not snapshotted_data.`StandardCost` is null) and (source_data.`StandardCost` is null))
        ) or snapshotted_data.`ListPrice` != source_data.`ListPrice`
        or
        (
            ((snapshotted_data.`ListPrice` is null) and not (source_data.`ListPrice` is null))
            or
            ((not snapshotted_data.`ListPrice` is null) and (source_data.`ListPrice` is null))
        ) or snapshotted_data.`Size` != source_data.`Size`
        or
        (
            ((snapshotted_data.`Size` is null) and not (source_data.`Size` is null))
            or
            ((not snapshotted_data.`Size` is null) and (source_data.`Size` is null))
        ) or snapshotted_data.`Weight` != source_data.`Weight`
        or
        (
            ((snapshotted_data.`Weight` is null) and not (source_data.`Weight` is null))
            or
            ((not snapshotted_data.`Weight` is null) and (source_data.`Weight` is null))
        ) or snapshotted_data.`ProductCategoryID` != source_data.`ProductCategoryID`
        or
        (
            ((snapshotted_data.`ProductCategoryID` is null) and not (source_data.`ProductCategoryID` is null))
            or
            ((not snapshotted_data.`ProductCategoryID` is null) and (source_data.`ProductCategoryID` is null))
        ) or snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`SellStartDate` != source_data.`SellStartDate`
        or
        (
            ((snapshotted_data.`SellStartDate` is null) and not (source_data.`SellStartDate` is null))
            or
            ((not snapshotted_data.`SellStartDate` is null) and (source_data.`SellStartDate` is null))
        ) or snapshotted_data.`SellEndDate` != source_data.`SellEndDate`
        or
        (
            ((snapshotted_data.`SellEndDate` is null) and not (source_data.`SellEndDate` is null))
            or
            ((not snapshotted_data.`SellEndDate` is null) and (source_data.`SellEndDate` is null))
        ) or snapshotted_data.`DiscontinuedDate` != source_data.`DiscontinuedDate`
        or
        (
            ((snapshotted_data.`DiscontinuedDate` is null) and not (source_data.`DiscontinuedDate` is null))
            or
            ((not snapshotted_data.`DiscontinuedDate` is null) and (source_data.`DiscontinuedDate` is null))
        ) or snapshotted_data.`ThumbNailPhoto` != source_data.`ThumbNailPhoto`
        or
        (
            ((snapshotted_data.`ThumbNailPhoto` is null) and not (source_data.`ThumbNailPhoto` is null))
            or
            ((not snapshotted_data.`ThumbNailPhoto` is null) and (source_data.`ThumbNailPhoto` is null))
        ) or snapshotted_data.`ThumbnailPhotoFileName` != source_data.`ThumbnailPhotoFileName`
        or
        (
            ((snapshotted_data.`ThumbnailPhotoFileName` is null) and not (source_data.`ThumbnailPhotoFileName` is null))
            or
            ((not snapshotted_data.`ThumbnailPhotoFileName` is null) and (source_data.`ThumbnailPhotoFileName` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m01:21:22.853863 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:23.299554 [debug] [Thread-1 (]: SQL status: OK in 0.44999998807907104 seconds
[0m01:21:23.300554 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=728675c1-9645-43da-8bc3-b1519adadbba) - Closing cursor
[0m01:21:23.303555 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=1.4438483715057373s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:23.303555 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=1.4438483715057373s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:23.304555 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:23.304555 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
describe table `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
[0m01:21:23.305556 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:23.445606 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:23.448606 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=f4167a54-ca09-4563-9783-cc4e40e3f687) - Closing cursor
[0m01:21:23.450607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=1.5908997058868408s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:23.451607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=1.591900110244751s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:23.451607 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:23.452607 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
describe table `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m01:21:23.453608 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:23.612656 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:23.615657 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=f5ff89cf-7903-42b5-9af5-6b2292a3577e) - Closing cursor
[0m01:21:23.618658 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=1.7589507102966309s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:23.618658 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=1.7589507102966309s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:23.619658 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:23.619658 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
describe table `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
[0m01:21:23.620658 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:23.764668 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:23.767669 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=fb5d6d27-ba68-4cf0-87a8-da17e8a69157) - Closing cursor
[0m01:21:23.769670 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=1.9099624156951904s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:23.770670 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=1.9109628200531006s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:23.770670 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:23.771670 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
describe table `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m01:21:23.772670 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:23.927687 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:23.930688 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=c82275d3-b4b5-4e71-aa24-5a79ffc2d7d9) - Closing cursor
[0m01:21:23.933688 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=2.073981285095215s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:23.934689 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=2.0749824047088623s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:23.934689 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:23.935690 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
describe table `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
[0m01:21:23.935690 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:24.073555 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:24.077556 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=5a6cd9f8-b497-48c7-9361-3c74e1959179) - Closing cursor
[0m01:21:24.079557 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.product_snapshot"
[0m01:21:24.080557 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=2.2208499908447266s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:24.081557 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=2.2218501567840576s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:24.081557 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:24.082558 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`product_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m01:21:24.084558 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:27.688038 [debug] [Thread-1 (]: SQL status: OK in 3.5999999046325684 seconds
[0m01:21:27.689038 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=d61a6d84-57b0-44d5-bc73-3c49365d982b) - Closing cursor
[0m01:21:27.692039 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
[0m01:21:27.693039 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=5.833332300186157s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:27.693039 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=5.833332300186157s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:27.694040 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.product_snapshot"
[0m01:21:27.694040 [debug] [Thread-1 (]: On snapshot.medallion_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.product_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
[0m01:21:27.695040 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:28.165853 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m01:21:28.166854 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=2aaf850f-a7a1-4ca6-978e-d8d6761edb91) - Closing cursor
[0m01:21:28.168854 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m01:21:28.169854 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:28.169854 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:28.170854 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023878FADCA0>]}
[0m01:21:28.171855 [info ] [Thread-1 (]: 4 of 7 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 6.31s]
[0m01:21:28.172855 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m01:21:28.172855 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m01:21:28.173855 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m01:21:28.174855 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0050013065338134766s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:28.174855 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m01:21:28.175855 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.006001472473144531s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m01:21:28.175855 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.006001472473144531s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Acquired connection on thread (19220, 2464), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m01:21:28.176855 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m01:21:28.181857 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m01:21:28.185858 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.01600360870361328s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:28.186858 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.017004013061523438s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:28.187858 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:28.187858 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m01:21:28.188858 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:28.481262 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m01:21:28.484263 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=65a38e3e-e0d8-4d0c-a2c4-8b7b4c9f09aa) - Closing cursor
[0m01:21:28.487264 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.3164091110229492s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:28.487264 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.3174099922180176s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:28.488264 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:28.488264 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m01:21:28.489265 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:28.626294 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:28.629296 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=ff9c4829-844f-42e0-a9a3-669c6d65a549) - Closing cursor
[0m01:21:28.633298 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.46244359016418457s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:28.633298 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.4634439945220947s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:28.634298 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:28.634298 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m01:21:28.635299 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:28.785331 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m01:21:28.788331 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=5b0c4f75-c12b-4278-8598-0af482cee4b0) - Closing cursor
[0m01:21:28.791332 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.6214783191680908s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:28.792333 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.6224792003631592s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:28.792333 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:28.793333 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m01:21:28.793333 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:28.954369 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:28.958369 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=fea2898d-3704-424c-8b48-671f5a2088b1) - Closing cursor
[0m01:21:28.963371 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7925169467926025s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:28.963371 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.7935173511505127s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:28.964371 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:28.965373 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductModelID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productmodel_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m01:21:28.966371 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:29.314451 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m01:21:29.315451 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=63519d5e-da55-4a32-8caa-910c1f61ca91) - Closing cursor
[0m01:21:29.318451 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.1485967636108398s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:29.318451 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.1485967636108398s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:29.319452 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:29.319452 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m01:21:29.320452 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:29.498492 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m01:21:29.501493 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=66ad3d24-58e3-47c2-8fc7-45999337b59c) - Closing cursor
[0m01:21:29.503493 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.3336389064788818s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:29.504493 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.3336389064788818s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:29.504493 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:29.505494 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m01:21:29.505494 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:29.660529 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:29.663529 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=c7520b68-1c6c-4ba0-ac58-0f0c37053c68) - Closing cursor
[0m01:21:29.666530 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.495675802230835s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:29.666530 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.4966762065887451s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:29.667530 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:29.667530 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m01:21:29.668530 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:29.801562 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m01:21:29.805563 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=34e3e3c3-bfa7-4289-9f40-9a631e875288) - Closing cursor
[0m01:21:29.808564 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.6387097835540771s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:29.809564 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.6387097835540771s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:29.809564 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:29.810565 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m01:21:29.810565 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:29.966613 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:29.968615 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=a6906730-824b-451a-8647-e878c2bbbc22) - Closing cursor
[0m01:21:29.971614 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.8017604351043701s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:29.972614 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.802760362625122s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:29.972614 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:29.973616 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m01:21:29.973616 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:30.112429 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:30.116431 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=407b4b14-ffa0-4875-8a70-88879880051a) - Closing cursor
[0m01:21:30.118431 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:30.120430 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.9505765438079834s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:30.121431 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=1.9505765438079834s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:30.121431 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:30.122431 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productmodel_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m01:21:30.123432 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:33.617855 [debug] [Thread-1 (]: SQL status: OK in 3.490000009536743 seconds
[0m01:21:33.618855 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=0932eb0c-2279-4078-82d1-5de62a6bff25) - Closing cursor
[0m01:21:33.623855 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m01:21:33.624856 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.4550018310546875s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:33.625857 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=5.456002712249756s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:33.626857 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.productmodel_snapshot"
[0m01:21:33.626857 [debug] [Thread-1 (]: On snapshot.medallion_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.productmodel_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m01:21:33.627857 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:34.045356 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m01:21:34.047356 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=989617ca-4736-490f-bcdc-e866c59cc9e7) - Closing cursor
[0m01:21:34.048356 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m01:21:34.049356 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:34.050356 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:34.050356 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023879009460>]}
[0m01:21:34.051357 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 5.88s]
[0m01:21:34.052358 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m01:21:34.053357 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m01:21:34.054357 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m01:21:34.055358 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.004001617431640625s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:34.055358 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m01:21:34.056358 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.00500178337097168s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m01:21:34.056358 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.006001710891723633s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Acquired connection on thread (19220, 2464), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m01:21:34.057358 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m01:21:34.061360 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m01:21:34.065360 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.015004158020019531s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:34.066360 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.01600480079650879s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:34.066360 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:34.067361 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m01:21:34.068361 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:34.368958 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m01:21:34.371958 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=7c3be6ff-f8e4-4d6a-8318-0f6e372bb55d) - Closing cursor
[0m01:21:34.373959 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.32360267639160156s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:34.374959 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.32360267639160156s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:34.374959 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:34.375960 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
select * from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m01:21:34.375960 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:34.543957 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m01:21:34.547958 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=ec195001-d599-4697-b516-fe88dd542cb5) - Closing cursor
[0m01:21:34.549959 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.4996030330657959s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:34.550959 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.500603199005127s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:34.551960 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:34.551960 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m01:21:34.552960 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:34.702995 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m01:21:34.704995 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=e60a2802-8612-4bad-8b3d-8ab9630ad0be) - Closing cursor
[0m01:21:34.707995 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.657639741897583s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:34.708995 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.658639669418335s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:34.708995 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:34.709996 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m01:21:34.710996 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:34.877008 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m01:21:34.880009 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=28154f61-2ec8-4604-be8a-36bd9a9233af) - Closing cursor
[0m01:21:34.884012 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.8336560726165771s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:34.884012 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.8336560726165771s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:34.885012 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:34.886011 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderDetailID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m01:21:34.887011 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:35.313051 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m01:21:35.315052 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=195af886-8ee2-4889-8a94-3bd0de1d99d2) - Closing cursor
[0m01:21:35.317052 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.2666962146759033s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:35.318052 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.2666962146759033s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:35.318052 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:35.319052 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m01:21:35.320053 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:35.455069 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:35.460069 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=e14f7080-6c03-4264-bc81-9c351e86f8ac) - Closing cursor
[0m01:21:35.464070 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.4127142429351807s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:35.464070 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.4137146472930908s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:35.465071 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:35.465071 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m01:21:35.466071 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:35.630089 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:35.633089 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=68f5244c-3ac8-4aea-97d7-761ff3524b0f) - Closing cursor
[0m01:21:35.636090 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.5847346782684326s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:35.636090 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.5857343673706055s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:35.637090 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:35.637090 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m01:21:35.638091 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:35.769120 [debug] [Thread-1 (]: SQL status: OK in 0.12999999523162842 seconds
[0m01:21:35.773121 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=8b850516-0c24-4759-bb53-e9a9429596ff) - Closing cursor
[0m01:21:35.777122 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7267661094665527s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:35.778122 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.7277660369873047s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:35.779122 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:35.779122 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m01:21:35.780122 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:35.939143 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:35.942144 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=19dbd7ac-7ad8-4589-afa2-7dedfccacbd3) - Closing cursor
[0m01:21:35.945145 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.894789457321167s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:35.945145 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=1.894789457321167s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:35.946145 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:35.947145 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m01:21:35.947145 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:36.086063 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:36.090063 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=7cb44742-ad30-44cf-b98e-6d567e06a580) - Closing cursor
[0m01:21:36.092065 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:36.093064 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=2.0427088737487793s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:36.094064 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=2.043708562850952s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:36.095065 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:36.095065 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m01:21:36.096065 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:39.439532 [debug] [Thread-1 (]: SQL status: OK in 3.3399999141693115 seconds
[0m01:21:39.440531 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=72a7e8f0-7bc6-432d-ace3-0a558288553e) - Closing cursor
[0m01:21:39.443532 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m01:21:39.444532 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.394176483154297s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:39.444532 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=5.394176483154297s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:39.445532 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderdetail_snapshot"
[0m01:21:39.445532 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderdetail_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m01:21:39.446533 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:39.867588 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m01:21:39.868589 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=800f23da-0a4c-401a-b946-2c0771a28c50) - Closing cursor
[0m01:21:39.870589 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m01:21:39.871589 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:39.871589 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:39.872589 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023878FB5B50>]}
[0m01:21:39.873590 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 5.82s]
[0m01:21:39.874592 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m01:21:39.875591 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m01:21:39.875591 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m01:21:39.876591 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.00500178337097168s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:39.877591 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m01:21:39.877591 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0060024261474609375s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m01:21:39.878591 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.007002115249633789s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Acquired connection on thread (19220, 2464), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m01:21:39.879592 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m01:21:39.883592 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m01:21:39.887593 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.016004562377929688s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:39.888594 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.017004728317260742s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:39.889595 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:39.889595 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m01:21:39.890596 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:40.126645 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m01:21:40.130645 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=d0228d47-1f72-454d-b05c-df1fea5fa68d) - Closing cursor
[0m01:21:40.135646 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.2640571594238281s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:40.136646 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.2650575637817383s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:40.137647 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:40.138647 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
select * from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m01:21:40.139647 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:40.291660 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m01:21:40.294661 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=0bf3dc40-af1a-4c95-abf0-2891ecb41081) - Closing cursor
[0m01:21:40.297662 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.4260728359222412s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:40.297662 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.4260728359222412s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:40.298662 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:40.298662 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m01:21:40.299662 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:40.457667 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:40.460668 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=19207b64-2ccb-452a-805a-f052cb751090) - Closing cursor
[0m01:21:40.464668 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.592078685760498s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:40.464668 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.5930793285369873s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:40.465668 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:40.465668 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m01:21:40.466668 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:40.622608 [debug] [Thread-1 (]: SQL status: OK in 0.1599999964237213 seconds
[0m01:21:40.625608 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=44d87186-4865-47ca-8985-f389f5211f6b) - Closing cursor
[0m01:21:40.629609 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.7570207118988037s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:40.629609 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.7580204010009766s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:40.630610 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:40.631610 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m01:21:40.633610 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:41.103661 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m01:21:41.104661 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=1632a2a3-066e-410f-8f08-72e00b2f4428) - Closing cursor
[0m01:21:41.106662 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.235072374343872s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:41.107661 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.236072301864624s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:41.107661 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:41.108662 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m01:21:41.108662 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:41.252784 [debug] [Thread-1 (]: SQL status: OK in 0.14000000059604645 seconds
[0m01:21:41.255784 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=1fb3622e-9c7a-4ff5-a73f-368b017b834a) - Closing cursor
[0m01:21:41.259786 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.3881971836090088s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:41.260787 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.3891983032226562s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:41.260787 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:41.261787 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m01:21:41.261787 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:41.533795 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m01:21:41.536797 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=0eb05933-eab6-4c4c-b556-993945d73f52) - Closing cursor
[0m01:21:41.538797 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.667207956314087s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:41.539797 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.668208360671997s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:41.539797 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:41.540798 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m01:21:41.541798 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:41.686820 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m01:21:41.689821 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=fd4714fd-86fe-4981-8115-ebb100c24b49) - Closing cursor
[0m01:21:41.691821 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.8202319145202637s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:41.692822 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.821232795715332s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:41.692822 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:41.693822 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m01:21:41.693822 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:41.844853 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m01:21:41.847854 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=7ba1676b-525e-4ff2-bbb0-a26868268d58) - Closing cursor
[0m01:21:41.849854 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.9782652854919434s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:41.850855 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=1.9792656898498535s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:41.851855 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:41.851855 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m01:21:41.852855 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:41.997891 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m01:21:42.001892 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=d31d5355-7c22-4861-a83d-455c542a79ca) - Closing cursor
[0m01:21:42.003892 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:42.004893 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.1333043575286865s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:42.005894 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=2.134305000305176s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:42.005894 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:42.006894 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderheader_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m01:21:42.007895 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:45.932336 [debug] [Thread-1 (]: SQL status: OK in 3.9200000762939453 seconds
[0m01:21:45.933336 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=8e2a047b-79d3-4802-b41e-24fae90421b6) - Closing cursor
[0m01:21:45.936337 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m01:21:45.937338 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=6.065748929977417s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Checking idleness
[0m01:21:45.938338 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=6.065748929977417s, acquire-count=1, language=sql, thread-identifier=(19220, 2464), compute-name=) - Retrieving connection
[0m01:21:45.938338 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark.salesorderheader_snapshot"
[0m01:21:45.938338 [debug] [Thread-1 (]: On snapshot.medallion_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "snapshot.medallion_spark.salesorderheader_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m01:21:45.939338 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=Unknown) - Created cursor
[0m01:21:46.433085 [debug] [Thread-1 (]: SQL status: OK in 0.49000000953674316 seconds
[0m01:21:46.434085 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=087335a1-9795-4de3-a5c9-33f04114891d, command-id=14e7c6af-e9fe-48b8-8be0-3b3b5c8c6c47) - Closing cursor
[0m01:21:46.436086 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m01:21:46.436086 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:46.437086 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2441564149024, session-id=087335a1-9795-4de3-a5c9-33f04114891d, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(19220, 2464), compute-name=) - Released connection
[0m01:21:46.438086 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '565b60b5-3dda-441c-a58c-cfdbbc815be8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023878BF3320>]}
[0m01:21:46.439087 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 6.56s]
[0m01:21:46.440087 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m01:21:46.441087 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=43.564996004104614s, acquire-count=0, language=None, thread-identifier=(19220, 17992), compute-name=) - Checking idleness
[0m01:21:46.441087 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=43.564996004104614s, acquire-count=0, language=None, thread-identifier=(19220, 17992), compute-name=) - Reusing connection previously named master
[0m01:21:46.442087 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=43.565996408462524s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Acquired connection on thread (19220, 17992), using default compute resource
[0m01:21:46.442087 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=43.565996408462524s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Checking idleness
[0m01:21:46.443088 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=None, name=master, idle-time=43.566996574401855s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Retrieving connection
[0m01:21:46.443088 [debug] [MainThread]: On master: ROLLBACK
[0m01:21:46.444088 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:21:46.642417 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=3a17101e-6ef3-4f7a-b34f-517c48e5a304, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Connection created
[0m01:21:46.643416 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:21:46.644416 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=3a17101e-6ef3-4f7a-b34f-517c48e5a304, name=master, idle-time=0.0019996166229248047s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Checking idleness
[0m01:21:46.645417 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=3a17101e-6ef3-4f7a-b34f-517c48e5a304, name=master, idle-time=0.003000020980834961s, acquire-count=1, language=None, thread-identifier=(19220, 17992), compute-name=) - Retrieving connection
[0m01:21:46.645417 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:21:46.646417 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:21:46.647417 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2441564148832, session-id=3a17101e-6ef3-4f7a-b34f-517c48e5a304, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(19220, 17992), compute-name=) - Released connection
[0m01:21:46.647417 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:21:46.648417 [debug] [MainThread]: On master: ROLLBACK
[0m01:21:46.649418 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:21:46.649418 [debug] [MainThread]: On master: Close
[0m01:21:46.650418 [debug] [MainThread]: Databricks adapter: Connection(session-id=3a17101e-6ef3-4f7a-b34f-517c48e5a304) - Closing connection
[0m01:21:46.713059 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m01:21:46.714059 [debug] [MainThread]: On list_hive_metastore: Close
[0m01:21:46.714059 [debug] [MainThread]: Databricks adapter: Connection(session-id=4bb7e805-52de-4519-aab2-952176039c7e) - Closing connection
[0m01:21:46.781412 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m01:21:46.782412 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m01:21:46.782412 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:21:46.783413 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m01:21:46.783413 [debug] [MainThread]: Databricks adapter: Connection(session-id=5aed727a-8eb3-46c1-935b-a5a61efa021c) - Closing connection
[0m01:21:46.836423 [debug] [MainThread]: Connection 'snapshot.medallion_spark.salesorderheader_snapshot' was properly closed.
[0m01:21:46.837424 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: ROLLBACK
[0m01:21:46.837424 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:21:46.838424 [debug] [MainThread]: On snapshot.medallion_spark.salesorderheader_snapshot: Close
[0m01:21:46.838424 [debug] [MainThread]: Databricks adapter: Connection(session-id=087335a1-9795-4de3-a5c9-33f04114891d) - Closing connection
[0m01:21:46.890436 [info ] [MainThread]: 
[0m01:21:46.891436 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 45.44 seconds (45.44s).
[0m01:21:46.894437 [debug] [MainThread]: Command end result
[0m01:21:46.940446 [info ] [MainThread]: 
[0m01:21:46.941452 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:21:46.942448 [info ] [MainThread]: 
[0m01:21:46.943448 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m01:21:46.944448 [debug] [MainThread]: Command `dbt snapshot` succeeded at 01:21:46.944448 after 48.01 seconds
[0m01:21:46.945449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002385CCD4440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238786DD190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023879089D30>]}
[0m01:21:46.945449 [debug] [MainThread]: Flushing usage events
[0m01:21:58.180030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA119D7740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA144E11C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA144E1220>]}


============================== 01:21:58.185031 | 24058db5-bd75-4f2f-b823-ac44a59fb841 ==============================
[0m01:21:58.185031 [info ] [MainThread]: Running with dbt=1.8.2
[0m01:21:58.186031 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m01:21:58.367073 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:21:58.367073 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:21:58.368073 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:21:59.875413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '24058db5-bd75-4f2f-b823-ac44a59fb841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA13530410>]}
[0m01:21:59.933426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '24058db5-bd75-4f2f-b823-ac44a59fb841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA14121250>]}
[0m01:21:59.934427 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m01:21:59.947430 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m01:22:00.173480 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:22:00.174481 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:22:00.226493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24058db5-bd75-4f2f-b823-ac44a59fb841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA2EFB1610>]}
[0m01:22:00.451544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24058db5-bd75-4f2f-b823-ac44a59fb841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA2F131FA0>]}
[0m01:22:00.452544 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m01:22:00.453544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24058db5-bd75-4f2f-b823-ac44a59fb841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA2EFB0BC0>]}
[0m01:22:00.455545 [info ] [MainThread]: 
[0m01:22:00.457545 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16424, 8268), compute-name=) - Creating connection
[0m01:22:00.457545 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:22:00.458545 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Acquired connection on thread (16424, 8268), using default compute resource
[0m01:22:00.464547 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036603461776, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16424, 4352), compute-name=) - Creating connection
[0m01:22:00.464547 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m01:22:00.465547 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036603461776, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16424, 4352), compute-name=) - Acquired connection on thread (16424, 4352), using default compute resource
[0m01:22:00.465547 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036603461776, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16424, 4352), compute-name=) - Checking idleness
[0m01:22:00.466547 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036603461776, session-id=None, name=list_hive_metastore, idle-time=0.0010004043579101562s, acquire-count=1, language=None, thread-identifier=(16424, 4352), compute-name=) - Retrieving connection
[0m01:22:00.467547 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m01:22:00.468548 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m01:22:00.468548 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:22:00.637585 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036603461776, session-id=e1224ccd-37dc-4ce1-be13-b32f31d0ff7e, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16424, 4352), compute-name=) - Connection created
[0m01:22:00.638586 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e1224ccd-37dc-4ce1-be13-b32f31d0ff7e, command-id=Unknown) - Created cursor
[0m01:22:00.733608 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m01:22:00.737609 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e1224ccd-37dc-4ce1-be13-b32f31d0ff7e, command-id=9b64fa1f-517f-4731-9cec-fe821776d895) - Closing cursor
[0m01:22:00.738609 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036603461776, session-id=e1224ccd-37dc-4ce1-be13-b32f31d0ff7e, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16424, 4352), compute-name=) - Released connection
[0m01:22:00.740609 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16424, 7300), compute-name=) - Creating connection
[0m01:22:00.740609 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m01:22:00.741609 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Acquired connection on thread (16424, 7300), using default compute resource
[0m01:22:00.741609 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Checking idleness
[0m01:22:00.742609 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0010008811950683594s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Retrieving connection
[0m01:22:00.742609 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:22:00.743609 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:22:00.743609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:22:00.907646 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Connection created
[0m01:22:00.907646 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=Unknown) - Created cursor
[0m01:22:01.037136 [debug] [ThreadPool]: SQL status: OK in 0.28999999165534973 seconds
[0m01:22:01.042136 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=00f145ec-5183-4979-bda8-5f6976e1611f) - Closing cursor
[0m01:22:01.057139 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_saleslt, idle-time=0.15049386024475098s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Checking idleness
[0m01:22:01.057139 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_saleslt, idle-time=0.15049386024475098s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Retrieving connection
[0m01:22:01.058139 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_saleslt, idle-time=0.15149378776550293s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Checking idleness
[0m01:22:01.058139 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_saleslt, idle-time=0.15149378776550293s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Retrieving connection
[0m01:22:01.059140 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:22:01.059140 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:22:01.060140 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m01:22:01.060140 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=Unknown) - Created cursor
[0m01:22:01.214956 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m01:22:01.216956 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=32bb7e6a-c879-4ae9-bf0c-b998a594e6e2) - Closing cursor
[0m01:22:01.222957 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_saleslt, idle-time=0.3163115978240967s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Checking idleness
[0m01:22:01.223956 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_saleslt, idle-time=0.31731104850769043s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Retrieving connection
[0m01:22:01.223956 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:22:01.224958 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m01:22:01.224958 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=Unknown) - Created cursor
[0m01:22:01.368545 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m01:22:01.370546 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=90f7e9ea-b412-4fb7-bff8-3d1b25db5278) - Closing cursor
[0m01:22:01.371546 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16424, 7300), compute-name=) - Released connection
[0m01:22:01.372546 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_saleslt, idle-time=0.0010001659393310547s, acquire-count=0, language=None, thread-identifier=(16424, 7300), compute-name=) - Checking idleness
[0m01:22:01.374547 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m01:22:01.375547 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_snapshots, idle-time=0.004001140594482422s, acquire-count=0, language=None, thread-identifier=(16424, 7300), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m01:22:01.375547 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_snapshots, idle-time=0.004001140594482422s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Acquired connection on thread (16424, 7300), using default compute resource
[0m01:22:01.376547 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_snapshots, idle-time=0.0050013065338134766s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Checking idleness
[0m01:22:01.376547 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_snapshots, idle-time=0.0050013065338134766s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Retrieving connection
[0m01:22:01.377547 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:22:01.377547 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m01:22:01.378548 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=Unknown) - Created cursor
[0m01:22:01.479170 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m01:22:01.483170 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=c6b3325f-2339-41d4-b5bb-d26c19b9e0f0) - Closing cursor
[0m01:22:01.488172 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_snapshots, idle-time=0.11662578582763672s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Checking idleness
[0m01:22:01.489173 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_snapshots, idle-time=0.11662578582763672s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Retrieving connection
[0m01:22:01.489173 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:22:01.490173 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m01:22:01.490173 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=Unknown) - Created cursor
[0m01:22:01.597209 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m01:22:01.599210 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=b22a62dd-ca88-4625-b4ff-e4b3a8b29467) - Closing cursor
[0m01:22:01.602210 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_snapshots, idle-time=0.23066377639770508s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Checking idleness
[0m01:22:01.602210 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_snapshots, idle-time=0.23066377639770508s, acquire-count=1, language=None, thread-identifier=(16424, 7300), compute-name=) - Retrieving connection
[0m01:22:01.603211 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:22:01.603211 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m01:22:01.604211 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=Unknown) - Created cursor
[0m01:22:01.739564 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m01:22:01.741565 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e98b083-675d-4997-8125-3d24df0512f5, command-id=3fddada4-819d-4f6e-8759-e0adae336d85) - Closing cursor
[0m01:22:01.742565 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2036604277472, session-id=0e98b083-675d-4997-8125-3d24df0512f5, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16424, 7300), compute-name=) - Released connection
[0m01:22:01.745565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24058db5-bd75-4f2f-b823-ac44a59fb841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA11486ED0>]}
[0m01:22:01.746566 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=1.288020372390747s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Checking idleness
[0m01:22:01.747566 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=1.2890210151672363s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Retrieving connection
[0m01:22:01.748566 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=1.2900211811065674s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Checking idleness
[0m01:22:01.749566 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=1.2910211086273193s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Retrieving connection
[0m01:22:01.750567 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:22:01.751567 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:22:01.752567 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16424, 8268), compute-name=) - Released connection
[0m01:22:01.753567 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:22:01.754568 [info ] [MainThread]: 
[0m01:22:01.757568 [debug] [Thread-1 (]: Began running node model.medallion_spark.dim_customer
[0m01:22:01.758569 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m01:22:01.759569 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0s, acquire-count=0, language=None, thread-identifier=(16424, 3252), compute-name=) - Creating connection
[0m01:22:01.760569 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_spark.dim_customer'
[0m01:22:01.760569 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Acquired connection on thread (16424, 3252), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m01:22:01.761569 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.dim_customer
[0m01:22:01.772572 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.dim_customer"
[0m01:22:01.775573 [debug] [Thread-1 (]: Began executing node model.medallion_spark.dim_customer
[0m01:22:01.798578 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m01:22:01.871594 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.dim_customer"
[0m01:22:01.872595 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.11202549934387207s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Checking idleness
[0m01:22:01.873594 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.11302495002746582s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Retrieving connection
[0m01:22:01.873594 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.11302495002746582s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Checking idleness
[0m01:22:01.874595 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.11402559280395508s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Retrieving connection
[0m01:22:01.875595 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:22:01.875595 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.dim_customer"
[0m01:22:01.876595 [debug] [Thread-1 (]: On model.medallion_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m01:22:01.876595 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:22:02.054644 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Connection created
[0m01:22:02.055643 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, command-id=Unknown) - Created cursor
[0m01:22:05.582680 [debug] [Thread-1 (]: SQL status: OK in 3.7100000381469727 seconds
[0m01:22:05.584681 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, command-id=daf39df7-db4f-467f-b5d0-4eb8de850fb5) - Closing cursor
[0m01:22:05.622689 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16424, 3252), compute-name=) - Released connection
[0m01:22:05.622689 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16424, 3252), compute-name=) - Released connection
[0m01:22:05.624690 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24058db5-bd75-4f2f-b823-ac44a59fb841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA2F313D10>]}
[0m01:22:05.625690 [info ] [Thread-1 (]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 3.86s]
[0m01:22:05.626690 [debug] [Thread-1 (]: Finished running node model.medallion_spark.dim_customer
[0m01:22:05.626690 [debug] [Thread-1 (]: Began running node model.medallion_spark.dim_product
[0m01:22:05.627690 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m01:22:05.628690 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_customer, idle-time=0.006000995635986328s, acquire-count=0, language=sql, thread-identifier=(16424, 3252), compute-name=) - Checking idleness
[0m01:22:05.628690 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.dim_customer, now model.medallion_spark.dim_product)
[0m01:22:05.629691 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_product, idle-time=0.007001161575317383s, acquire-count=0, language=sql, thread-identifier=(16424, 3252), compute-name=) - Reusing connection previously named model.medallion_spark.dim_customer
[0m01:22:05.629691 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_product, idle-time=0.007001161575317383s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Acquired connection on thread (16424, 3252), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m01:22:05.630691 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.dim_product
[0m01:22:05.634692 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.dim_product"
[0m01:22:05.636692 [debug] [Thread-1 (]: Began executing node model.medallion_spark.dim_product
[0m01:22:05.638693 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m01:22:05.641694 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.dim_product"
[0m01:22:05.643695 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_product, idle-time=0.021005868911743164s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Checking idleness
[0m01:22:05.644695 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_product, idle-time=0.022005319595336914s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Retrieving connection
[0m01:22:05.645695 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.dim_product"
[0m01:22:05.645695 [debug] [Thread-1 (]: On model.medallion_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m01:22:05.646695 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, command-id=Unknown) - Created cursor
[0m01:22:12.895562 [debug] [Thread-1 (]: SQL status: OK in 7.25 seconds
[0m01:22:12.896563 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, command-id=a6187aa8-71fc-4878-ac1b-62c5edaec44c) - Closing cursor
[0m01:22:12.956583 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16424, 3252), compute-name=) - Released connection
[0m01:22:12.957583 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16424, 3252), compute-name=) - Released connection
[0m01:22:12.958583 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24058db5-bd75-4f2f-b823-ac44a59fb841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA2F2D4EF0>]}
[0m01:22:12.958583 [info ] [Thread-1 (]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 7.33s]
[0m01:22:12.960584 [debug] [Thread-1 (]: Finished running node model.medallion_spark.dim_product
[0m01:22:12.960584 [debug] [Thread-1 (]: Began running node model.medallion_spark.sales
[0m01:22:12.961584 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m01:22:12.962584 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.dim_product, idle-time=0.0050008296966552734s, acquire-count=0, language=sql, thread-identifier=(16424, 3252), compute-name=) - Checking idleness
[0m01:22:12.962584 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.dim_product, now model.medallion_spark.sales)
[0m01:22:12.963584 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.sales, idle-time=0.006000995635986328s, acquire-count=0, language=sql, thread-identifier=(16424, 3252), compute-name=) - Reusing connection previously named model.medallion_spark.dim_product
[0m01:22:12.963584 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.sales, idle-time=0.006000995635986328s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Acquired connection on thread (16424, 3252), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m01:22:12.964585 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.sales
[0m01:22:12.968586 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.sales"
[0m01:22:12.969586 [debug] [Thread-1 (]: Began executing node model.medallion_spark.sales
[0m01:22:12.972587 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m01:22:12.974587 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark.sales"
[0m01:22:12.976587 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.sales, idle-time=0.019004106521606445s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Checking idleness
[0m01:22:12.977588 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.sales, idle-time=0.020004987716674805s, acquire-count=1, language=sql, thread-identifier=(16424, 3252), compute-name=) - Retrieving connection
[0m01:22:12.977588 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark.sales"
[0m01:22:12.978588 [debug] [Thread-1 (]: On model.medallion_spark.sales: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "node_id": "model.medallion_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m01:22:12.979588 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, command-id=Unknown) - Created cursor
[0m01:22:16.762027 [debug] [Thread-1 (]: SQL status: OK in 3.7799999713897705 seconds
[0m01:22:16.763027 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, command-id=b6565d79-f95c-48b8-aa82-223fadf6b861) - Closing cursor
[0m01:22:16.766027 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16424, 3252), compute-name=) - Released connection
[0m01:22:16.766027 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2036606099216, session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(16424, 3252), compute-name=) - Released connection
[0m01:22:16.767028 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24058db5-bd75-4f2f-b823-ac44a59fb841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA2F290080>]}
[0m01:22:16.768028 [info ] [Thread-1 (]: 3 of 3 OK created sql table model saleslt.sales ................................ [[32mOK[0m in 3.81s]
[0m01:22:16.769029 [debug] [Thread-1 (]: Finished running node model.medallion_spark.sales
[0m01:22:16.770028 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=15.01746129989624s, acquire-count=0, language=None, thread-identifier=(16424, 8268), compute-name=) - Checking idleness
[0m01:22:16.770028 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=15.01746129989624s, acquire-count=0, language=None, thread-identifier=(16424, 8268), compute-name=) - Reusing connection previously named master
[0m01:22:16.771028 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=15.01846170425415s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Acquired connection on thread (16424, 8268), using default compute resource
[0m01:22:16.771028 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=15.01846170425415s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Checking idleness
[0m01:22:16.772029 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=None, name=master, idle-time=15.019461870193481s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Retrieving connection
[0m01:22:16.772029 [debug] [MainThread]: On master: ROLLBACK
[0m01:22:16.773029 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:22:16.942089 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=7b69f1cb-398b-4815-879b-89fd58853659, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Connection created
[0m01:22:16.943089 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:22:16.943089 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=7b69f1cb-398b-4815-879b-89fd58853659, name=master, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Checking idleness
[0m01:22:16.944089 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=7b69f1cb-398b-4815-879b-89fd58853659, name=master, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(16424, 8268), compute-name=) - Retrieving connection
[0m01:22:16.944089 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:22:16.944089 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:22:16.945090 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2036604280064, session-id=7b69f1cb-398b-4815-879b-89fd58853659, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(16424, 8268), compute-name=) - Released connection
[0m01:22:16.945090 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:22:16.946090 [debug] [MainThread]: On master: ROLLBACK
[0m01:22:16.946090 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:22:16.947090 [debug] [MainThread]: On master: Close
[0m01:22:16.947090 [debug] [MainThread]: Databricks adapter: Connection(session-id=7b69f1cb-398b-4815-879b-89fd58853659) - Closing connection
[0m01:22:17.001836 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m01:22:17.002838 [debug] [MainThread]: On list_hive_metastore: Close
[0m01:22:17.003839 [debug] [MainThread]: Databricks adapter: Connection(session-id=e1224ccd-37dc-4ce1-be13-b32f31d0ff7e) - Closing connection
[0m01:22:17.069125 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m01:22:17.069125 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m01:22:17.070127 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:22:17.070127 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m01:22:17.071127 [debug] [MainThread]: Databricks adapter: Connection(session-id=0e98b083-675d-4997-8125-3d24df0512f5) - Closing connection
[0m01:22:17.120145 [debug] [MainThread]: Connection 'model.medallion_spark.sales' was properly closed.
[0m01:22:17.121146 [debug] [MainThread]: On model.medallion_spark.sales: ROLLBACK
[0m01:22:17.122146 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:22:17.122146 [debug] [MainThread]: On model.medallion_spark.sales: Close
[0m01:22:17.123146 [debug] [MainThread]: Databricks adapter: Connection(session-id=6b1ae2a7-7543-451a-8c68-ef4acc045e22) - Closing connection
[0m01:22:17.179340 [info ] [MainThread]: 
[0m01:22:17.180341 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 16.72 seconds (16.72s).
[0m01:22:17.182342 [debug] [MainThread]: Command end result
[0m01:22:17.232353 [info ] [MainThread]: 
[0m01:22:17.233353 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:22:17.233353 [info ] [MainThread]: 
[0m01:22:17.234354 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m01:22:17.235354 [debug] [MainThread]: Command `dbt run` succeeded at 01:22:17.235354 after 19.17 seconds
[0m01:22:17.236354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA14093800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA13C05E80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA13C05700>]}
[0m01:22:17.237354 [debug] [MainThread]: Flushing usage events
[0m01:51:39.514419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B88F720260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B88F721130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B88F720200>]}


============================== 01:51:39.521736 | 571a4604-0e64-4dda-8b11-b03d45359337 ==============================
[0m01:51:39.521736 [info ] [MainThread]: Running with dbt=1.8.2
[0m01:51:39.523736 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:51:39.742783 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:51:39.743783 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:51:39.743783 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:51:41.524275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '571a4604-0e64-4dda-8b11-b03d45359337', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8AA07E660>]}
[0m01:51:41.584289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '571a4604-0e64-4dda-8b11-b03d45359337', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8AA0695E0>]}
[0m01:51:41.585290 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m01:51:41.599293 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m01:51:41.839990 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:51:41.839990 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:51:41.889005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '571a4604-0e64-4dda-8b11-b03d45359337', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B88D2506B0>]}
[0m01:51:42.159449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '571a4604-0e64-4dda-8b11-b03d45359337', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8AA07C6B0>]}
[0m01:51:42.160449 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m01:51:42.161449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '571a4604-0e64-4dda-8b11-b03d45359337', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8AA1F2120>]}
[0m01:51:42.163449 [info ] [MainThread]: 
[0m01:51:42.164450 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m01:51:42.166450 [debug] [MainThread]: Command end result
[0m01:51:42.216623 [debug] [MainThread]: Command `dbt test` succeeded at 01:51:42.216623 after 2.89 seconds
[0m01:51:42.217617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B88E484410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8AA0AA570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8AA159C40>]}
[0m01:51:42.217617 [debug] [MainThread]: Flushing usage events
[0m01:54:14.925621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C2C937D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C2CADCC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C2947CA70>]}


============================== 01:54:14.930622 | 91cc0449-d7c6-4a8a-bc03-92a3810d32f8 ==============================
[0m01:54:14.930622 [info ] [MainThread]: Running with dbt=1.8.2
[0m01:54:14.930622 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt docs generate', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:54:15.106664 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:54:15.107664 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:54:15.107664 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:54:16.729397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '91cc0449-d7c6-4a8a-bc03-92a3810d32f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C2CE7DBE0>]}
[0m01:54:16.797421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '91cc0449-d7c6-4a8a-bc03-92a3810d32f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C47507A40>]}
[0m01:54:16.798421 [info ] [MainThread]: Registered adapter: databricks=1.8.1
[0m01:54:16.815425 [debug] [MainThread]: checksum: f0a141b9de567cb7f131613ba2682f40b3a1c6a157f81c6e5435862eff68f109, vars: {}, profile: , target: , version: 1.8.2
[0m01:54:17.056788 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:54:17.057787 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:54:17.109797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '91cc0449-d7c6-4a8a-bc03-92a3810d32f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C474A46B0>]}
[0m01:54:17.142810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '91cc0449-d7c6-4a8a-bc03-92a3810d32f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C47BB7EF0>]}
[0m01:54:17.143808 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 586 macros
[0m01:54:17.144809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '91cc0449-d7c6-4a8a-bc03-92a3810d32f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C478609B0>]}
[0m01:54:17.146809 [info ] [MainThread]: 
[0m01:54:17.148810 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1564571565520, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15300, 15836), compute-name=) - Creating connection
[0m01:54:17.148810 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:54:17.149811 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1564571565520, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15300, 15836), compute-name=) - Acquired connection on thread (15300, 15836), using default compute resource
[0m01:54:17.156813 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15300, 18596), compute-name=) - Creating connection
[0m01:54:17.157813 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m01:54:17.158813 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Acquired connection on thread (15300, 18596), using default compute resource
[0m01:54:17.158813 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Checking idleness
[0m01:54:17.159813 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0010001659393310547s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Retrieving connection
[0m01:54:17.159813 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:54:17.160813 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m01:54:17.160813 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:54:17.512523 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Connection created
[0m01:54:17.513523 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=Unknown) - Created cursor
[0m01:54:17.825521 [debug] [ThreadPool]: SQL status: OK in 0.6600000262260437 seconds
[0m01:54:17.830522 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=c80c28c4-f349-4723-9e11-b988e7d1edda) - Closing cursor
[0m01:54:17.848527 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_snapshots, idle-time=0.33600401878356934s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Checking idleness
[0m01:54:17.849527 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_snapshots, idle-time=0.3370041847229004s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Retrieving connection
[0m01:54:17.850529 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_snapshots, idle-time=0.33800578117370605s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Checking idleness
[0m01:54:17.850529 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_snapshots, idle-time=0.33800578117370605s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Retrieving connection
[0m01:54:17.851530 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:54:17.851530 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:54:17.852529 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m01:54:17.852529 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=Unknown) - Created cursor
[0m01:54:18.046596 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m01:54:18.131004 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=98d68c46-880e-4df0-ae2e-3321bcd82d6e) - Closing cursor
[0m01:54:18.138006 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_snapshots, idle-time=0.6244819164276123s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Checking idleness
[0m01:54:18.138006 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_snapshots, idle-time=0.6254827976226807s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Retrieving connection
[0m01:54:18.139005 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:54:18.139005 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m01:54:18.140005 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=Unknown) - Created cursor
[0m01:54:18.352959 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m01:54:18.355960 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=3690a2f3-6826-41f8-b0e8-b37ddad5a98e) - Closing cursor
[0m01:54:18.356960 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15300, 18596), compute-name=) - Released connection
[0m01:54:18.357961 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_snapshots, idle-time=0.001001596450805664s, acquire-count=0, language=None, thread-identifier=(15300, 18596), compute-name=) - Checking idleness
[0m01:54:18.359961 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m01:54:18.360969 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_saleslt, idle-time=0.0030012130737304688s, acquire-count=0, language=None, thread-identifier=(15300, 18596), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m01:54:18.360969 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_saleslt, idle-time=0.0040094852447509766s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Acquired connection on thread (15300, 18596), using default compute resource
[0m01:54:18.361965 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_saleslt, idle-time=0.0040094852447509766s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Checking idleness
[0m01:54:18.361965 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_saleslt, idle-time=0.005005359649658203s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Retrieving connection
[0m01:54:18.361965 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:54:18.362965 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:54:18.362965 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=Unknown) - Created cursor
[0m01:54:18.453994 [debug] [ThreadPool]: SQL status: OK in 0.09000000357627869 seconds
[0m01:54:18.457995 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=a0c00d70-4ace-485e-b4e1-184a35e668c6) - Closing cursor
[0m01:54:18.460996 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_saleslt, idle-time=0.10403609275817871s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Checking idleness
[0m01:54:18.462103 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_saleslt, idle-time=0.10514330863952637s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Retrieving connection
[0m01:54:18.462996 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:54:18.462996 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m01:54:18.463998 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=Unknown) - Created cursor
[0m01:54:18.560073 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m01:54:18.562073 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=96c4b564-b4ed-49dc-af1c-067b074c59d9) - Closing cursor
[0m01:54:18.565074 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_saleslt, idle-time=0.2081141471862793s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Checking idleness
[0m01:54:18.565074 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_saleslt, idle-time=0.2081141471862793s, acquire-count=1, language=None, thread-identifier=(15300, 18596), compute-name=) - Retrieving connection
[0m01:54:18.566075 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:54:18.566075 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m01:54:18.567075 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=Unknown) - Created cursor
[0m01:54:18.801862 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m01:54:18.804863 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, command-id=f5b087dc-658f-4e50-acbd-2151497a3ffd) - Closing cursor
[0m01:54:18.805863 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571719904, session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15300, 18596), compute-name=) - Released connection
[0m01:54:18.809864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '91cc0449-d7c6-4a8a-bc03-92a3810d32f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C477B9A90>]}
[0m01:54:18.809864 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1564571565520, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15300, 15836), compute-name=) - Released connection
[0m01:54:18.810864 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:54:18.811866 [info ] [MainThread]: 
[0m01:54:18.814865 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.address_snapshot
[0m01:54:18.815866 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15300, 15920), compute-name=) - Creating connection
[0m01:54:18.815866 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_spark.address_snapshot'
[0m01:54:18.816866 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(15300, 15920), compute-name=) - Acquired connection on thread (15300, 15920), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m01:54:18.816866 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.address_snapshot
[0m01:54:18.826868 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.address_snapshot
[0m01:54:18.827868 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.828868 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.829869 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.address_snapshot
[0m01:54:18.829869 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customer_snapshot
[0m01:54:18.830870 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.address_snapshot, idle-time=0.0030024051666259766s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Checking idleness
[0m01:54:18.831869 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.address_snapshot, now snapshot.medallion_spark.customer_snapshot)
[0m01:54:18.831869 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.004000663757324219s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Reusing connection previously named snapshot.medallion_spark.address_snapshot
[0m01:54:18.832870 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.005002021789550781s, acquire-count=1, language=sql, thread-identifier=(15300, 15920), compute-name=) - Acquired connection on thread (15300, 15920), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m01:54:18.833869 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customer_snapshot
[0m01:54:18.838871 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customer_snapshot
[0m01:54:18.839873 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.839873 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.840872 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customer_snapshot
[0m01:54:18.841872 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.customeraddress_snapshot
[0m01:54:18.842873 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.customer_snapshot, idle-time=0.0029997825622558594s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Checking idleness
[0m01:54:18.842873 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customer_snapshot, now snapshot.medallion_spark.customeraddress_snapshot)
[0m01:54:18.843873 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0039997100830078125s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customer_snapshot
[0m01:54:18.844873 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0049991607666015625s, acquire-count=1, language=sql, thread-identifier=(15300, 15920), compute-name=) - Acquired connection on thread (15300, 15920), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m01:54:18.845872 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.customeraddress_snapshot
[0m01:54:18.851876 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.customeraddress_snapshot
[0m01:54:18.854875 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.856875 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.861877 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.customeraddress_snapshot
[0m01:54:18.862878 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.product_snapshot
[0m01:54:18.863876 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.customeraddress_snapshot, idle-time=0.007999658584594727s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Checking idleness
[0m01:54:18.864878 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.customeraddress_snapshot, now snapshot.medallion_spark.product_snapshot)
[0m01:54:18.865879 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.product_snapshot, idle-time=0.010002613067626953s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Reusing connection previously named snapshot.medallion_spark.customeraddress_snapshot
[0m01:54:18.867878 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.product_snapshot, idle-time=0.011002063751220703s, acquire-count=1, language=sql, thread-identifier=(15300, 15920), compute-name=) - Acquired connection on thread (15300, 15920), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m01:54:18.868879 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.product_snapshot
[0m01:54:18.875879 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.product_snapshot
[0m01:54:18.876880 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.878881 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.880395 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.product_snapshot
[0m01:54:18.881389 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.productmodel_snapshot
[0m01:54:18.883390 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.product_snapshot, idle-time=0.005509853363037109s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Checking idleness
[0m01:54:18.885390 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.product_snapshot, now snapshot.medallion_spark.productmodel_snapshot)
[0m01:54:18.887390 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.009510278701782227s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Reusing connection previously named snapshot.medallion_spark.product_snapshot
[0m01:54:18.889392 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.011511802673339844s, acquire-count=1, language=sql, thread-identifier=(15300, 15920), compute-name=) - Acquired connection on thread (15300, 15920), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m01:54:18.890391 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.productmodel_snapshot
[0m01:54:18.900394 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.productmodel_snapshot
[0m01:54:18.902393 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.903393 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.904394 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.productmodel_snapshot
[0m01:54:18.906394 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m01:54:18.907395 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.productmodel_snapshot, idle-time=0.004002094268798828s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Checking idleness
[0m01:54:18.908394 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.productmodel_snapshot, now snapshot.medallion_spark.salesorderdetail_snapshot)
[0m01:54:18.909395 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.005001068115234375s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Reusing connection previously named snapshot.medallion_spark.productmodel_snapshot
[0m01:54:18.909395 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.00600123405456543s, acquire-count=1, language=sql, thread-identifier=(15300, 15920), compute-name=) - Acquired connection on thread (15300, 15920), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m01:54:18.910395 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderdetail_snapshot
[0m01:54:18.915396 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderdetail_snapshot
[0m01:54:18.916397 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.917396 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.918397 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderdetail_snapshot
[0m01:54:18.920399 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark.salesorderheader_snapshot
[0m01:54:18.922398 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.salesorderdetail_snapshot, idle-time=0.0040035247802734375s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Checking idleness
[0m01:54:18.923398 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderdetail_snapshot, now snapshot.medallion_spark.salesorderheader_snapshot)
[0m01:54:18.924399 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.006001710891723633s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderdetail_snapshot
[0m01:54:18.928404 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.011007547378540039s, acquire-count=1, language=sql, thread-identifier=(15300, 15920), compute-name=) - Acquired connection on thread (15300, 15920), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m01:54:18.935403 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark.salesorderheader_snapshot
[0m01:54:18.944403 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark.salesorderheader_snapshot
[0m01:54:18.945403 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.947403 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.948407 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark.salesorderheader_snapshot
[0m01:54:18.949406 [debug] [Thread-1 (]: Began running node model.medallion_spark.dim_customer
[0m01:54:18.951405 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=snapshot.medallion_spark.salesorderheader_snapshot, idle-time=0.004001140594482422s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Checking idleness
[0m01:54:18.952405 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark.salesorderheader_snapshot, now model.medallion_spark.dim_customer)
[0m01:54:18.952405 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.0060024261474609375s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Reusing connection previously named snapshot.medallion_spark.salesorderheader_snapshot
[0m01:54:18.953405 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.007002115249633789s, acquire-count=1, language=sql, thread-identifier=(15300, 15920), compute-name=) - Acquired connection on thread (15300, 15920), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m01:54:18.954405 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.dim_customer
[0m01:54:18.962408 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.dim_customer"
[0m01:54:18.964408 [debug] [Thread-1 (]: Began executing node model.medallion_spark.dim_customer
[0m01:54:18.966409 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.967409 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.969409 [debug] [Thread-1 (]: Finished running node model.medallion_spark.dim_customer
[0m01:54:18.970409 [debug] [Thread-1 (]: Began running node model.medallion_spark.dim_product
[0m01:54:18.971409 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.dim_customer, idle-time=0.003999471664428711s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Checking idleness
[0m01:54:18.972409 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.dim_customer, now model.medallion_spark.dim_product)
[0m01:54:18.973409 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.dim_product, idle-time=0.005999565124511719s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Reusing connection previously named model.medallion_spark.dim_customer
[0m01:54:18.974409 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.dim_product, idle-time=0.0070002079010009766s, acquire-count=1, language=sql, thread-identifier=(15300, 15920), compute-name=) - Acquired connection on thread (15300, 15920), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m01:54:18.975410 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.dim_product
[0m01:54:18.983412 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.dim_product"
[0m01:54:18.985412 [debug] [Thread-1 (]: Began executing node model.medallion_spark.dim_product
[0m01:54:18.986412 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.987413 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:18.988413 [debug] [Thread-1 (]: Finished running node model.medallion_spark.dim_product
[0m01:54:18.989415 [debug] [Thread-1 (]: Began running node model.medallion_spark.sales
[0m01:54:18.990414 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.dim_product, idle-time=0.003000974655151367s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Checking idleness
[0m01:54:18.992413 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark.dim_product, now model.medallion_spark.sales)
[0m01:54:18.993414 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.sales, idle-time=0.00500035285949707s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Reusing connection previously named model.medallion_spark.dim_product
[0m01:54:18.994415 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.sales, idle-time=0.007001399993896484s, acquire-count=1, language=sql, thread-identifier=(15300, 15920), compute-name=) - Acquired connection on thread (15300, 15920), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m01:54:18.995415 [debug] [Thread-1 (]: Began compiling node model.medallion_spark.sales
[0m01:54:19.003416 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark.sales"
[0m01:54:19.005417 [debug] [Thread-1 (]: Began executing node model.medallion_spark.sales
[0m01:54:19.007417 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:19.009419 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1564575570672, session-id=None, name=model.medallion_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(15300, 15920), compute-name=) - Released connection
[0m01:54:19.011418 [debug] [Thread-1 (]: Finished running node model.medallion_spark.sales
[0m01:54:19.012418 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:54:19.013418 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m01:54:19.014419 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m01:54:19.014419 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:54:19.015421 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m01:54:19.015421 [debug] [MainThread]: Databricks adapter: Connection(session-id=3564359c-0c2f-4b73-ac16-3ffcdf17b56c) - Closing connection
[0m01:54:19.071213 [debug] [MainThread]: Connection 'model.medallion_spark.sales' was properly closed.
[0m01:54:19.074214 [debug] [MainThread]: Command end result
[0m01:54:19.380451 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1564082739088, session-id=None, name=generate_catalog, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15300, 15836), compute-name=) - Creating connection
[0m01:54:19.381452 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m01:54:19.381452 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1564082739088, session-id=None, name=generate_catalog, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15300, 15836), compute-name=) - Acquired connection on thread (15300, 15836), using default compute resource
[0m01:54:19.382451 [info ] [MainThread]: Building catalog
[0m01:54:19.385453 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=None, name=('hive_metastore', 'snapshots'), idle-time=0s, acquire-count=0, language=None, thread-identifier=(15300, 15688), compute-name=) - Creating connection
[0m01:54:19.386454 [debug] [ThreadPool]: Acquiring new databricks connection '('hive_metastore', 'snapshots')'
[0m01:54:19.387454 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=None, name=('hive_metastore', 'snapshots'), idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Acquired connection on thread (15300, 15688), using default compute resource
[0m01:54:19.389454 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=None, name=('hive_metastore', 'snapshots'), idle-time=0.0030002593994140625s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Checking idleness
[0m01:54:19.390454 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=None, name=('hive_metastore', 'snapshots'), idle-time=0.004000425338745117s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Retrieving connection
[0m01:54:19.390454 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=None, name=('hive_metastore', 'snapshots'), idle-time=0.004000425338745117s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Checking idleness
[0m01:54:19.391453 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=None, name=('hive_metastore', 'snapshots'), idle-time=0.004999399185180664s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Retrieving connection
[0m01:54:19.391453 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:54:19.393456 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'snapshots')"
[0m01:54:19.394455 [debug] [ThreadPool]: On ('hive_metastore', 'snapshots'): /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'snapshots')"} */

      select current_catalog()
  
[0m01:54:19.395455 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:54:19.650720 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'snapshots'), idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Connection created
[0m01:54:19.650720 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, command-id=Unknown) - Created cursor
[0m01:54:19.760224 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m01:54:19.763224 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, command-id=e7fb9a36-e372-40bc-b28e-0468a77c343d) - Closing cursor
[0m01:54:19.771225 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'snapshots'), idle-time=0.12050414085388184s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Checking idleness
[0m01:54:19.771225 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'snapshots'), idle-time=0.12150406837463379s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Retrieving connection
[0m01:54:19.772225 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'snapshots')"
[0m01:54:19.772225 [debug] [ThreadPool]: On ('hive_metastore', 'snapshots'): /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'snapshots')"} */
show table extended in `hive_metastore`.`snapshots` like 'address_snapshot|salesorderheader_snapshot|salesorderdetail_snapshot|productmodel_snapshot|customer_snapshot|customeraddress_snapshot|product_snapshot'
  
[0m01:54:19.773226 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, command-id=Unknown) - Created cursor
[0m01:54:20.481304 [debug] [ThreadPool]: SQL status: OK in 0.7099999785423279 seconds
[0m01:54:20.485307 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, command-id=87f9043b-e1b8-4415-bdd1-6b5d0f07fbcd) - Closing cursor
[0m01:54:20.492307 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'snapshots'), idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15300, 15688), compute-name=) - Released connection
[0m01:54:20.494307 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'snapshots'), idle-time=0.0010001659393310547s, acquire-count=0, language=None, thread-identifier=(15300, 15688), compute-name=) - Checking idleness
[0m01:54:20.494307 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly ('hive_metastore', 'snapshots'), now ('hive_metastore', 'saleslt'))
[0m01:54:20.495308 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'saleslt'), idle-time=0.002000570297241211s, acquire-count=0, language=None, thread-identifier=(15300, 15688), compute-name=) - Reusing connection previously named ('hive_metastore', 'snapshots')
[0m01:54:20.495308 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'saleslt'), idle-time=0.0030007362365722656s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Acquired connection on thread (15300, 15688), using default compute resource
[0m01:54:20.498309 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'saleslt'), idle-time=0.006001949310302734s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Checking idleness
[0m01:54:20.499309 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'saleslt'), idle-time=0.007002353668212891s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Retrieving connection
[0m01:54:20.500309 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'saleslt')"
[0m01:54:20.501309 [debug] [ThreadPool]: On ('hive_metastore', 'saleslt'): /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'saleslt')"} */

      select current_catalog()
  
[0m01:54:20.501309 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, command-id=Unknown) - Created cursor
[0m01:54:20.595338 [debug] [ThreadPool]: SQL status: OK in 0.09000000357627869 seconds
[0m01:54:20.597339 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, command-id=beeb11b8-c6de-4a36-834b-0ce598f64b11) - Closing cursor
[0m01:54:20.600339 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'saleslt'), idle-time=0.1080319881439209s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Checking idleness
[0m01:54:20.600339 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'saleslt'), idle-time=0.1080319881439209s, acquire-count=1, language=None, thread-identifier=(15300, 15688), compute-name=) - Retrieving connection
[0m01:54:20.601339 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'saleslt')"
[0m01:54:20.601339 [debug] [ThreadPool]: On ('hive_metastore', 'saleslt'): /* {"app": "dbt", "dbt_version": "1.8.2", "dbt_databricks_version": "1.8.1", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'saleslt')"} */
show table extended in `hive_metastore`.`saleslt` like 'dim_product|product|productdescription|salesorderheader|address|productmodel|customer|customeraddress|dim_customer|salesorderdetail|productcategory|sales'
  
[0m01:54:20.602339 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, command-id=Unknown) - Created cursor
[0m01:54:21.071381 [debug] [ThreadPool]: SQL status: OK in 0.4699999988079071 seconds
[0m01:54:21.074382 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, command-id=1594063b-68b6-4ff7-a338-913c5b2929d7) - Closing cursor
[0m01:54:21.080383 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1564571137856, session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb, name=('hive_metastore', 'saleslt'), idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15300, 15688), compute-name=) - Released connection
[0m01:54:21.084384 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1564082739088, session-id=None, name=generate_catalog, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(15300, 15836), compute-name=) - Released connection
[0m01:54:21.137395 [info ] [MainThread]: Catalog written to C:\Users\shuru\OneDrive\Desktop\database\medallion_spark\target\catalog.json
[0m01:54:21.138396 [debug] [MainThread]: Command `dbt docs generate` succeeded at 01:54:21.138396 after 6.36 seconds
[0m01:54:21.139397 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m01:54:21.139397 [debug] [MainThread]: Connection '('hive_metastore', 'saleslt')' was properly closed.
[0m01:54:21.140397 [debug] [MainThread]: On ('hive_metastore', 'saleslt'): ROLLBACK
[0m01:54:21.140397 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:54:21.141397 [debug] [MainThread]: On ('hive_metastore', 'saleslt'): Close
[0m01:54:21.141397 [debug] [MainThread]: Databricks adapter: Connection(session-id=9ed1f625-1b0e-49f8-bc41-fc5cf806aabb) - Closing connection
[0m01:54:21.196409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C2CC168D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C2BE1C5F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C2BE1CA70>]}
[0m01:54:21.197410 [debug] [MainThread]: Flushing usage events
[0m01:54:42.782022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002116284A6F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002116284A780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021162FB7CB0>]}


============================== 01:54:42.787022 | 448626a0-6ced-4404-9e93-2b06f5765ee5 ==============================
[0m01:54:42.787022 [info ] [MainThread]: Running with dbt=1.8.2
[0m01:54:42.788023 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\shuru\\OneDrive\\Desktop\\database\\medallion_spark\\logs', 'profiles_dir': 'C:\\Users\\shuru\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt docs serve', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m01:54:42.961062 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:54:42.961062 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:54:42.962062 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:54:44.760629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '448626a0-6ced-4404-9e93-2b06f5765ee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002117DA73920>]}
[0m01:54:44.829646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '448626a0-6ced-4404-9e93-2b06f5765ee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002116315A630>]}
